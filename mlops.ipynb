{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[MLOPs collab notebook](https://colab.research.google.com/drive/1Q3FYIECUH8kFj8XScmDEEiPN7zx1yLZE?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "# MLOps Comprehensive Summary Notes\n",
    "\n",
    "## 1. Git and GitHub\n",
    "\n",
    "- **Git**: Distributed version control system that tracks code changes locally\n",
    "- **GitHub**: Web-based platform hosting remote Git repositories\n",
    "- **Key Commands**: \n",
    "  ```\n",
    "  git init         # Start repository\n",
    "  git add .        # Stage changes\n",
    "  git commit -m    # Record changes\n",
    "  git push/pull    # Upload/download changes\n",
    "  git branch       # Create branches\n",
    "  git checkout     # Switch branches\n",
    "  git merge        # Combine branches\n",
    "  ```\n",
    "- **Best Practices**:\n",
    "  - Pull before pushing to avoid conflicts\n",
    "  - Use branches for features/fixes\n",
    "  - Write meaningful commit messages\n",
    "  - Never commit sensitive data\n",
    "  - Use `.gitignore` for excluding files\n",
    "\n",
    "## 2. Streamlit\n",
    "\n",
    "- Web app framework for data science projects using pure Python\n",
    "- **Core Elements**:\n",
    "  ```python\n",
    "  st.title()       # Add page title\n",
    "  st.header()      # Add section header\n",
    "  st.text()        # Add basic text\n",
    "  st.dataframe()   # Display data\n",
    "  st.button()      # Create clickable button\n",
    "  st.slider()      # Create adjustable slider\n",
    "  st.file_uploader() # Allow file uploads\n",
    "  ```\n",
    "- **Key Features**:\n",
    "  - Reactive execution (automatic reruns)\n",
    "  - Built-in visualization support\n",
    "  - Widget state management with `st.session_state`\n",
    "  - Caching with `@st.cache_data` and `@st.cache_resource`\n",
    "- **Limitations**: No built-in security, limited app structure control\n",
    "\n",
    "# 3. Web APIs using Flask and FastAPI\n",
    "\n",
    "## Flask\n",
    "- Lightweight, flexible Python web framework for creating APIs\n",
    "- **Core Components**:\n",
    "  ```python\n",
    "  app = Flask(__name__)  # Initialize Flask app\n",
    "  \n",
    "  @app.route('/endpoint', methods=['GET'])\n",
    "  def function():        # Define endpoint function\n",
    "      return jsonify({'result': 'data'})\n",
    "  ```\n",
    "- **Request Types**:\n",
    "  - GET: Retrieve data\n",
    "  - POST: Submit data\n",
    "  - PUT: Update existing data\n",
    "  - DELETE: Remove data\n",
    "\n",
    "## FastAPI\n",
    "- Modern, high-performance API framework with automatic documentation\n",
    "- **Key Advantages**:\n",
    "  - Type checking with Pydantic models\n",
    "  - Automatic OpenAPI documentation\n",
    "  - Higher performance than Flask (based on Starlette)\n",
    "  - Async support for non-blocking operations\n",
    "  - Built-in dependency injection\n",
    "  \n",
    "- **Core Components**:\n",
    "  ```python\n",
    "  app = FastAPI()  # Initialize FastAPI app\n",
    "  \n",
    "  @app.get(\"/endpoint\")\n",
    "  def function(param: str = Query(\"default\")):\n",
    "      return {\"result\": \"data\"}\n",
    "      \n",
    "  # With type validation\n",
    "  from pydantic import BaseModel\n",
    "  \n",
    "  class Item(BaseModel):\n",
    "      name: str\n",
    "      price: float\n",
    "      \n",
    "  @app.post(\"/items/\")\n",
    "  def create_item(item: Item):\n",
    "      return item\n",
    "  ```\n",
    "\n",
    "## Flask vs FastAPI Comparison\n",
    "- **Performance**: FastAPI is generally faster due to ASGI vs Flask's WSGI\n",
    "- **Documentation**: FastAPI provides automatic interactive docs (Swagger UI and ReDoc)\n",
    "- **Type Checking**: FastAPI has built-in data validation and serialization\n",
    "- **Async Support**: FastAPI natively supports async, Flask requires extensions\n",
    "- **Learning Curve**: Flask is simpler to get started, FastAPI requires understanding type annotations\n",
    "\n",
    "## Best Practices for Both\n",
    "- Input validation and error handling\n",
    "- Authentication and authorization\n",
    "- Structured response formats\n",
    "- Versioning APIs\n",
    "- Proper status codes and error messages\n",
    "- Rate limiting for public APIs\n",
    "- Comprehensive logging\n",
    "- Testing endpoints with pytest\n",
    "\n",
    "## When to Choose Each\n",
    "- **Choose Flask**: For simple APIs, legacy applications, or when team is familiar with it\n",
    "- **Choose FastAPI**: For new projects, performance-critical APIs, when documentation is important, or when working with complex data models\n",
    "\n",
    "Both frameworks are excellent choices for ML model serving, with FastAPI gaining popularity for new projects due to its performance and built-in features.\n",
    "\n",
    "## 4. Docker and DockerHub\n",
    "\n",
    "- **Docker**: Platform for building, shipping and running containerized applications\n",
    "- **Key Components**:\n",
    "  - Dockerfile: Text file with container build instructions\n",
    "  - Image: Immutable snapshot containing code, dependencies, and configs\n",
    "  - Container: Running instance of an image\n",
    "  - Volume: Persistent storage mechanism\n",
    "- **Basic Commands**:\n",
    "  ```bash\n",
    "  docker build -t my-image .     # Build image\n",
    "  docker run -p 5000:5000 my-image  # Run container\n",
    "  docker ps                      # List running containers\n",
    "  docker push username/image     # Push to registry\n",
    "  ```\n",
    "- **DockerHub**: Public registry for sharing and downloading Docker images\n",
    "\n",
    "## 5. Deploying Docker on AWS (ECR & ECS)\n",
    "\n",
    "- **ECR** (Elastic Container Registry): AWS service for storing Docker images\n",
    "- **ECS** (Elastic Container Service): AWS service for running containerized applications\n",
    "- **Deployment Steps**:\n",
    "  1. Build Docker image locally\n",
    "  2. Create ECR repository\n",
    "  3. Push image to ECR\n",
    "  4. Create ECS cluster\n",
    "  5. Define task definition (container specs, resources)\n",
    "  6. Create ECS service\n",
    "  7. Set up load balancing and networking\n",
    "- **Fargate**: Serverless compute engine for ECS, eliminating server management\n",
    "\n",
    "## 6. CI/CD Pipelines\n",
    "\n",
    "- **CI** (Continuous Integration): Automatically build and test code changes\n",
    "- **CD** (Continuous Delivery/Deployment): Automate release process to production\n",
    "- **GitHub Actions Workflow**:\n",
    "  ```yaml\n",
    "  name: CI/CD Pipeline\n",
    "  on: [push, pull_request]\n",
    "  jobs:\n",
    "    test:\n",
    "      runs-on: ubuntu-latest\n",
    "      steps:\n",
    "        - uses: actions/checkout@v2\n",
    "        - name: Run tests\n",
    "          run: pytest\n",
    "    deploy:\n",
    "      needs: test\n",
    "      if: github.ref == 'refs/heads/main'\n",
    "      steps:\n",
    "        - name: Deploy to production\n",
    "          run: deploy-script.sh\n",
    "  ```\n",
    "- **Best Practices**:\n",
    "  - Automate testing at multiple levels\n",
    "  - Environment-specific configurations\n",
    "  - Implement quality gates\n",
    "  - Monitor deployment health\n",
    "\n",
    "## 7. MLFlow\n",
    "\n",
    "- Open-source platform for managing the ML lifecycle\n",
    "- **Key Components**:\n",
    "  - Tracking: Log parameters, metrics, artifacts\n",
    "  - Projects: Package code for reproducibility\n",
    "  - Models: Package models for deployment\n",
    "  - Registry: Store and version models\n",
    "- **Basic Usage**:\n",
    "  ```python\n",
    "  import mlflow\n",
    "  \n",
    "  with mlflow.start_run():\n",
    "      mlflow.log_param(\"param\", value)\n",
    "      mlflow.log_metric(\"metric\", value)\n",
    "      mlflow.sklearn.log_model(model, \"model\")\n",
    "  ```\n",
    "- **Benefits**: Experiment tracking, reproducibility, model versioning, deployment management\n",
    "\n",
    "## 8. ML System Design\n",
    "\n",
    "- **Key Principles**: Scalability, reliability, maintainability, adaptability\n",
    "- **Architecture Patterns**:\n",
    "  - Layered architecture (data → features → model → serving)\n",
    "  - Microservice architecture for ML components\n",
    "- **Components**:\n",
    "  - Data collection and preprocessing\n",
    "  - Feature store for reliable feature access\n",
    "  - Model training and evaluation\n",
    "  - Model serving (batch/real-time)\n",
    "  - Monitoring and feedback loops\n",
    "- **Best Practices**:\n",
    "  - Start simple, iterate fast\n",
    "  - Design for failure\n",
    "  - Make reproducibility a priority\n",
    "  - Monitor from day one\n",
    "  - Consider security and privacy\n",
    "\n",
    "## 9. ML Pipelines with SageMaker\n",
    "\n",
    "- **Amazon SageMaker**: Fully managed service for building, training, and deploying ML models\n",
    "- **Key Components**:\n",
    "  - SageMaker Studio: Integrated development environment\n",
    "  - Processing: Data preparation at scale\n",
    "  - Training: Distributed model training\n",
    "  - Model Registry: Version control for models\n",
    "  - Endpoints: Deploy models for real-time inference\n",
    "- **Pipeline Steps**:\n",
    "  1. Data preparation/processing\n",
    "  2. Feature engineering\n",
    "  3. Model training\n",
    "  4. Model evaluation\n",
    "  5. Model registration\n",
    "  6. Model deployment\n",
    "- **Benefits**: Managed infrastructure, scalable training, simplified deployment\n",
    "\n",
    "## 10. Processing Large-Scale Data with Apache Spark\n",
    "\n",
    "- **PySpark**: Python API for Apache Spark distributed computing framework\n",
    "- **Key Components**:\n",
    "  - SparkSession: Entry point for DataFrame and SQL operations\n",
    "  - DataFrame: Distributed collection of data organized in named columns\n",
    "  - RDD: Low-level distributed collection of objects\n",
    "- **Basic Operations**:\n",
    "  ```python\n",
    "  spark = SparkSession.builder.getOrCreate()\n",
    "  df = spark.read.csv(\"data.csv\", header=True)\n",
    "  df.select(\"col1\", \"col2\").filter(df.col1 > 100).show()\n",
    "  ```\n",
    "- **Performance Tips**:\n",
    "  - Use appropriate file formats (Parquet/ORC)\n",
    "  - Cache strategically with `persist()`\n",
    "  - Partition data properly\n",
    "  - Broadcast small tables in joins\n",
    "  - Configure resources appropriately\n",
    "- **Use Cases**: ETL, large-scale ML, log analysis, real-time processing\n",
    "\n",
    "These technologies form the foundation of modern MLOps, enabling teams to build, deploy, and maintain machine learning systems at scale with efficiency and reliability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Git and GitHub - MLOps Essential Notes\n",
    "\n",
    "## Key Concepts Comparison\n",
    "\n",
    "| Feature | Git | GitHub |\n",
    "|---------|-----|--------|\n",
    "| Definition | Distributed version control system | Web-based hosting service for Git repositories |\n",
    "| Purpose | Tracks changes in source code during development | Provides cloud storage, collaboration tools, and CI/CD integration |\n",
    "| Scope | Local environment (on your machine) | Remote environment (on the cloud) |\n",
    "| Access | Via command line or GUI clients | Via web browser or desktop client |\n",
    "| Privacy | Local, not visible to others unless pushed | Public or private repositories with access control |\n",
    "| Functionality | Version control operations (commit, branch, merge) | Additional features: Issues, Pull Requests, Actions, Projects |\n",
    "\n",
    "## Git Workflow ⚠️ IMPORTANT\n",
    "\n",
    "```\n",
    "Working Directory → Staging Area → Local Repository → Remote Repository\n",
    "```\n",
    "\n",
    "![Git Workflow](https://example.com/image.png)\n",
    "\n",
    "## Essential Git Commands\n",
    "\n",
    "| Command | Purpose | Usage Example |\n",
    "|---------|---------|---------------|\n",
    "| `git init` | Initialize a new repository | `git init` (Run ONCE per project) |\n",
    "| `git add` | Stage changes | `git add .` (all files) or `git add file.txt` (specific file) |\n",
    "| `git commit` | Record changes | `git commit -m \"Added feature X\"` |\n",
    "| `git status` | Check current state | `git status` |\n",
    "| `git log` | View commit history | `git log` or `git log --oneline` |\n",
    "| `git push` | Upload to remote | `git push origin main` |\n",
    "| `git pull` | Download from remote | `git pull origin main` |\n",
    "| `git branch` | Create/list branches | `git branch feature-x` |\n",
    "| `git checkout` | Switch branches | `git checkout feature-x` |\n",
    "| `git merge` | Combine branches | `git merge feature-x` |\n",
    "| `git clone` | Copy a repository | `git clone https://github.com/user/repo.git` |\n",
    "\n",
    "## ⚠️ REMEMBER THIS\n",
    "\n",
    "1. Always **pull before you push** to avoid merge conflicts\n",
    "2. Write **meaningful commit messages** to track changes effectively\n",
    "3. Create **new branches** for new features or bug fixes\n",
    "4. **Never commit** sensitive data (API keys, passwords, tokens)\n",
    "5. Use `.gitignore` to exclude unnecessary files (virtual environments, build directories, etc.)\n",
    "\n",
    "## Branching Strategy for ML Projects\n",
    "\n",
    "```\n",
    "main (production) → development → feature branches\n",
    "```\n",
    "\n",
    "- **main**: Stable production code\n",
    "- **development**: Integration branch for testing\n",
    "- **feature-x/model-training**: Individual feature development\n",
    "- **hotfix-x**: Emergency fixes for production\n",
    "\n",
    "## Git for Data Science Workflows\n",
    "\n",
    "- **Model versioning**: Track changes to model architecture and hyperparameters\n",
    "- **Dataset versioning**: Use Git LFS (Large File Storage) or DVC (Data Version Control) for large datasets\n",
    "- **Experiment tracking**: Use commit messages to document experiment results\n",
    "- **Collaboration**: Multiple data scientists can work on different features simultaneously\n",
    "\n",
    "## GitHub Features for ML Teams\n",
    "\n",
    "- **Issues**: Track bugs, enhancements, and tasks\n",
    "- **Pull Requests**: Code review and discussion\n",
    "- **Actions**: Automated workflows (CI/CD, model training, testing)\n",
    "- **Projects**: Kanban boards for project management\n",
    "- **Packages**: Host model artifacts and dependencies\n",
    "\n",
    "## Common Git Mistakes and Solutions\n",
    "\n",
    "| Mistake | Solution |\n",
    "|---------|----------|\n",
    "| Committed to wrong branch | `git reset HEAD~1` followed by `git stash`, then switch branch |\n",
    "| Large dataset committed | Set up Git LFS or DVC, then use `git filter-branch` to remove history |\n",
    "| Bad commit message | `git commit --amend` to edit most recent commit message |\n",
    "| Merge conflicts | Resolve conflicts in code editor, then `git add` and `git commit` |\n",
    "| Accidentally deleted work | `git reflog` to find and restore lost commits |\n",
    "\n",
    "## Advanced Git Commands for ML Projects\n",
    "\n",
    "```bash\n",
    "# Create a new branch for a model experiment\n",
    "git checkout -b experiment/xgboost-tuning\n",
    "\n",
    "# See differences in model configuration files\n",
    "git diff model_config.yaml\n",
    "\n",
    "# Temporarily save work in progress\n",
    "git stash save \"halfway through hyperparameter tuning\"\n",
    "\n",
    "# Apply saved work later\n",
    "git stash apply\n",
    "\n",
    "# Tag a specific model version\n",
    "git tag -a \"model-v1.0\" -m \"First production model\"\n",
    "\n",
    "# Create a patch for specific changes\n",
    "git format-patch main --stdout > model-fix.patch\n",
    "```\n",
    "\n",
    "This should provide a solid foundation for Git and GitHub in the context of MLOps. Let me know if you'd like me to expand on any specific area or if you're ready to move on to the next topic!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Streamlit for ML Applications\n",
    "\n",
    "## What is Streamlit?\n",
    "\n",
    "Streamlit is an open-source Python framework for building interactive web applications for data science and machine learning with minimal effort and no front-end experience required.\n",
    "\n",
    "## Key Features Comparison: Streamlit vs Other Web Frameworks\n",
    "\n",
    "| Feature | Streamlit | Flask/Django | Dash |\n",
    "|---------|-----------|--------------|------|\n",
    "| Learning Curve | Very Low (Python-only) | Moderate-High (Python + HTML/CSS/JS) | Moderate (Python + React) |\n",
    "| Setup Time | Minutes | Hours/Days | Hours |\n",
    "| Primary Use Case | Data Apps & ML Prototypes | Full Web Applications | Data Dashboards |\n",
    "| State Management | Automatic (Reactive) | Manual | Callback-based |\n",
    "| Customization | Limited but growing | Unlimited | Moderate |\n",
    "| Deployment | Easy (Streamlit Cloud) | Complex | Moderate (Dash Enterprise) |\n",
    "| Authentication | Limited built-in | Extensive options | Available in paid tier |\n",
    "| API Creation | Not supported | Well-supported | Limited |\n",
    "\n",
    "## ⚠️ Critical Streamlit Concepts\n",
    "\n",
    "1. **Reactive Execution**: The entire script reruns on any interaction\n",
    "2. **Caching**: Use `@st.cache_data` and `@st.cache_resource` to prevent redundant computations\n",
    "3. **App Flow**: Top-to-bottom execution model (unlike traditional web apps)\n",
    "4. **Statelessness**: Variables don't persist between interactions (use session state)\n",
    "\n",
    "## Basic Elements\n",
    "\n",
    "```python\n",
    "# Core display elements\n",
    "st.title(\"My ML Application\")\n",
    "st.header(\"Model Performance\")\n",
    "st.subheader(\"Training Results\")\n",
    "st.text(\"Plain text explanation\")\n",
    "st.markdown(\"**Bold** or *italic* formatting\")\n",
    "st.code(\"import pandas as pd\", language=\"python\")\n",
    "st.latex(r'''e^{i\\pi} + 1 = 0''')\n",
    "\n",
    "# Alerts and notifications\n",
    "st.success(\"Model trained successfully!\")\n",
    "st.info(\"Processing your data...\")\n",
    "st.warning(\"Missing values detected\")\n",
    "st.error(\"Training failed\")\n",
    "st.exception(Exception(\"Out of memory error\"))\n",
    "```\n",
    "\n",
    "## Interactive Widgets for ML Apps\n",
    "\n",
    "```python\n",
    "# Input widgets\n",
    "name = st.text_input(\"Dataset name\")\n",
    "description = st.text_area(\"Description\")\n",
    "number = st.number_input(\"Number of iterations\", min_value=1, max_value=1000)\n",
    "model_type = st.selectbox(\"Select model\", [\"Linear Regression\", \"Random Forest\", \"XGBoost\"])\n",
    "features = st.multiselect(\"Select features\", [\"age\", \"gender\", \"income\", \"location\"])\n",
    "learning_rate = st.slider(\"Learning rate\", min_value=0.001, max_value=0.1, step=0.001)\n",
    "is_normalize = st.checkbox(\"Normalize data\")\n",
    "train_button = st.button(\"Train Model\")\n",
    "\n",
    "# File uploads\n",
    "uploaded_file = st.file_uploader(\"Upload CSV\", type=[\"csv\"])\n",
    "if uploaded_file is not None:\n",
    "    df = pd.read_csv(uploaded_file)\n",
    "    st.dataframe(df.head())\n",
    "```\n",
    "\n",
    "## Data Visualization in Streamlit\n",
    "\n",
    "```python\n",
    "# Displaying data\n",
    "st.dataframe(df)  # Interactive table\n",
    "st.table(df)      # Static table\n",
    "st.json(data)     # JSON viewer\n",
    "\n",
    "# Charts\n",
    "st.line_chart(df)\n",
    "st.area_chart(df)\n",
    "st.bar_chart(df)\n",
    "\n",
    "# Matplotlib integration\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y)\n",
    "st.pyplot(fig)\n",
    "\n",
    "# Plotly integration\n",
    "fig = px.scatter(df, x=\"feature1\", y=\"feature2\", color=\"target\")\n",
    "st.plotly_chart(fig)\n",
    "\n",
    "# Maps (for geospatial ML)\n",
    "st.map(df[df['lat'] and df['lon']])\n",
    "```\n",
    "\n",
    "## ⚠️ Performance Optimization\n",
    "\n",
    "```python\n",
    "# Cache expensive computations (CRITICAL for ML apps)\n",
    "@st.cache_data\n",
    "def load_data():\n",
    "    df = pd.read_csv(\"large_dataset.csv\")\n",
    "    return df\n",
    "\n",
    "@st.cache_resource\n",
    "def train_model(params):\n",
    "    model = RandomForest(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "# Session state for preserving values between reruns\n",
    "if 'model' not in st.session_state:\n",
    "    st.session_state.model = None\n",
    "\n",
    "if st.button(\"Train\"):\n",
    "    st.session_state.model = train_model(params)\n",
    "```\n",
    "\n",
    "## ML App Structure Pattern\n",
    "\n",
    "```python\n",
    "# 1. Setup and Configuration\n",
    "st.title(\"ML Model Explorer\")\n",
    "st.sidebar.header(\"Parameters\")\n",
    "\n",
    "# 2. Data Loading (cached)\n",
    "@st.cache_data\n",
    "def load_data():\n",
    "    return pd.read_csv(\"data.csv\")\n",
    "\n",
    "df = load_data()\n",
    "\n",
    "# 3. Feature Selection (in sidebar)\n",
    "features = st.sidebar.multiselect(\"Features\", df.columns)\n",
    "target = st.sidebar.selectbox(\"Target\", df.columns)\n",
    "\n",
    "# 4. Model Configuration\n",
    "model_type = st.sidebar.selectbox(\"Model\", [\"Linear\", \"Tree\", \"Ensemble\"])\n",
    "params = {}\n",
    "if model_type == \"Linear\":\n",
    "    params['alpha'] = st.sidebar.slider(\"Alpha\", 0.01, 10.0)\n",
    "elif model_type == \"Tree\":\n",
    "    params['max_depth'] = st.sidebar.slider(\"Max Depth\", 1, 20)\n",
    "\n",
    "# 5. Training and Evaluation\n",
    "if st.button(\"Train Model\"):\n",
    "    with st.spinner(\"Training in progress...\"):\n",
    "        X = df[features]\n",
    "        y = df[target]\n",
    "        model = train_model(X, y, model_type, params)\n",
    "        metrics = evaluate_model(model, X, y)\n",
    "        \n",
    "        # Display results\n",
    "        st.success(\"Training complete!\")\n",
    "        st.metric(\"R² Score\", f\"{metrics['r2']:.4f}\")\n",
    "        \n",
    "        # Plot predictions\n",
    "        fig = plot_predictions(model, X, y)\n",
    "        st.pyplot(fig)\n",
    "```\n",
    "\n",
    "## Deployment Options\n",
    "\n",
    "| Option | Description | Best For |\n",
    "|--------|-------------|----------|\n",
    "| Streamlit Cloud | Free hosting for public GitHub repos | Public demos, portfolios |\n",
    "| Heroku | Platform-as-a-Service | Small to medium-scale apps |\n",
    "| AWS/GCP/Azure | Cloud providers with full infrastructure | Production ML applications |\n",
    "| Docker | Containerized deployment | Team environments, CI/CD pipelines |\n",
    "\n",
    "## Running Commands\n",
    "\n",
    "```bash\n",
    "# Local development\n",
    "streamlit run app.py\n",
    "\n",
    "# With specific port\n",
    "streamlit run app.py --server.port 8080\n",
    "\n",
    "# With server address (for network access)\n",
    "streamlit run app.py --server.address 0.0.0.0\n",
    "```\n",
    "\n",
    "## Best Practices for ML Apps\n",
    "\n",
    "1. **Structure your code** - Organize into functions and modules\n",
    "2. **Cache computation-heavy operations** - Especially data loading, preprocessing, and model training\n",
    "3. **Use session state wisely** - For storing model objects, user preferences, and app state\n",
    "4. **Implement progress indicators** - Use `st.progress()` for long-running tasks\n",
    "5. **Handle errors gracefully** - Wrap model training in try/except blocks\n",
    "6. **Add download capabilities** - Let users download trained models or predictions\n",
    "7. **Design with mobile in mind** - Use columns and responsive layouts\n",
    "8. **Separate UI from computation** - Keep business logic independent from UI code\n",
    "\n",
    "## Example: Complete ML App\n",
    "\n",
    "```python\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "st.set_page_config(page_title=\"ML Model Trainer\", layout=\"wide\")\n",
    "\n",
    "@st.cache_data\n",
    "def load_data(file):\n",
    "    return pd.read_csv(file)\n",
    "\n",
    "@st.cache_resource\n",
    "def train_model(_X_train, _y_train, n_estimators, max_depth):\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(_X_train, _y_train)\n",
    "    return model\n",
    "\n",
    "# Sidebar for navigation and parameters\n",
    "st.sidebar.title(\"Navigation\")\n",
    "page = st.sidebar.radio(\"Go to\", [\"Data Explorer\", \"Model Training\", \"Model Evaluation\"])\n",
    "\n",
    "# File uploader\n",
    "uploaded_file = st.sidebar.file_uploader(\"Upload CSV file\", type=[\"csv\"])\n",
    "\n",
    "if uploaded_file is not None:\n",
    "    # Load data\n",
    "    df = load_data(uploaded_file)\n",
    "    \n",
    "    if page == \"Data Explorer\":\n",
    "        st.title(\"Data Explorer\")\n",
    "        \n",
    "        # Display dataset info\n",
    "        st.subheader(\"Dataset Overview\")\n",
    "        st.write(f\"Rows: {df.shape[0]}, Columns: {df.shape[1]}\")\n",
    "        \n",
    "        col1, col2 = st.columns(2)\n",
    "        with col1:\n",
    "            st.subheader(\"First 5 rows\")\n",
    "            st.dataframe(df.head())\n",
    "        with col2:\n",
    "            st.subheader(\"Statistical Summary\")\n",
    "            st.dataframe(df.describe())\n",
    "        \n",
    "        # Data visualization\n",
    "        st.subheader(\"Data Visualization\")\n",
    "        chart_type = st.selectbox(\"Select Chart Type\", [\"Histogram\", \"Correlation\", \"Boxplot\"])\n",
    "        \n",
    "        if chart_type == \"Histogram\":\n",
    "            col = st.selectbox(\"Select Column\", df.select_dtypes(include=[np.number]).columns)\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.hist(df[col], bins=30)\n",
    "            ax.set_title(f\"Histogram of {col}\")\n",
    "            st.pyplot(fig)\n",
    "            \n",
    "        elif chart_type == \"Correlation\":\n",
    "            corr = df.select_dtypes(include=[np.number]).corr()\n",
    "            fig, ax = plt.subplots(figsize=(10, 8))\n",
    "            sns.heatmap(corr, annot=True, cmap='coolwarm', ax=ax)\n",
    "            st.pyplot(fig)\n",
    "            \n",
    "        elif chart_type == \"Boxplot\":\n",
    "            col = st.selectbox(\"Select Column\", df.select_dtypes(include=[np.number]).columns)\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.boxplot(df[col])\n",
    "            ax.set_title(f\"Boxplot of {col}\")\n",
    "            st.pyplot(fig)\n",
    "    \n",
    "    elif page == \"Model Training\":\n",
    "        st.title(\"Model Training\")\n",
    "        \n",
    "        # Feature selection\n",
    "        st.subheader(\"Feature Selection\")\n",
    "        feature_cols = st.multiselect(\"Select Features\", df.columns)\n",
    "        target_col = st.selectbox(\"Select Target\", df.columns)\n",
    "        \n",
    "        # Model parameters\n",
    "        st.subheader(\"Model Parameters\")\n",
    "        n_estimators = st.slider(\"Number of Trees\", 10, 500, 100)\n",
    "        max_depth = st.slider(\"Max Depth\", 1, 30, 10)\n",
    "        test_size = st.slider(\"Test Size\", 0.1, 0.5, 0.2)\n",
    "        \n",
    "        if st.button(\"Train Model\") and feature_cols and target_col:\n",
    "            if len(feature_cols) > 0:\n",
    "                # Split data\n",
    "                X = df[feature_cols]\n",
    "                y = df[target_col]\n",
    "                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "                \n",
    "                # Train model with progress bar\n",
    "                with st.spinner(\"Training model...\"):\n",
    "                    model = train_model(X_train, y_train, n_estimators, max_depth)\n",
    "                \n",
    "                # Store results in session state\n",
    "                st.session_state.model = model\n",
    "                st.session_state.X_test = X_test\n",
    "                st.session_state.y_test = y_test\n",
    "                st.session_state.feature_cols = feature_cols\n",
    "                \n",
    "                st.success(\"Model trained successfully!\")\n",
    "            else:\n",
    "                st.error(\"Please select at least one feature\")\n",
    "    \n",
    "    elif page == \"Model Evaluation\":\n",
    "        st.title(\"Model Evaluation\")\n",
    "        \n",
    "        if 'model' not in st.session_state:\n",
    "            st.warning(\"Please train a model first\")\n",
    "        else:\n",
    "            # Get predictions\n",
    "            model = st.session_state.model\n",
    "            X_test = st.session_state.X_test\n",
    "            y_test = st.session_state.y_test\n",
    "            \n",
    "            y_pred = model.predict(X_test)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "            recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "            \n",
    "            # Display metrics\n",
    "            col1, col2, col3 = st.columns(3)\n",
    "            col1.metric(\"Accuracy\", f\"{accuracy:.4f}\")\n",
    "            col2.metric(\"Precision\", f\"{precision:.4f}\")\n",
    "            col3.metric(\"Recall\", f\"{recall:.4f}\")\n",
    "            \n",
    "            # Feature importance\n",
    "            st.subheader(\"Feature Importance\")\n",
    "            feature_cols = st.session_state.feature_cols\n",
    "            importances = model.feature_importances_\n",
    "            indices = np.argsort(importances)[::-1]\n",
    "            \n",
    "            fig, ax = plt.subplots()\n",
    "            ax.bar(range(len(importances)), importances[indices])\n",
    "            ax.set_xticks(range(len(importances)))\n",
    "            ax.set_xticklabels([feature_cols[i] for i in indices], rotation=90)\n",
    "            ax.set_title(\"Feature Importance\")\n",
    "            st.pyplot(fig)\n",
    "            \n",
    "            # Let user download the model\n",
    "            model_pkl = pickle.dumps(model)\n",
    "            st.download_button(\"Download Model\", model_pkl, \"model.pkl\")\n",
    "else:\n",
    "    st.info(\"Please upload a CSV file to get started\")\n",
    "```\n",
    "\n",
    "Remember that Streamlit is ideal for quickly building ML prototypes and internal tools, but for production-grade applications with high security requirements, you may need to consider more robust frameworks like Flask or FastAPI with Streamlit serving as the front-end component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Web APIs using Flask for ML Applications\n",
    "\n",
    "## What are Web APIs?\n",
    "\n",
    "Web APIs (Application Programming Interfaces) allow different software systems to communicate over the internet. In the context of ML applications, APIs enable:\n",
    "\n",
    "- Serving ML model predictions\n",
    "- Accepting data for processing\n",
    "- Sharing ML resources across systems\n",
    "- Creating scalable, production-ready ML services\n",
    "\n",
    "## HTTP Protocol Fundamentals\n",
    "\n",
    "| Method | Purpose | Example in ML Context |\n",
    "|--------|---------|------------------------|\n",
    "| GET | Retrieve data | Get model predictions, fetch model metadata |\n",
    "| POST | Send data | Submit data for prediction, upload training data |\n",
    "| PUT | Update data | Update model parameters |\n",
    "| DELETE | Remove data | Delete a model from registry |\n",
    "\n",
    "## Flask Framework Overview\n",
    "\n",
    "Flask is a lightweight Python web framework ideal for creating APIs for ML models because:\n",
    "\n",
    "- **Simple & minimal**: Quick to set up with minimal boilerplate\n",
    "- **Flexible**: Doesn't enforce a particular project structure\n",
    "- **Python-native**: Integrates easily with ML libraries (scikit-learn, TensorFlow, etc.)\n",
    "- **Extensible**: Add functionality with extensions like Flask-RESTful\n",
    "\n",
    "## Setting Up a Basic Flask API\n",
    "\n",
    "```python\n",
    "from flask import Flask\n",
    "\n",
    "# Initialize the Flask application\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Define a simple endpoint\n",
    "@app.route(\"/ping\", methods=['GET'])\n",
    "def ping():\n",
    "    return {\"message\": \"Hi there, I'm working!!\"}\n",
    "\n",
    "# Run the application\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True)\n",
    "```\n",
    "\n",
    "**Code Explanation:**\n",
    "1. `from flask import Flask` - Import the Flask class\n",
    "2. `app = Flask(__name__)` - Create an instance of the Flask class, where `__name__` is a Python predefined variable representing the module name\n",
    "3. `@app.route(\"/ping\", methods=['GET'])` - A decorator that maps the URL \"/ping\" to the function below it, accepting only GET requests\n",
    "4. `def ping():` - The function that will be executed when someone visits the \"/ping\" URL\n",
    "5. `return {\"message\": \"Hi there, I'm working!!\"}` - Return a JSON response\n",
    "6. `app.run(debug=True)` - Start the server with debug mode enabled\n",
    "\n",
    "## Creating a ML Model Prediction API\n",
    "\n",
    "```python\n",
    "from flask import Flask, request, jsonify\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Initialize Flask app\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Load the pre-trained model\n",
    "with open('model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    # Get JSON data from the request\n",
    "    data = request.get_json(force=True)\n",
    "    \n",
    "    # Extract features from the request\n",
    "    features = data['features']\n",
    "    \n",
    "    # Convert to numpy array and reshape if needed\n",
    "    features_array = np.array(features).reshape(1, -1)\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = model.predict(features_array)\n",
    "    \n",
    "    # Return prediction as JSON\n",
    "    return jsonify({\n",
    "        'status': 'success',\n",
    "        'prediction': prediction.tolist(),\n",
    "        'probability': model.predict_proba(features_array).tolist() if hasattr(model, 'predict_proba') else None\n",
    "    })\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True, host='0.0.0.0', port=5000)\n",
    "```\n",
    "\n",
    "**Code Explanation:**\n",
    "1. `from flask import Flask, request, jsonify` - Import Flask and additional modules:\n",
    "   - `request`: Handles incoming HTTP requests\n",
    "   - `jsonify`: Converts Python objects to JSON responses\n",
    "2. `with open('model.pkl', 'rb') as f:` - Open the pickled model file in binary read mode\n",
    "3. `model = pickle.load(f)` - Deserialize the model from the file\n",
    "4. `@app.route('/predict', methods=['POST'])` - Create an endpoint at \"/predict\" that accepts POST requests\n",
    "5. `data = request.get_json(force=True)` - Parse the JSON body of the request\n",
    "6. `features = data['features']` - Extract the features from the request data\n",
    "7. `features_array = np.array(features).reshape(1, -1)` - Convert to numpy array and ensure correct shape\n",
    "8. `prediction = model.predict(features_array)` - Use the model to make a prediction\n",
    "9. `return jsonify({...})` - Return the prediction results as JSON\n",
    "10. `app.run(debug=True, host='0.0.0.0', port=5000)` - Run the server, binding to all interfaces (0.0.0.0) on port 5000\n",
    "\n",
    "## Building a More Complete ML API\n",
    "\n",
    "```python\n",
    "from flask import Flask, request, jsonify\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize Flask app\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Load the model and preprocessing components\n",
    "try:\n",
    "    with open('model.pkl', 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    with open('scaler.pkl', 'rb') as f:\n",
    "        scaler = pickle.load(f)\n",
    "    \n",
    "    logger.info(\"Model and scaler loaded successfully\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error loading model: {str(e)}\")\n",
    "    model = None\n",
    "    scaler = None\n",
    "\n",
    "# Health check endpoint\n",
    "@app.route('/health', methods=['GET'])\n",
    "def health():\n",
    "    if model is not None:\n",
    "        return jsonify({\n",
    "            'status': 'healthy',\n",
    "            'model_loaded': True,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        })\n",
    "    else:\n",
    "        return jsonify({\n",
    "            'status': 'unhealthy',\n",
    "            'model_loaded': False,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }), 503  # Service Unavailable status code\n",
    "\n",
    "# Model metadata endpoint\n",
    "@app.route('/metadata', methods=['GET'])\n",
    "def metadata():\n",
    "    if model is None:\n",
    "        return jsonify({'error': 'Model not loaded'}), 503\n",
    "    \n",
    "    return jsonify({\n",
    "        'model_type': type(model).__name__,\n",
    "        'features': model.feature_names_in_.tolist() if hasattr(model, 'feature_names_in_') else None,\n",
    "        'target': model.classes_.tolist() if hasattr(model, 'classes_') else None,\n",
    "        'version': '1.0.0'  # Version tracking is important in ML\n",
    "    })\n",
    "\n",
    "# Prediction endpoint\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    if model is None or scaler is None:\n",
    "        return jsonify({'error': 'Model or scaler not loaded'}), 503\n",
    "    \n",
    "    # Log request time\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    try:\n",
    "        # Get JSON data\n",
    "        data = request.get_json(force=True)\n",
    "        \n",
    "        # Validate input\n",
    "        if 'features' not in data:\n",
    "            return jsonify({'error': 'No features provided'}), 400\n",
    "        \n",
    "        # Extract features and convert to DataFrame to ensure correct format\n",
    "        input_features = pd.DataFrame([data['features']])\n",
    "        \n",
    "        # Preprocess data\n",
    "        scaled_features = scaler.transform(input_features)\n",
    "        \n",
    "        # Make prediction\n",
    "        prediction = model.predict(scaled_features)\n",
    "        probabilities = model.predict_proba(scaled_features) if hasattr(model, 'predict_proba') else None\n",
    "        \n",
    "        # Calculate processing time\n",
    "        processing_time = (datetime.now() - start_time).total_seconds()\n",
    "        \n",
    "        # Log successful prediction\n",
    "        logger.info(f\"Prediction made in {processing_time} seconds\")\n",
    "        \n",
    "        # Return prediction\n",
    "        response = {\n",
    "            'status': 'success',\n",
    "            'prediction': prediction.tolist(),\n",
    "            'prediction_label': prediction[0] if isinstance(prediction[0], (int, str)) else prediction[0].item(),\n",
    "            'probability': probabilities.tolist() if probabilities is not None else None,\n",
    "            'processing_time_seconds': processing_time\n",
    "        }\n",
    "        \n",
    "        return jsonify(response)\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Log error\n",
    "        logger.error(f\"Prediction error: {str(e)}\")\n",
    "        \n",
    "        # Return error message\n",
    "        return jsonify({\n",
    "            'status': 'error',\n",
    "            'error': str(e),\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }), 500\n",
    "\n",
    "# Batch prediction endpoint\n",
    "@app.route('/batch-predict', methods=['POST'])\n",
    "def batch_predict():\n",
    "    if model is None or scaler is None:\n",
    "        return jsonify({'error': 'Model or scaler not loaded'}), 503\n",
    "    \n",
    "    try:\n",
    "        # Get JSON data\n",
    "        data = request.get_json(force=True)\n",
    "        \n",
    "        # Validate input\n",
    "        if 'instances' not in data:\n",
    "            return jsonify({'error': 'No instances provided'}), 400\n",
    "        \n",
    "        # Extract features and convert to DataFrame\n",
    "        batch_features = pd.DataFrame(data['instances'])\n",
    "        \n",
    "        # Preprocess data\n",
    "        scaled_batch = scaler.transform(batch_features)\n",
    "        \n",
    "        # Make batch predictions\n",
    "        predictions = model.predict(scaled_batch)\n",
    "        probabilities = model.predict_proba(scaled_batch) if hasattr(model, 'predict_proba') else None\n",
    "        \n",
    "        # Return batch predictions\n",
    "        return jsonify({\n",
    "            'status': 'success',\n",
    "            'predictions': predictions.tolist(),\n",
    "            'probabilities': probabilities.tolist() if probabilities is not None else None\n",
    "        })\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Log error\n",
    "        logger.error(f\"Batch prediction error: {str(e)}\")\n",
    "        \n",
    "        # Return error message\n",
    "        return jsonify({\n",
    "            'status': 'error',\n",
    "            'error': str(e)\n",
    "        }), 500\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True, host='0.0.0.0', port=5000)\n",
    "```\n",
    "\n",
    "**Code Explanation:**\n",
    "\n",
    "1. **Logging Setup**: \n",
    "   - `logging.basicConfig()` - Configures the logging system\n",
    "   - `logger = logging.getLogger(__name__)` - Creates a logger instance specific to this module\n",
    "\n",
    "2. **Model Loading**:\n",
    "   - We use a try-except block to handle potential errors when loading models\n",
    "   - Both the model and preprocessing components (scaler) are loaded\n",
    "\n",
    "3. **Health Check Endpoint** (`/health`):\n",
    "   - Provides system status information\n",
    "   - Returns HTTP 503 if the model isn't loaded correctly\n",
    "   - Useful for monitoring and alerting systems\n",
    "\n",
    "4. **Metadata Endpoint** (`/metadata`):\n",
    "   - Exposes information about the model\n",
    "   - Helps clients understand what features to send and what outputs to expect\n",
    "   - Includes version information which is critical for ML model tracking\n",
    "\n",
    "5. **Prediction Endpoint** (`/predict`):\n",
    "   - Validates incoming data\n",
    "   - Preprocesses features using the scaler\n",
    "   - Makes predictions and returns results\n",
    "   - Includes error handling and timing metrics\n",
    "\n",
    "6. **Batch Prediction Endpoint** (`/batch-predict`):\n",
    "   - Processes multiple instances at once\n",
    "   - More efficient for high-throughput scenarios\n",
    "\n",
    "## Testing Your Flask API\n",
    "\n",
    "### Using curl\n",
    "\n",
    "```bash\n",
    "# Test health endpoint\n",
    "curl -X GET http://localhost:5000/health\n",
    "\n",
    "# Test prediction endpoint\n",
    "curl -X POST http://localhost:5000/predict \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\"features\": [5.1, 3.5, 1.4, 0.2]}'\n",
    "\n",
    "# Test batch prediction\n",
    "curl -X POST http://localhost:5000/batch-predict \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\"instances\": [[5.1, 3.5, 1.4, 0.2], [6.2, 3.4, 5.4, 2.3]]}'\n",
    "```\n",
    "\n",
    "### Using Python requests\n",
    "\n",
    "```python\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Base URL\n",
    "base_url = \"http://localhost:5000\"\n",
    "\n",
    "# Test health endpoint\n",
    "response = requests.get(f\"{base_url}/health\")\n",
    "print(\"Health check:\", response.json())\n",
    "\n",
    "# Test prediction endpoint\n",
    "features = [5.1, 3.5, 1.4, 0.2]  # Example features\n",
    "response = requests.post(\n",
    "    f\"{base_url}/predict\",\n",
    "    json={\"features\": features}\n",
    ")\n",
    "print(\"Prediction:\", response.json())\n",
    "\n",
    "# Test batch prediction\n",
    "features_batch = [\n",
    "    [5.1, 3.5, 1.4, 0.2],\n",
    "    [6.2, 3.4, 5.4, 2.3]\n",
    "]\n",
    "response = requests.post(\n",
    "    f\"{base_url}/batch-predict\",\n",
    "    json={\"instances\": features_batch}\n",
    ")\n",
    "print(\"Batch prediction:\", response.json())\n",
    "```\n",
    "\n",
    "## Best Practices for ML APIs\n",
    "\n",
    "1. **Input Validation**: Always validate incoming data before processing\n",
    "2. **Error Handling**: Use try-except blocks and return appropriate HTTP status codes\n",
    "3. **Logging**: Log predictions, errors, and performance metrics\n",
    "4. **Versioning**: Include version information in your API endpoints\n",
    "5. **Monitoring**: Implement health checks and performance tracking\n",
    "6. **Documentation**: Document expected inputs, outputs, and error codes\n",
    "7. **Security**: Implement authentication for production APIs\n",
    "8. **Rate Limiting**: Protect your API from abuse with rate limits\n",
    "\n",
    "## Flask Extensions for ML APIs\n",
    "\n",
    "1. **Flask-RESTful**: Simplifies building REST APIs\n",
    "2. **Flask-CORS**: Handles Cross-Origin Resource Sharing\n",
    "3. **Flask-JWT-Extended**: Adds JWT authentication\n",
    "4. **Flask-Limiter**: Implements rate limiting\n",
    "5. **Flask-Caching**: Caches API responses for better performance\n",
    "\n",
    "## API Structure Patterns for ML Applications\n",
    "\n",
    "### Monolithic Pattern\n",
    "```\n",
    "/predict         - Single endpoint for all models\n",
    "```\n",
    "\n",
    "### Resource-Based Pattern\n",
    "```\n",
    "/models          - List available models\n",
    "/models/<id>     - Get model details\n",
    "/models/<id>/predict - Get prediction from specific model\n",
    "```\n",
    "\n",
    "### Versioned Pattern\n",
    "```\n",
    "/v1/predict      - Version 1 of prediction API\n",
    "/v2/predict      - Version 2 with different features\n",
    "```\n",
    "\n",
    "## ⚠️ Common Pitfalls to Avoid\n",
    "\n",
    "1. **Reloading the model on every request** - Load once at startup\n",
    "2. **Not handling high-dimensional or categorical data properly**\n",
    "3. **Missing input validation**\n",
    "4. **Returning raw numpy arrays** - Convert to lists with `.tolist()`\n",
    "5. **Not handling model failures gracefully**\n",
    "6. **Performance bottlenecks** - Consider async processing for long-running predictions\n",
    "\n",
    "## Production Deployment Considerations\n",
    "\n",
    "1. **WSGI Server**: Use Gunicorn or uWSGI in production, not Flask's built-in server\n",
    "2. **Load Balancing**: Distribute traffic across multiple API instances\n",
    "3. **Authentication**: Implement API keys or OAuth\n",
    "4. **Containerization**: Package your API in Docker for consistent deployment\n",
    "5. **Monitoring**: Track API usage, performance, and model drift\n",
    "\n",
    "Flask provides a solid foundation for serving ML models through APIs, but for production systems, consider frameworks like FastAPI which offer additional features like automatic documentation and async support."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Web APIs using Flask and FastAPI\n",
    "\n",
    "## HTTP Protocol Fundamentals\n",
    "\n",
    "| Method | Purpose | Example in ML Context |\n",
    "|--------|---------|------------------------|\n",
    "| GET | Retrieve data | Get model predictions, fetch model metadata |\n",
    "| POST | Send data | Submit data for prediction, upload training data |\n",
    "| PUT | Update data | Update model parameters |\n",
    "| DELETE | Remove data | Delete a model from registry |\n",
    "\n",
    "## Flask vs FastAPI Comparison\n",
    "\n",
    "| Feature | Flask | FastAPI |\n",
    "|---------|-------|---------|\n",
    "| **Performance** | Moderate (WSGI) | High (ASGI) |\n",
    "| **Requests/sec** | 2,000-5,000 | 10,000-20,000 |\n",
    "| **Type Validation** | Manual or with extensions | Built-in with Pydantic |\n",
    "| **Documentation** | Manual or with extensions | Automatic (Swagger/ReDoc) |\n",
    "| **Async Support** | Limited, requires extensions | Native async/await |\n",
    "| **Learning Curve** | Gentle, minimal | Steeper, requires type hints |\n",
    "| **Flexibility** | Very high | High but more opinionated |\n",
    "| **Maturity** | Mature (since 2010) | Newer (since 2018) |\n",
    "| **Ecosystem** | Extensive | Growing |\n",
    "| **Dependencies** | Minimal | More dependencies |\n",
    "| **Python Version** | 3.5+ | 3.6+ |\n",
    "| **Best For** | Simple APIs, legacy systems | Modern APIs, complex schemas |\n",
    "\n",
    "## Flask Framework\n",
    "\n",
    "Flask is a lightweight Python web framework ideal for creating APIs:\n",
    "\n",
    "```python\n",
    "from flask import Flask, request, jsonify\n",
    "import pickle\n",
    "\n",
    "# Initialize the Flask application\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Load the pre-trained model\n",
    "with open('model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "# Define a simple health check endpoint\n",
    "@app.route(\"/health\", methods=['GET'])\n",
    "def health():\n",
    "    return {\"status\": \"healthy\"}\n",
    "\n",
    "# Create prediction endpoint\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    # Get JSON data from the request\n",
    "    data = request.get_json(force=True)\n",
    "    \n",
    "    # Extract features\n",
    "    features = data['features']\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = model.predict([features])\n",
    "    \n",
    "    # Return prediction as JSON\n",
    "    return jsonify({\n",
    "        'prediction': prediction.tolist(),\n",
    "        'model_version': '1.0'\n",
    "    })\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True, host='0.0.0.0', port=5000)\n",
    "```\n",
    "\n",
    "## FastAPI Framework\n",
    "\n",
    "FastAPI is a modern, high-performance framework specifically designed for APIs:\n",
    "\n",
    "```python\n",
    "from fastapi import FastAPI, Query, HTTPException\n",
    "from pydantic import BaseModel\n",
    "import pickle\n",
    "import numpy as np\n",
    "from typing import List, Optional\n",
    "\n",
    "# Define data models with validation\n",
    "class PredictionRequest(BaseModel):\n",
    "    features: List[float]\n",
    "    \n",
    "class PredictionResponse(BaseModel):\n",
    "    prediction: List[float]\n",
    "    probability: Optional[List[float]] = None\n",
    "    model_version: str\n",
    "\n",
    "# Initialize the application\n",
    "app = FastAPI(\n",
    "    title=\"ML Model API\",\n",
    "    description=\"API for making predictions with ML model\",\n",
    "    version=\"1.0.0\"\n",
    ")\n",
    "\n",
    "# Load the model\n",
    "with open('model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "@app.get(\"/health\")\n",
    "def health_check():\n",
    "    return {\"status\": \"healthy\"}\n",
    "\n",
    "@app.post(\"/predict\", response_model=PredictionResponse)\n",
    "async def predict(request: PredictionRequest):\n",
    "    try:\n",
    "        # Convert features to numpy array\n",
    "        features = np.array(request.features).reshape(1, -1)\n",
    "        \n",
    "        # Make prediction\n",
    "        prediction = model.predict(features)\n",
    "        \n",
    "        # Get probabilities if available\n",
    "        probabilities = None\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            probabilities = model.predict_proba(features).tolist()\n",
    "        \n",
    "        return PredictionResponse(\n",
    "            prediction=prediction.tolist(),\n",
    "            probability=probabilities,\n",
    "            model_version=\"1.0\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "```\n",
    "\n",
    "## API Design Best Practices\n",
    "\n",
    "| Category | Best Practice | Example |\n",
    "|----------|---------------|---------|\n",
    "| **URL Design** | Use clear resource naming | `/predictions` (not `/get_pred`) |\n",
    "| | Use nouns, not verbs | `/models` (not `/get_models`) |\n",
    "| | Include versioning | `/api/v1/predict` |\n",
    "| **Status Codes** | Use appropriate HTTP codes | 200: OK, 400: Bad Request, 404: Not Found |\n",
    "| **Authentication** | Implement token or OAuth | Bearer tokens, API keys |\n",
    "| **Rate Limiting** | Protect against abuse | Limit to X requests per minute |\n",
    "| **Validation** | Validate all inputs | Check data types, ranges, formats |\n",
    "| **Response Format** | Consistent structure | Always include status, data, and error fields |\n",
    "| **Documentation** | Document all endpoints | Parameters, responses, error codes |\n",
    "\n",
    "## Authentication Implementation Examples\n",
    "\n",
    "```python\n",
    "# Flask example\n",
    "from flask_httpauth import HTTPTokenAuth\n",
    "auth = HTTPTokenAuth()\n",
    "\n",
    "@auth.verify_token\n",
    "def verify_token(token):\n",
    "    return token in valid_tokens\n",
    "\n",
    "@app.route('/protected')\n",
    "@auth.login_required\n",
    "def protected():\n",
    "    return jsonify({'result': 'success'})\n",
    "\n",
    "# FastAPI example\n",
    "from fastapi.security import APIKeyHeader\n",
    "from fastapi import Security, Depends, HTTPException\n",
    "\n",
    "api_key_header = APIKeyHeader(name=\"X-API-Key\")\n",
    "\n",
    "def get_api_key(api_key: str = Security(api_key_header)):\n",
    "    if api_key not in valid_api_keys:\n",
    "        raise HTTPException(status_code=403, detail=\"Invalid API Key\")\n",
    "    return api_key\n",
    "\n",
    "@app.get(\"/protected\")\n",
    "def protected(api_key: str = Depends(get_api_key)):\n",
    "    return {\"result\": \"success\"}\n",
    "```\n",
    "\n",
    "## Rate Limiting Examples\n",
    "\n",
    "```python\n",
    "# Flask with Flask-Limiter\n",
    "from flask_limiter import Limiter\n",
    "from flask_limiter.util import get_remote_address\n",
    "\n",
    "limiter = Limiter(\n",
    "    app,\n",
    "    key_func=get_remote_address,\n",
    "    default_limits=[\"200 per day\", \"50 per hour\"]\n",
    ")\n",
    "\n",
    "@app.route(\"/limited\")\n",
    "@limiter.limit(\"5 per minute\")\n",
    "def limited():\n",
    "    return jsonify({\"data\": \"limited endpoint\"})\n",
    "\n",
    "# FastAPI with slowapi\n",
    "from slowapi import Limiter, _rate_limit_exceeded_handler\n",
    "from slowapi.util import get_remote_address\n",
    "from slowapi.errors import RateLimitExceeded\n",
    "\n",
    "limiter = Limiter(key_func=get_remote_address)\n",
    "app.state.limiter = limiter\n",
    "app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)\n",
    "\n",
    "@app.get(\"/limited\")\n",
    "@limiter.limit(\"5/minute\")\n",
    "async def limited():\n",
    "    return {\"data\": \"limited endpoint\"}\n",
    "```\n",
    "\n",
    "## Testing Strategies\n",
    "\n",
    "| Test Type | Purpose | Example Tool | \n",
    "|-----------|---------|--------------|\n",
    "| **Unit Tests** | Test individual routes | pytest |\n",
    "| **Integration Tests** | Test API as a whole | pytest with client fixture |\n",
    "| **Load Tests** | Test performance under load | Locust, k6 |\n",
    "| **Contract Tests** | Verify API specification | Dredd, Pact |\n",
    "\n",
    "## Testing Examples\n",
    "\n",
    "```python\n",
    "# Flask with pytest\n",
    "def test_predict_endpoint(client):\n",
    "    response = client.post(\n",
    "        '/predict',\n",
    "        json={'features': [1.0, 2.0, 3.0, 4.0]}\n",
    "    )\n",
    "    assert response.status_code == 200\n",
    "    assert 'prediction' in response.json\n",
    "    \n",
    "# FastAPI with pytest\n",
    "from fastapi.testclient import TestClient\n",
    "\n",
    "client = TestClient(app)\n",
    "\n",
    "def test_predict_endpoint():\n",
    "    response = client.post(\n",
    "        '/predict',\n",
    "        json={'features': [1.0, 2.0, 3.0, 4.0]}\n",
    "    )\n",
    "    assert response.status_code == 200\n",
    "    assert 'prediction' in response.json()\n",
    "```\n",
    "\n",
    "## Deployment Options\n",
    "\n",
    "| Server Type | Flask | FastAPI |\n",
    "|-------------|-------|---------|\n",
    "| **Development** | Flask built-in | Uvicorn |\n",
    "| **Production (WSGI)** | Gunicorn, uWSGI | (Not optimal) |\n",
    "| **Production (ASGI)** | (Not supported) | Uvicorn, Hypercorn |\n",
    "| **Container Orchestration** | Kubernetes, ECS | Kubernetes, ECS |\n",
    "| **Serverless** | AWS Lambda, GCP Functions | AWS Lambda, GCP Functions |\n",
    "\n",
    "## Deployment Commands\n",
    "\n",
    "| Framework | Server | Command |\n",
    "|-----------|--------|---------|\n",
    "| **Flask** | Gunicorn | `gunicorn -w 4 app:app` |\n",
    "| | uWSGI | `uwsgi --http :8000 --module app:app` |\n",
    "| **FastAPI** | Uvicorn | `uvicorn app:app --workers 4` |\n",
    "| | Hypercorn | `hypercorn app:app --workers 4` |\n",
    "\n",
    "## Batch Prediction Endpoints\n",
    "\n",
    "```python\n",
    "# Flask example\n",
    "@app.route('/batch-predict', methods=['POST'])\n",
    "def batch_predict():\n",
    "    data = request.get_json(force=True)\n",
    "    batch_features = data['features']\n",
    "    \n",
    "    predictions = []\n",
    "    for features in batch_features:\n",
    "        prediction = model.predict([features])\n",
    "        predictions.append({\n",
    "            'prediction': prediction.tolist(),\n",
    "            'model_version': '1.0'\n",
    "        })\n",
    "    \n",
    "    return jsonify(predictions)\n",
    "\n",
    "# FastAPI example\n",
    "@app.post(\"/batch-predict\", response_model=List[PredictionResponse])\n",
    "async def batch_predict(requests: List[PredictionRequest]):\n",
    "    predictions = []\n",
    "    for req in requests:\n",
    "        features = np.array(req.features).reshape(1, -1)\n",
    "        prediction = model.predict(features)\n",
    "        predictions.append(PredictionResponse(\n",
    "            prediction=prediction.tolist(),\n",
    "            model_version=\"1.0\"\n",
    "        ))\n",
    "    return predictions\n",
    "```\n",
    "\n",
    "## Monitoring and Observability\n",
    "\n",
    "| Aspect | Tools | Metrics to Track |\n",
    "|--------|-------|------------------|\n",
    "| **Logging** | Loguru, ELK Stack | Request/response details, errors |\n",
    "| **Metrics** | Prometheus, Grafana | Request rate, latency, error rate |\n",
    "| **Tracing** | Jaeger, Zipkin | Request flow, bottlenecks |\n",
    "| **Alerting** | Alertmanager, PagerDuty | SLA breaches, error spikes |\n",
    "\n",
    "## When to Choose Which Framework\n",
    "\n",
    "**Use Flask when:**\n",
    "- Building simple, straightforward APIs\n",
    "- Working with legacy Python codebases\n",
    "- Team is already familiar with Flask\n",
    "- Need maximum flexibility and customization\n",
    "\n",
    "**Use FastAPI when:**\n",
    "- Building new, production-grade APIs\n",
    "- Working with complex request/response schemas\n",
    "- Need automatic documentation\n",
    "- Performance is critical\n",
    "- Handling asynchronous operations\n",
    "\n",
    "Both frameworks are excellent choices for serving ML models, with FastAPI gaining popularity for new projects due to its modern features and performance benefits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Docker and Containerization for ML Applications\n",
    "\n",
    "## Key Concepts Comparison: Containers vs Virtual Machines\n",
    "\n",
    "| Feature | Containers | Virtual Machines |\n",
    "|---------|-----------|------------------|\n",
    "| Size | Lightweight (MBs) | Heavy (GBs) |\n",
    "| Startup Time | Seconds | Minutes |\n",
    "| Resource Efficiency | High | Lower |\n",
    "| Isolation Level | Process-level isolation | Complete hardware isolation |\n",
    "| OS | Shares host OS kernel | Requires full OS per VM |\n",
    "| Performance | Near-native | Overhead due to hypervisor |\n",
    "| Portability | Highly portable across environments | Less portable |\n",
    "| ML Use Case | Deploying models with dependencies | Complete ML environments |\n",
    "\n",
    "## Docker Components Explained\n",
    "\n",
    "1. **Dockerfile**: A text file with instructions to build an image\n",
    "2. **Image**: A static, immutable snapshot of a container's files and settings\n",
    "3. **Container**: A running instance of an image\n",
    "4. **Registry**: A repository of Docker images (e.g., Docker Hub)\n",
    "5. **Volume**: Persistent storage for container data\n",
    "6. **Network**: Communication layer between containers\n",
    "\n",
    "## Essential Docker Commands\n",
    "\n",
    "```bash\n",
    "# Build an image from a Dockerfile\n",
    "docker build -t my-ml-model:v1 .\n",
    "\n",
    "# List available images\n",
    "docker images\n",
    "\n",
    "# Run a container from an image\n",
    "docker run -p 5000:5000 my-ml-model:v1\n",
    "\n",
    "# List running containers\n",
    "docker ps\n",
    "\n",
    "# Stop a running container\n",
    "docker stop <container_id>\n",
    "\n",
    "# Remove a container\n",
    "docker rm <container_id>\n",
    "\n",
    "# Remove an image\n",
    "docker rmi my-ml-model:v1\n",
    "\n",
    "# Pull an image from Docker Hub\n",
    "docker pull tensorflow/tensorflow:latest-gpu\n",
    "\n",
    "# Push an image to Docker Hub\n",
    "docker push username/my-ml-model:v1\n",
    "\n",
    "# Execute a command in a running container\n",
    "docker exec -it <container_id> bash\n",
    "\n",
    "# View container logs\n",
    "docker logs <container_id>\n",
    "```\n",
    "\n",
    "## Writing a Dockerfile for ML Applications\n",
    "\n",
    "```dockerfile\n",
    "# Base image - use specific version for reproducibility\n",
    "FROM python:3.9-slim\n",
    "\n",
    "# Set working directory\n",
    "WORKDIR /app\n",
    "\n",
    "# Set environment variables\n",
    "ENV PYTHONDONTWRITEBYTECODE=1 \\\n",
    "    PYTHONUNBUFFERED=1 \\\n",
    "    MODEL_PATH=/app/models/model.pkl\n",
    "\n",
    "# Install system dependencies\n",
    "RUN apt-get update && apt-get install -y --no-install-recommends \\\n",
    "    build-essential \\\n",
    "    libgomp1 \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Install Python dependencies\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Copy application code\n",
    "COPY . .\n",
    "\n",
    "# Create a non-root user and switch to it\n",
    "RUN useradd -m modeluser\n",
    "USER modeluser\n",
    "\n",
    "# Expose the port the app runs on\n",
    "EXPOSE 5000\n",
    "\n",
    "# Command to run the application\n",
    "CMD [\"python\", \"app.py\"]\n",
    "```\n",
    "\n",
    "**Code Explanation:**\n",
    "\n",
    "1. `FROM python:3.9-slim`: Starts with a lightweight Python image\n",
    "2. `WORKDIR /app`: Sets the working directory inside the container\n",
    "3. `ENV PYTHONDONTWRITEBYTECODE=1 ...`: Sets environment variables to:\n",
    "   - Prevent Python from creating .pyc files\n",
    "   - Keep Python output unbuffered\n",
    "   - Set model path location\n",
    "4. `RUN apt-get update...`: Installs system dependencies needed for ML libraries\n",
    "5. `COPY requirements.txt .`: Copies the requirements file first (for better caching)\n",
    "6. `RUN pip install...`: Installs Python dependencies\n",
    "7. `COPY . .`: Copies the application code\n",
    "8. `RUN useradd -m modeluser...`: Creates a non-root user for security\n",
    "9. `EXPOSE 5000`: Documents that the application uses port 5000\n",
    "10. `CMD [\"python\", \"app.py\"]`: Specifies the command to run when the container starts\n",
    "\n",
    "## Requirements.txt for ML Applications\n",
    "\n",
    "```\n",
    "numpy==1.21.5\n",
    "pandas==1.3.5\n",
    "scikit-learn==1.0.2\n",
    "flask==2.0.2\n",
    "gunicorn==20.1.0\n",
    "joblib==1.1.0\n",
    "pytest==7.0.0\n",
    "```\n",
    "\n",
    "## Example ML Application for Containerization\n",
    "\n",
    "### app.py\n",
    "\n",
    "```python\n",
    "from flask import Flask, request, jsonify\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Load model - path from environment variable\n",
    "MODEL_PATH = os.environ.get('MODEL_PATH', 'model.pkl')\n",
    "\n",
    "@app.route('/health', methods=['GET'])\n",
    "def health():\n",
    "    return jsonify({\"status\": \"healthy\"})\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    try:\n",
    "        # Get model if not already loaded\n",
    "        if not hasattr(app, 'model'):\n",
    "            with open(MODEL_PATH, 'rb') as f:\n",
    "                app.model = pickle.load(f)\n",
    "        \n",
    "        # Parse input data\n",
    "        data = request.get_json()\n",
    "        features = np.array(data['features']).reshape(1, -1)\n",
    "        \n",
    "        # Make prediction\n",
    "        prediction = app.model.predict(features).tolist()\n",
    "        \n",
    "        # Return prediction\n",
    "        return jsonify({\n",
    "            'prediction': prediction,\n",
    "            'model_version': '1.0.0'\n",
    "        })\n",
    "    \n",
    "    except Exception as e:\n",
    "        return jsonify({'error': str(e)}), 500\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Run on all available interfaces\n",
    "    app.run(host='0.0.0.0', port=5000)\n",
    "```\n",
    "\n",
    "## ⚠️ Docker Best Practices for ML\n",
    "\n",
    "1. **Use specific base image versions** - Don't use `latest` tag\n",
    "2. **Layer your Dockerfile efficiently** - Order commands by change frequency\n",
    "3. **Minimize image size** - Use multi-stage builds for training vs. serving\n",
    "4. **Store models separately** - Use volumes or object storage\n",
    "5. **Use non-root users** - Improve security\n",
    "6. **Include only what's needed** - Use .dockerignore for datasets, logs, etc.\n",
    "7. **Set resource limits** - Specify memory and CPU constraints\n",
    "8. **Health checks** - Add HEALTHCHECK instruction\n",
    "9. **Cache Python packages** - Put requirements.txt first\n",
    "\n",
    "## Docker Compose for ML Environments\n",
    "\n",
    "Docker Compose lets you define multi-container applications with a YAML file.\n",
    "\n",
    "### docker-compose.yml\n",
    "\n",
    "```yaml\n",
    "version: '3.8'\n",
    "\n",
    "services:\n",
    "  model-api:\n",
    "    build: \n",
    "      context: .\n",
    "      dockerfile: Dockerfile\n",
    "    ports:\n",
    "      - \"5000:5000\"\n",
    "    volumes:\n",
    "      - ./models:/app/models\n",
    "    environment:\n",
    "      - MODEL_PATH=/app/models/model.pkl\n",
    "      - LOG_LEVEL=INFO\n",
    "    restart: unless-stopped\n",
    "    healthcheck:\n",
    "      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:5000/health\"]\n",
    "      interval: 30s\n",
    "      timeout: 10s\n",
    "      retries: 3\n",
    "      \n",
    "  model-monitor:\n",
    "    build: ./monitoring\n",
    "    depends_on:\n",
    "      - model-api\n",
    "    ports:\n",
    "      - \"8050:8050\"\n",
    "    volumes:\n",
    "      - ./logs:/app/logs\n",
    "      \n",
    "  database:\n",
    "    image: postgres:13\n",
    "    environment:\n",
    "      - POSTGRES_PASSWORD=password\n",
    "      - POSTGRES_USER=mluser\n",
    "      - POSTGRES_DB=mldb\n",
    "    volumes:\n",
    "      - postgres-data:/var/lib/postgresql/data\n",
    "    ports:\n",
    "      - \"5432:5432\"\n",
    "\n",
    "volumes:\n",
    "  postgres-data:\n",
    "```\n",
    "\n",
    "**Code Explanation:**\n",
    "\n",
    "1. `version: '3.8'`: Specifies the Docker Compose file format version\n",
    "2. `services:`: Defines the containers that make up the application\n",
    "3. `model-api:`: The main ML model serving API\n",
    "   - `build:`: Instructions to build from a Dockerfile\n",
    "   - `ports:`: Maps container port 5000 to host port 5000\n",
    "   - `volumes:`: Mounts the models directory for persistence\n",
    "   - `environment:`: Sets environment variables\n",
    "   - `healthcheck:`: Defines how to check if the service is healthy\n",
    "4. `model-monitor:`: A service for monitoring model performance\n",
    "5. `database:`: PostgreSQL database for storing predictions or metrics\n",
    "6. `volumes:`: Defines named volumes for persistent data\n",
    "\n",
    "## Multi-stage Builds for ML Applications\n",
    "\n",
    "```dockerfile\n",
    "# Build stage - includes training dependencies\n",
    "FROM python:3.9 AS builder\n",
    "\n",
    "WORKDIR /build\n",
    "COPY requirements-full.txt .\n",
    "RUN pip install --no-cache-dir -r requirements-full.txt\n",
    "\n",
    "# Copy code and run tests\n",
    "COPY . .\n",
    "RUN pytest tests/\n",
    "\n",
    "# Inference stage - smaller image for deployment\n",
    "FROM python:3.9-slim\n",
    "\n",
    "WORKDIR /app\n",
    "COPY requirements-serve.txt .\n",
    "RUN pip install --no-cache-dir -r requirements-serve.txt\n",
    "\n",
    "# Copy application code and model from builder\n",
    "COPY --from=builder /build/app ./app\n",
    "COPY --from=builder /build/models/model.pkl ./models/\n",
    "\n",
    "EXPOSE 5000\n",
    "\n",
    "CMD [\"python\", \"app/api.py\"]\n",
    "```\n",
    "\n",
    "## Working with Docker Hub\n",
    "\n",
    "Docker Hub is a public registry for sharing Docker images. Here's how to use it:\n",
    "\n",
    "```bash\n",
    "# Login to Docker Hub\n",
    "docker login\n",
    "\n",
    "# Tag your image (username/repository:tag)\n",
    "docker tag my-ml-model:v1 username/my-ml-model:v1\n",
    "\n",
    "# Push to Docker Hub\n",
    "docker push username/my-ml-model:v1\n",
    "\n",
    "# Pull from Docker Hub\n",
    "docker pull username/my-ml-model:v1\n",
    "```\n",
    "\n",
    "## DockerHub Alternatives for ML\n",
    "\n",
    "1. **Amazon ECR (Elastic Container Registry)**\n",
    "2. **Google Container Registry (GCR)**\n",
    "3. **Azure Container Registry (ACR)**\n",
    "4. **GitHub Container Registry**\n",
    "5. **Harbor** (open source container registry)\n",
    "\n",
    "## GPU Support for ML Containers\n",
    "\n",
    "```dockerfile\n",
    "# Use NVIDIA CUDA base image for GPU support\n",
    "FROM nvidia/cuda:11.6.2-cudnn8-runtime-ubuntu20.04\n",
    "\n",
    "# Install Python and essentials\n",
    "RUN apt-get update && apt-get install -y \\\n",
    "    python3 \\\n",
    "    python3-pip \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Install ML libraries with GPU support\n",
    "COPY requirements.txt .\n",
    "RUN pip3 install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Set working directory and copy application\n",
    "WORKDIR /app\n",
    "COPY . .\n",
    "\n",
    "# Set environment variables for GPU usage\n",
    "ENV NVIDIA_VISIBLE_DEVICES=all\n",
    "ENV NVIDIA_DRIVER_CAPABILITIES=compute,utility\n",
    "\n",
    "CMD [\"python3\", \"train.py\"]\n",
    "```\n",
    "\n",
    "To run GPU-enabled containers:\n",
    "\n",
    "```bash\n",
    "docker run --gpus all my-ml-gpu-model:v1\n",
    "```\n",
    "\n",
    "## Real-world ML Containerization Example - Complete Workflow\n",
    "\n",
    "### Project Structure\n",
    "\n",
    "```\n",
    "ml-project/\n",
    "├── app/\n",
    "│   ├── api.py              # Flask API code\n",
    "│   ├── preprocessing.py    # Data preprocessing functions\n",
    "│   └── prediction.py       # Model prediction logic\n",
    "├── models/\n",
    "│   └── model.pkl           # Trained model\n",
    "├── notebooks/\n",
    "│   └── model_development.ipynb # Development notebook\n",
    "├── scripts/\n",
    "│   ├── train.py            # Training script\n",
    "│   └── evaluate.py         # Evaluation script\n",
    "├── tests/\n",
    "│   ├── test_api.py         # API tests\n",
    "│   └── test_prediction.py  # Prediction tests\n",
    "├── .dockerignore           # Exclude files from Docker context\n",
    "├── Dockerfile              # Container definition\n",
    "├── docker-compose.yml      # Multi-container definition\n",
    "└── requirements.txt        # Python dependencies\n",
    "```\n",
    "\n",
    "### .dockerignore\n",
    "\n",
    "```\n",
    "notebooks/\n",
    "data/\n",
    ".git/\n",
    "__pycache__/\n",
    "*.pyc\n",
    "*.pyo\n",
    "*.pyd\n",
    ".pytest_cache/\n",
    ".coverage\n",
    "htmlcov/\n",
    "```\n",
    "\n",
    "### Dockerfile\n",
    "\n",
    "```dockerfile\n",
    "FROM python:3.9-slim\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "# Install dependencies\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Copy application code\n",
    "COPY app/ ./app/\n",
    "COPY models/ ./models/\n",
    "COPY scripts/ ./scripts/\n",
    "\n",
    "# Set environment variables\n",
    "ENV MODEL_PATH=/app/models/model.pkl\n",
    "ENV PYTHONPATH=/app\n",
    "\n",
    "# Create non-root user\n",
    "RUN useradd -m appuser\n",
    "USER appuser\n",
    "\n",
    "# Expose API port\n",
    "EXPOSE 5000\n",
    "\n",
    "# Start the application\n",
    "CMD [\"gunicorn\", \"--bind\", \"0.0.0.0:5000\", \"app.api:app\"]\n",
    "```\n",
    "\n",
    "### Building and Running the Container\n",
    "\n",
    "```bash\n",
    "# Build the Docker image\n",
    "docker build -t ml-prediction-service:v1 .\n",
    "\n",
    "# Run the container\n",
    "docker run -p 5000:5000 ml-prediction-service:v1\n",
    "\n",
    "# Test the API\n",
    "curl -X POST -H \"Content-Type: application/json\" \\\n",
    "  -d '{\"features\": [5.1, 3.5, 1.4, 0.2]}' \\\n",
    "  http://localhost:5000/predict\n",
    "```\n",
    "\n",
    "## CI/CD Integration with Docker\n",
    "\n",
    "```yaml\n",
    "# Example GitHub Actions workflow\n",
    "name: Build and Push Docker Image\n",
    "\n",
    "on:\n",
    "  push:\n",
    "    branches: [ main ]\n",
    "\n",
    "jobs:\n",
    "  build:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v2\n",
    "      \n",
    "      - name: Set up Docker Buildx\n",
    "        uses: docker/setup-buildx-action@v1\n",
    "      \n",
    "      - name: Login to DockerHub\n",
    "        uses: docker/login-action@v1\n",
    "        with:\n",
    "          username: ${{ secrets.DOCKERHUB_USERNAME }}\n",
    "          password: ${{ secrets.DOCKERHUB_TOKEN }}\n",
    "      \n",
    "      - name: Build and push\n",
    "        uses: docker/build-push-action@v2\n",
    "        with:\n",
    "          context: .\n",
    "          push: true\n",
    "          tags: username/ml-model:latest\n",
    "```\n",
    "\n",
    "## Common Docker Issues in ML and Solutions\n",
    "\n",
    "| Issue | Solution |\n",
    "|-------|----------|\n",
    "| Out of memory errors | Limit model size, batch processing, or increase container memory limits |\n",
    "| Slow container builds | Use .dockerignore, multi-stage builds, minimal base images |\n",
    "| Large image sizes | Use slimmer base images, clean up after installation |\n",
    "| GPU access issues | Use NVIDIA Container Toolkit, check driver compatibility |\n",
    "| Security vulnerabilities | Regular updates, non-root users, scan images |\n",
    "| Data persistence problems | Use Docker volumes for model storage |\n",
    "\n",
    "## ⚠️ Security Best Practices\n",
    "\n",
    "1. **Never include credentials in images** - Use environment variables or secrets management\n",
    "2. **Scan images for vulnerabilities** - Use tools like Trivy, Clair, or Docker Scan\n",
    "3. **Use official base images** - Stick to verified images\n",
    "4. **Keep images updated** - Rebuild regularly to incorporate security patches\n",
    "5. **Run as non-root user** - Limit container privileges\n",
    "6. **Use read-only filesystems** - When possible\n",
    "7. **Set resource limits** - Prevent DoS situations\n",
    "\n",
    "Understanding Docker is crucial for MLOps as it provides consistency across environments and is the foundation for scalable, reproducible ML deployments. With containers, you can ensure your models run the same way in development and production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Deploying Docker Containers on AWS using ECR and ECS\n",
    "\n",
    "## AWS Container Services Overview\n",
    "\n",
    "| Service | Purpose | Best For |\n",
    "|---------|---------|----------|\n",
    "| **ECR** (Elastic Container Registry) | Store, manage, and deploy Docker container images | Secure storage of ML model images |\n",
    "| **ECS** (Elastic Container Service) | Orchestration service to run containers | Running ML APIs and batch prediction services |\n",
    "| **Fargate** | Serverless compute engine for containers | Hosting ML models without managing servers |\n",
    "| **EKS** (Elastic Kubernetes Service) | Managed Kubernetes service | Complex ML workflows requiring orchestration |\n",
    "\n",
    "## ECR (Elastic Container Registry)\n",
    "\n",
    "ECR is AWS's managed Docker container registry service that allows you to store, manage, and deploy container images securely.\n",
    "\n",
    "### Key Features of ECR\n",
    "\n",
    "- **Private repositories** with resource-based permissions\n",
    "- **Encryption** of images at rest\n",
    "- **Image scanning** for security vulnerabilities\n",
    "- **Image lifecycle policies** for repository management\n",
    "- **Cross-region and cross-account replication**\n",
    "- **Integration with AWS IAM** for access control\n",
    "\n",
    "## ECS (Elastic Container Service)\n",
    "\n",
    "ECS is a fully managed container orchestration service that allows you to run and scale containerized applications.\n",
    "\n",
    "### Key Components of ECS\n",
    "\n",
    "1. **Cluster**: A logical grouping of tasks or services\n",
    "2. **Task Definition**: Blueprint for your application (similar to a Docker Compose file)\n",
    "3. **Task**: Instance of a task definition running in a cluster\n",
    "4. **Service**: Maintains and scales a specified number of tasks\n",
    "5. **Container Instance**: EC2 instance running the ECS agent\n",
    "\n",
    "### Fargate vs EC2 Launch Types\n",
    "\n",
    "| Feature | Fargate | EC2 |\n",
    "|---------|---------|-----|\n",
    "| Server Management | Serverless (no EC2 instances to manage) | Self-managed EC2 instances |\n",
    "| Pricing | Pay per task/second | Pay for EC2 instances regardless of usage |\n",
    "| Isolation | Task-level isolation | Instance-level with shared resources |\n",
    "| Configuration | Less control over infrastructure | More control over instance types, networking |\n",
    "| Scaling | Task-level scaling | Instance-level scaling |\n",
    "| ML Use Case | Periodic model inference, lightweight APIs | Resource-intensive training, GPU workloads |\n",
    "\n",
    "## ⚠️ Important AWS Configuration Concepts\n",
    "\n",
    "1. **IAM Roles**: Permissions for ECS tasks to access AWS resources\n",
    "2. **Security Groups**: Control network traffic to and from your containers\n",
    "3. **VPC**: Networking environment for your ECS tasks\n",
    "4. **Load Balancers**: Distribute traffic to your containers\n",
    "5. **CloudWatch**: Monitor container logs and metrics\n",
    "6. **Auto Scaling**: Automatically adjust container count based on load\n",
    "\n",
    "## Step-by-Step Deployment Process\n",
    "\n",
    "### Step 1: Install and Configure AWS CLI\n",
    "\n",
    "```bash\n",
    "# Install AWS CLI\n",
    "pip install awscli\n",
    "\n",
    "# Configure AWS CLI with your credentials\n",
    "aws configure\n",
    "\n",
    "# Enter your AWS Access Key, Secret Key, region (e.g., us-east-1), and output format (json)\n",
    "```\n",
    "\n",
    "**Explanation**: \n",
    "- AWS CLI provides a command-line interface to interact with AWS services\n",
    "- You'll need IAM credentials with appropriate permissions for ECR, ECS, and other related services\n",
    "- The region should be chosen based on proximity to users or data residency requirements\n",
    "\n",
    "### Step 2: Create an ECR Repository\n",
    "\n",
    "```bash\n",
    "# Create a repository for your ML model\n",
    "aws ecr create-repository --repository-name ml-model-api --image-scanning-configuration scanOnPush=true\n",
    "\n",
    "# Note the repositoryUri in the output\n",
    "# Format: {aws_account_id}.dkr.ecr.{region}.amazonaws.com/{repository-name}\n",
    "```\n",
    "\n",
    "**Explanation**:\n",
    "- This creates a private Docker image repository in your AWS account\n",
    "- `scanOnPush=true` enables automatic vulnerability scanning of your images\n",
    "- The repository URI will be used to tag and push your Docker images\n",
    "\n",
    "### Step 3: Build and Tag Your Docker Image\n",
    "\n",
    "```bash\n",
    "# Build your Docker image locally\n",
    "docker build -t ml-model-api .\n",
    "\n",
    "# Tag the image for ECR (replace with your repository URI)\n",
    "docker tag ml-model-api:latest 123456789012.dkr.ecr.us-east-1.amazonaws.com/ml-model-api:latest\n",
    "```\n",
    "\n",
    "**Explanation**:\n",
    "- The tag should match your ECR repository URI\n",
    "- You can use version tags (v1, v2) or descriptive tags (latest, stable) to manage different versions\n",
    "\n",
    "### Step 4: Authenticate and Push to ECR\n",
    "\n",
    "```bash\n",
    "# Log in to ECR\n",
    "aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin 123456789012.dkr.ecr.us-east-1.amazonaws.com\n",
    "\n",
    "# Push the image to ECR\n",
    "docker push 123456789012.dkr.ecr.us-east-1.amazonaws.com/ml-model-api:latest\n",
    "```\n",
    "\n",
    "**Explanation**:\n",
    "- `get-login-password` retrieves a token valid for 12 hours\n",
    "- This authenticates your Docker client with ECR\n",
    "- The push command uploads your local image to ECR\n",
    "\n",
    "### Step 5: Create an ECS Cluster\n",
    "\n",
    "```bash\n",
    "# Create a cluster using Fargate\n",
    "aws ecs create-cluster --cluster-name ml-model-cluster\n",
    "\n",
    "# Output will include the cluster ARN and status\n",
    "```\n",
    "\n",
    "**Explanation**:\n",
    "- A cluster is a logical grouping of tasks or services\n",
    "- For Fargate, no EC2 instances are provisioned at this stage\n",
    "- The cluster name should be descriptive of its purpose\n",
    "\n",
    "### Step 6: Create a Task Definition\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"family\": \"ml-model-task\",\n",
    "  \"networkMode\": \"awsvpc\",\n",
    "  \"executionRoleArn\": \"arn:aws:iam::123456789012:role/ecsTaskExecutionRole\",\n",
    "  \"taskRoleArn\": \"arn:aws:iam::123456789012:role/ecsTaskRole\",\n",
    "  \"requiresCompatibilities\": [\"FARGATE\"],\n",
    "  \"cpu\": \"1024\",\n",
    "  \"memory\": \"2048\",\n",
    "  \"containerDefinitions\": [\n",
    "    {\n",
    "      \"name\": \"ml-model-container\",\n",
    "      \"image\": \"123456789012.dkr.ecr.us-east-1.amazonaws.com/ml-model-api:latest\",\n",
    "      \"essential\": true,\n",
    "      \"portMappings\": [\n",
    "        {\n",
    "          \"containerPort\": 5000,\n",
    "          \"hostPort\": 5000,\n",
    "          \"protocol\": \"tcp\"\n",
    "        }\n",
    "      ],\n",
    "      \"logConfiguration\": {\n",
    "        \"logDriver\": \"awslogs\",\n",
    "        \"options\": {\n",
    "          \"awslogs-group\": \"/ecs/ml-model-task\",\n",
    "          \"awslogs-region\": \"us-east-1\",\n",
    "          \"awslogs-stream-prefix\": \"ecs\"\n",
    "        }\n",
    "      },\n",
    "      \"environment\": [\n",
    "        {\n",
    "          \"name\": \"MODEL_VERSION\",\n",
    "          \"value\": \"v1\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"LOG_LEVEL\",\n",
    "          \"value\": \"INFO\"\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "Save this as `task-definition.json` and run:\n",
    "\n",
    "```bash\n",
    "# Create the task definition\n",
    "aws ecs register-task-definition --cli-input-json file://task-definition.json\n",
    "```\n",
    "\n",
    "**Explanation**:\n",
    "- **family**: A name for your task definition family (like a group for versions)\n",
    "- **networkMode**: \"awsvpc\" is required for Fargate\n",
    "- **executionRoleArn**: IAM role that allows ECS to pull images and push logs\n",
    "- **taskRoleArn**: IAM role that allows your container to access AWS services\n",
    "- **requiresCompatibilities**: Specifies this is a Fargate task\n",
    "- **cpu/memory**: Resources allocated to the task (1024 CPU units = 1 vCPU)\n",
    "- **containerDefinitions**: Container configuration details\n",
    "  - **image**: ECR image URI\n",
    "  - **portMappings**: Maps container port to host port\n",
    "  - **logConfiguration**: Sets up CloudWatch logs\n",
    "  - **environment**: Environment variables for your container\n",
    "\n",
    "### Step 7: Create an ECS Service\n",
    "\n",
    "```bash\n",
    "# Create the service\n",
    "aws ecs create-service \\\n",
    "  --cluster ml-model-cluster \\\n",
    "  --service-name ml-model-service \\\n",
    "  --task-definition ml-model-task:1 \\\n",
    "  --desired-count 2 \\\n",
    "  --launch-type FARGATE \\\n",
    "  --platform-version LATEST \\\n",
    "  --network-configuration \"awsvpcConfiguration={subnets=[subnet-12345678,subnet-87654321],securityGroups=[sg-12345678],assignPublicIp=ENABLED}\" \\\n",
    "  --load-balancer \"targetGroupArn=arn:aws:elasticloadbalancing:us-east-1:123456789012:targetgroup/ml-model-tg/1234567890123456,containerName=ml-model-container,containerPort=5000\"\n",
    "```\n",
    "\n",
    "**Explanation**:\n",
    "- **cluster**: The cluster where tasks will run\n",
    "- **service-name**: A descriptive name for the service\n",
    "- **task-definition**: The task definition to use (including version)\n",
    "- **desired-count**: Number of tasks to maintain\n",
    "- **launch-type**: FARGATE for serverless\n",
    "- **network-configuration**: \n",
    "  - **subnets**: Where tasks will be placed (need at least two for high availability)\n",
    "  - **securityGroups**: Firewall rules for your tasks\n",
    "  - **assignPublicIp**: Whether tasks get public IPs\n",
    "- **load-balancer**: Optional configuration for distributing traffic\n",
    "\n",
    "### Step 8: Setting Up a Load Balancer (Optional but Recommended)\n",
    "\n",
    "Before creating the service, you should set up an Application Load Balancer:\n",
    "\n",
    "```bash\n",
    "# Create a target group\n",
    "aws elbv2 create-target-group \\\n",
    "  --name ml-model-tg \\\n",
    "  --protocol HTTP \\\n",
    "  --port 5000 \\\n",
    "  --vpc-id vpc-12345678 \\\n",
    "  --target-type ip \\\n",
    "  --health-check-path /health\n",
    "\n",
    "# Create a load balancer\n",
    "aws elbv2 create-load-balancer \\\n",
    "  --name ml-model-alb \\\n",
    "  --subnets subnet-12345678 subnet-87654321 \\\n",
    "  --security-groups sg-12345678\n",
    "\n",
    "# Create a listener\n",
    "aws elbv2 create-listener \\\n",
    "  --load-balancer-arn arn:aws:elasticloadbalancing:us-east-1:123456789012:loadbalancer/app/ml-model-alb/1234567890123456 \\\n",
    "  --protocol HTTP \\\n",
    "  --port 80 \\\n",
    "  --default-actions Type=forward,TargetGroupArn=arn:aws:elasticloadbalancing:us-east-1:123456789012:targetgroup/ml-model-tg/1234567890123456\n",
    "```\n",
    "\n",
    "**Explanation**:\n",
    "- The target group defines where to route traffic (to your containers)\n",
    "- The load balancer distributes incoming traffic across your containers\n",
    "- The listener defines what port/protocol the load balancer should listen on\n",
    "- For ML applications, load balancing is crucial for reliability and scaling\n",
    "\n",
    "## Auto Scaling for ML Workloads\n",
    "\n",
    "```bash\n",
    "# Create an auto scaling target for your service\n",
    "aws application-autoscaling register-scalable-target \\\n",
    "  --service-namespace ecs \\\n",
    "  --scalable-dimension ecs:service:DesiredCount \\\n",
    "  --resource-id service/ml-model-cluster/ml-model-service \\\n",
    "  --min-capacity 2 \\\n",
    "  --max-capacity 10\n",
    "\n",
    "# Create a scaling policy based on CPU utilization\n",
    "aws application-autoscaling put-scaling-policy \\\n",
    "  --service-namespace ecs \\\n",
    "  --scalable-dimension ecs:service:DesiredCount \\\n",
    "  --resource-id service/ml-model-cluster/ml-model-service \\\n",
    "  --policy-name cpu-tracking-scaling-policy \\\n",
    "  --policy-type TargetTrackingScaling \\\n",
    "  --target-tracking-scaling-policy-configuration '{ \n",
    "    \"TargetValue\": 70.0, \n",
    "    \"PredefinedMetricSpecification\": { \n",
    "      \"PredefinedMetricType\": \"ECSServiceAverageCPUUtilization\" \n",
    "    },\n",
    "    \"ScaleOutCooldown\": 60,\n",
    "    \"ScaleInCooldown\": 60\n",
    "  }'\n",
    "```\n",
    "\n",
    "**Explanation**:\n",
    "- **min-capacity/max-capacity**: Defines the scaling boundaries\n",
    "- **TargetValue**: Target CPU utilization percentage (70% in this example)\n",
    "- **ScaleOutCooldown**: Seconds before scaling out again after scaling out\n",
    "- **ScaleInCooldown**: Seconds before scaling in again after scaling in\n",
    "- For ML workloads, you might also consider custom metrics like prediction latency\n",
    "\n",
    "## Infrastructure as Code (IaC) Options\n",
    "\n",
    "While command-line deployment works, it's better to use Infrastructure as Code for reproducibility:\n",
    "\n",
    "### AWS CloudFormation Example\n",
    "\n",
    "```yaml\n",
    "AWSTemplateFormatVersion: '2010-09-09'\n",
    "Description: 'ML Model Deployment on ECS'\n",
    "\n",
    "Resources:\n",
    "  # ECR Repository\n",
    "  MLModelRepository:\n",
    "    Type: AWS::ECR::Repository\n",
    "    Properties:\n",
    "      RepositoryName: ml-model-api\n",
    "      ImageScanningConfiguration:\n",
    "        ScanOnPush: true\n",
    "      LifecyclePolicy:\n",
    "        LifecyclePolicyText: |\n",
    "          {\n",
    "            \"rules\": [\n",
    "              {\n",
    "                \"rulePriority\": 1,\n",
    "                \"description\": \"Keep only the last 10 images\",\n",
    "                \"selection\": {\n",
    "                  \"tagStatus\": \"any\",\n",
    "                  \"countType\": \"imageCountMoreThan\",\n",
    "                  \"countNumber\": 10\n",
    "                },\n",
    "                \"action\": {\n",
    "                  \"type\": \"expire\"\n",
    "                }\n",
    "              }\n",
    "            ]\n",
    "          }\n",
    "\n",
    "  # ECS Cluster\n",
    "  MLModelCluster:\n",
    "    Type: AWS::ECS::Cluster\n",
    "    Properties:\n",
    "      ClusterName: ml-model-cluster\n",
    "\n",
    "  # ECS Task Definition\n",
    "  MLModelTaskDefinition:\n",
    "    Type: AWS::ECS::TaskDefinition\n",
    "    Properties:\n",
    "      Family: ml-model-task\n",
    "      NetworkMode: awsvpc\n",
    "      RequiresCompatibilities:\n",
    "        - FARGATE\n",
    "      Cpu: '1024'\n",
    "      Memory: '2048'\n",
    "      ExecutionRoleArn: !GetAtt ECSTaskExecutionRole.Arn\n",
    "      TaskRoleArn: !GetAtt ECSTaskRole.Arn\n",
    "      ContainerDefinitions:\n",
    "        - Name: ml-model-container\n",
    "          Image: !Sub ${AWS::AccountId}.dkr.ecr.${AWS::Region}.amazonaws.com/ml-model-api:latest\n",
    "          Essential: true\n",
    "          PortMappings:\n",
    "            - ContainerPort: 5000\n",
    "              HostPort: 5000\n",
    "              Protocol: tcp\n",
    "          LogConfiguration:\n",
    "            LogDriver: awslogs\n",
    "            Options:\n",
    "              awslogs-group: !Ref CloudWatchLogsGroup\n",
    "              awslogs-region: !Ref AWS::Region\n",
    "              awslogs-stream-prefix: ecs\n",
    "          Environment:\n",
    "            - Name: MODEL_VERSION\n",
    "              Value: v1\n",
    "            - Name: LOG_LEVEL\n",
    "              Value: INFO\n",
    "\n",
    "  # More resources: IAM roles, security groups, load balancer, etc.\n",
    "```\n",
    "\n",
    "### AWS CDK Example (Python)\n",
    "\n",
    "```python\n",
    "from aws_cdk import (\n",
    "    core,\n",
    "    aws_ecr as ecr,\n",
    "    aws_ecs as ecs,\n",
    "    aws_ec2 as ec2,\n",
    "    aws_iam as iam,\n",
    "    aws_elasticloadbalancingv2 as elbv2,\n",
    ")\n",
    "\n",
    "class MLModelStack(core.Stack):\n",
    "    def __init__(self, scope: core.Construct, id: str, **kwargs) -> None:\n",
    "        super().__init__(scope, id, **kwargs)\n",
    "\n",
    "        # Create ECR Repository\n",
    "        repository = ecr.Repository(\n",
    "            self, \"MLModelRepository\",\n",
    "            repository_name=\"ml-model-api\",\n",
    "            removal_policy=core.RemovalPolicy.DESTROY,\n",
    "            image_scan_on_push=True\n",
    "        )\n",
    "\n",
    "        # Create VPC\n",
    "        vpc = ec2.Vpc(self, \"MLModelVPC\", max_azs=2)\n",
    "\n",
    "        # Create ECS Cluster\n",
    "        cluster = ecs.Cluster(\n",
    "            self, \"MLModelCluster\",\n",
    "            vpc=vpc,\n",
    "            cluster_name=\"ml-model-cluster\"\n",
    "        )\n",
    "\n",
    "        # Task Definition\n",
    "        task_definition = ecs.FargateTaskDefinition(\n",
    "            self, \"MLModelTaskDef\",\n",
    "            memory_limit_mib=2048,\n",
    "            cpu=1024\n",
    "        )\n",
    "\n",
    "        # Add container to task definition\n",
    "        container = task_definition.add_container(\n",
    "            \"MLModelContainer\",\n",
    "            image=ecs.ContainerImage.from_ecr_repository(repository),\n",
    "            logging=ecs.LogDrivers.aws_logs(\n",
    "                stream_prefix=\"ml-model\"\n",
    "            ),\n",
    "            environment={\n",
    "                \"MODEL_VERSION\": \"v1\",\n",
    "                \"LOG_LEVEL\": \"INFO\"\n",
    "            }\n",
    "        )\n",
    "\n",
    "        container.add_port_mappings(\n",
    "            ecs.PortMapping(container_port=5000)\n",
    "        )\n",
    "\n",
    "        # Create Service with Load Balancer\n",
    "        service = ecs.FargateService(\n",
    "            self, \"MLModelService\",\n",
    "            cluster=cluster,\n",
    "            task_definition=task_definition,\n",
    "            desired_count=2,\n",
    "            assign_public_ip=True,\n",
    "            service_name=\"ml-model-service\"\n",
    "        )\n",
    "\n",
    "        # Add Auto Scaling\n",
    "        scaling = service.auto_scale_task_count(\n",
    "            min_capacity=2,\n",
    "            max_capacity=10\n",
    "        )\n",
    "\n",
    "        scaling.scale_on_cpu_utilization(\n",
    "            \"CpuScaling\",\n",
    "            target_utilization_percent=70,\n",
    "            scale_in_cooldown=core.Duration.seconds(60),\n",
    "            scale_out_cooldown=core.Duration.seconds(60)\n",
    "        )\n",
    "\n",
    "        # Add Load Balancer\n",
    "        lb = elbv2.ApplicationLoadBalancer(\n",
    "            self, \"MLModelLB\",\n",
    "            vpc=vpc,\n",
    "            internet_facing=True\n",
    "        )\n",
    "\n",
    "        listener = lb.add_listener(\n",
    "            \"Listener\",\n",
    "            port=80\n",
    "        )\n",
    "\n",
    "        # Add target group to the service\n",
    "        health_check = elbv2.HealthCheck(\n",
    "            path=\"/health\",\n",
    "            interval=core.Duration.seconds(30)\n",
    "        )\n",
    "\n",
    "        listener.add_targets(\n",
    "            \"MLModelTarget\",\n",
    "            port=5000,\n",
    "            targets=[service],\n",
    "            health_check=health_check\n",
    "        )\n",
    "\n",
    "        # Output the load balancer DNS\n",
    "        core.CfnOutput(\n",
    "            self, \"LoadBalancerDNS\",\n",
    "            value=lb.load_balancer_dns_name\n",
    "        )\n",
    "```\n",
    "\n",
    "## ⚠️ Important Considerations for ML Deployments on ECS\n",
    "\n",
    "1. **Resource Allocation**: ML models can be memory-intensive - ensure you allocate sufficient memory\n",
    "\n",
    "2. **Initialization Time**: ML models can take time to load - configure health check timeouts appropriately:\n",
    "   ```bash\n",
    "   # Adjust health check settings in target group\n",
    "   aws elbv2 modify-target-group \\\n",
    "     --target-group-arn arn:aws:elasticloadbalancing:us-east-1:123456789012:targetgroup/ml-model-tg/1234567890123456 \\\n",
    "     --health-check-interval-seconds 30 \\\n",
    "     --health-check-timeout-seconds 10 \\\n",
    "     --healthy-threshold-count 2 \\\n",
    "     --unhealthy-threshold-count 3 \\\n",
    "     --health-check-path /health\n",
    "   ```\n",
    "\n",
    "3. **GPU Support**: For deep learning models requiring GPU:\n",
    "   ```json\n",
    "   {\n",
    "     \"family\": \"ml-gpu-task\",\n",
    "     \"requiresCompatibilities\": [\"EC2\"],\n",
    "     \"executionRoleArn\": \"...\",\n",
    "     \"taskRoleArn\": \"...\",\n",
    "     \"networkMode\": \"awsvpc\",\n",
    "     \"containerDefinitions\": [\n",
    "       {\n",
    "         \"name\": \"ml-gpu-container\",\n",
    "         \"image\": \"...\",\n",
    "         \"resourceRequirements\": [\n",
    "           {\n",
    "             \"type\": \"GPU\",\n",
    "             \"value\": \"1\"\n",
    "           }\n",
    "         ]\n",
    "       }\n",
    "     ]\n",
    "   }\n",
    "   ```\n",
    "\n",
    "4. **Secrets Management**: For API keys or database credentials:\n",
    "   ```json\n",
    "   {\n",
    "     \"secrets\": [\n",
    "       {\n",
    "         \"name\": \"DATABASE_PASSWORD\",\n",
    "         \"valueFrom\": \"arn:aws:ssm:us-east-1:123456789012:parameter/ml-model/db-password\"\n",
    "       }\n",
    "     ]\n",
    "   }\n",
    "   ```\n",
    "\n",
    "5. **Model Storage Options**:\n",
    "   - **Small models**: Package directly in the container\n",
    "   - **Large models**: \n",
    "     - Store in S3 and download at startup\n",
    "     - Use EFS (Elastic File System) as persistent storage\n",
    "\n",
    "6. **Cost Optimization**:\n",
    "   - Use Spot instances for non-critical workloads\n",
    "   - Schedule batch prediction jobs during off-peak hours\n",
    "   - Right-size your containers (memory/CPU)\n",
    "\n",
    "## Monitoring ML Services on ECS\n",
    "\n",
    "```bash\n",
    "# Create a CloudWatch dashboard for your ML service\n",
    "aws cloudwatch put-dashboard \\\n",
    "  --dashboard-name MLModelServiceDashboard \\\n",
    "  --dashboard-body '{\n",
    "    \"widgets\": [\n",
    "      {\n",
    "        \"type\": \"metric\",\n",
    "        \"x\": 0,\n",
    "        \"y\": 0,\n",
    "        \"width\": 12,\n",
    "        \"height\": 6,\n",
    "        \"properties\": {\n",
    "          \"metrics\": [\n",
    "            [\"AWS/ECS\", \"CPUUtilization\", \"ServiceName\", \"ml-model-service\", \"ClusterName\", \"ml-model-cluster\"]\n",
    "          ],\n",
    "          \"period\": 60,\n",
    "          \"stat\": \"Average\",\n",
    "          \"region\": \"us-east-1\",\n",
    "          \"title\": \"CPU Utilization\"\n",
    "        }\n",
    "      },\n",
    "      {\n",
    "        \"type\": \"metric\",\n",
    "        \"x\": 0,\n",
    "        \"y\": 6,\n",
    "        \"width\": 12,\n",
    "        \"height\": 6,\n",
    "        \"properties\": {\n",
    "          \"metrics\": [\n",
    "            [\"AWS/ECS\", \"MemoryUtilization\", \"ServiceName\", \"ml-model-service\", \"ClusterName\", \"ml-model-cluster\"]\n",
    "          ],\n",
    "          \"period\": 60,\n",
    "          \"stat\": \"Average\",\n",
    "          \"region\": \"us-east-1\",\n",
    "          \"title\": \"Memory Utilization\"\n",
    "        }\n",
    "      }\n",
    "    ]\n",
    "  }'\n",
    "```\n",
    "\n",
    "### Key Metrics to Monitor for ML Services\n",
    "\n",
    "1. **Infrastructure Metrics**:\n",
    "   - CPU/Memory utilization\n",
    "   - Network I/O\n",
    "   - Task count\n",
    "\n",
    "2. **Application Metrics**:\n",
    "   - Prediction latency\n",
    "   - Throughput (requests per second)\n",
    "   - Error rates\n",
    "   - Model loading time\n",
    "\n",
    "3. **ML-specific Metrics**:\n",
    "   - Prediction confidence scores\n",
    "   - Feature distribution drift\n",
    "   - Model accuracy over time\n",
    "\n",
    "## Real-world Deployment Architecture\n",
    "\n",
    "For production ML systems, a complete architecture might look like:\n",
    "\n",
    "```\n",
    "[Client] → [Route 53] → [CloudFront] → [ALB] → [ECS Service] → [Model Containers]\n",
    "                                                    ↑\n",
    "                          [S3 Bucket for Models] ───┘\n",
    "                                ↑\n",
    "                        [CI/CD Pipeline] ─── [ECR]\n",
    "```\n",
    "\n",
    "This setup provides:\n",
    "- Global caching and distribution (CloudFront)\n",
    "- Load balancing and scaling (ALB + ECS auto scaling)\n",
    "- Separation of model artifacts from container images (S3)\n",
    "- Automated deployment pipelines\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Deploying ML models on AWS ECS provides a scalable, reliable platform for serving predictions. By leveraging ECR for image management and ECS for orchestration, you can build robust ML services that automatically scale based on demand.\n",
    "\n",
    "Remember to iterate on your infrastructure as your ML application evolves - start with a simple setup and add complexity (monitoring, auto-scaling, CI/CD) as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Setting up CI/CD Pipelines for ML Applications\n",
    "\n",
    "## What is CI/CD for ML Projects?\n",
    "\n",
    "Continuous Integration and Continuous Deployment (CI/CD) for Machine Learning involves automating the testing, building, and deployment of ML models and applications. It extends traditional software CI/CD practices to handle ML-specific challenges.\n",
    "\n",
    "## Key CI/CD Components for ML Applications\n",
    "\n",
    "| Component | Traditional Software CI/CD | ML-Specific CI/CD |\n",
    "|-----------|----------------------------|-------------------|\n",
    "| **Code Validation** | Syntax checks, linting | Data validation, model validation |\n",
    "| **Testing** | Unit tests, integration tests | Model performance tests, data drift tests |\n",
    "| **Building** | Creating executable artifacts | Training models, packaging models |\n",
    "| **Deployment** | Deploying code to servers | Deploying models to inference endpoints |\n",
    "| **Monitoring** | Application metrics | Model performance, drift detection |\n",
    "\n",
    "## GitHub Actions for ML CI/CD\n",
    "\n",
    "GitHub Actions is a powerful CI/CD platform integrated directly with GitHub repositories that allows you to automate your ML workflow.\n",
    "\n",
    "### Key Components\n",
    "\n",
    "1. **Workflows**: YAML files in `.github/workflows/` directory that define automated processes\n",
    "2. **Events**: Triggers that start workflows (push, pull request, scheduled, etc.)\n",
    "3. **Jobs**: Groups of steps that run on a runner (virtual machine)\n",
    "4. **Steps**: Individual tasks that run commands or actions\n",
    "5. **Actions**: Reusable units of code for common tasks\n",
    "\n",
    "## Example GitHub Actions Workflow for ML Project\n",
    "\n",
    "```yaml\n",
    "name: ML Model CI/CD Pipeline\n",
    "\n",
    "on:\n",
    "  push:\n",
    "    branches: [ main ]\n",
    "  pull_request:\n",
    "    branches: [ main ]\n",
    "  # Schedule training job weekly\n",
    "  schedule:\n",
    "    - cron: '0 0 * * 0'  # Run at midnight on Sundays\n",
    "\n",
    "jobs:\n",
    "  test:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v2\n",
    "      \n",
    "      - name: Set up Python\n",
    "        uses: actions/setup-python@v2\n",
    "        with:\n",
    "          python-version: '3.9'\n",
    "          \n",
    "      - name: Install dependencies\n",
    "        run: |\n",
    "          python -m pip install --upgrade pip\n",
    "          pip install -r requirements.txt\n",
    "          pip install pytest pytest-cov\n",
    "          \n",
    "      - name: Lint with flake8\n",
    "        run: |\n",
    "          pip install flake8\n",
    "          flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n",
    "          \n",
    "      - name: Test with pytest\n",
    "        run: |\n",
    "          pytest tests/ --cov=src/ --cov-report=xml\n",
    "          \n",
    "      - name: Upload coverage report\n",
    "        uses: codecov/codecov-action@v1\n",
    "        with:\n",
    "          file: ./coverage.xml\n",
    "          \n",
    "  train:\n",
    "    needs: test\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v2\n",
    "      \n",
    "      - name: Set up Python\n",
    "        uses: actions/setup-python@v2\n",
    "        with:\n",
    "          python-version: '3.9'\n",
    "          \n",
    "      - name: Install dependencies\n",
    "        run: |\n",
    "          python -m pip install --upgrade pip\n",
    "          pip install -r requirements.txt\n",
    "          \n",
    "      - name: Train model\n",
    "        run: python src/train.py\n",
    "          \n",
    "      - name: Evaluate model\n",
    "        run: python src/evaluate.py\n",
    "        \n",
    "      - name: Upload model artifacts\n",
    "        uses: actions/upload-artifact@v2\n",
    "        with:\n",
    "          name: model-artifacts\n",
    "          path: models/model.pkl\n",
    "  \n",
    "  deploy:\n",
    "    needs: train\n",
    "    if: github.event_name == 'push' && github.ref == 'refs/heads/main'\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v2\n",
    "      \n",
    "      - name: Download model artifacts\n",
    "        uses: actions/download-artifact@v2\n",
    "        with:\n",
    "          name: model-artifacts\n",
    "          path: models/\n",
    "      \n",
    "      - name: Configure AWS credentials\n",
    "        uses: aws-actions/configure-aws-credentials@v1\n",
    "        with:\n",
    "          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}\n",
    "          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n",
    "          aws-region: us-east-1\n",
    "          \n",
    "      - name: Login to Amazon ECR\n",
    "        id: login-ecr\n",
    "        uses: aws-actions/amazon-ecr-login@v1\n",
    "        \n",
    "      - name: Build, tag, and push image to Amazon ECR\n",
    "        env:\n",
    "          ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}\n",
    "          ECR_REPOSITORY: ml-model-api\n",
    "          IMAGE_TAG: ${{ github.sha }}\n",
    "        run: |\n",
    "          docker build -t $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG .\n",
    "          docker push $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG\n",
    "          \n",
    "      - name: Update ECS service\n",
    "        run: |\n",
    "          aws ecs update-service --cluster ml-model-cluster --service ml-model-service --force-new-deployment\n",
    "```\n",
    "\n",
    "**Workflow Explanation:**\n",
    "\n",
    "1. **Triggers**:\n",
    "   - Code pushes to main branch\n",
    "   - Pull requests to main branch\n",
    "   - Weekly schedule for regular retraining\n",
    "\n",
    "2. **Jobs**:\n",
    "   - **test**: Runs linting and unit tests to validate code quality\n",
    "   - **train**: Trains and evaluates the ML model\n",
    "   - **deploy**: Builds Docker image and deploys to AWS ECS\n",
    "\n",
    "3. **Key Steps**:\n",
    "   - Install dependencies\n",
    "   - Run tests with code coverage\n",
    "   - Train and evaluate model\n",
    "   - Build and push Docker image to ECR\n",
    "   - Deploy updated model to ECS\n",
    "\n",
    "## Testing in ML Pipelines\n",
    "\n",
    "### Unit Tests for ML Code\n",
    "\n",
    "```python\n",
    "# test_preprocessing.py\n",
    "import pytest\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from src.preprocessing import normalize_features, handle_missing_values\n",
    "\n",
    "def test_normalize_features():\n",
    "    # Create test data\n",
    "    test_data = pd.DataFrame({\n",
    "        'feature1': [1, 2, 3, 4, 5],\n",
    "        'feature2': [10, 20, 30, 40, 50]\n",
    "    })\n",
    "    \n",
    "    # Apply normalization\n",
    "    normalized = normalize_features(test_data)\n",
    "    \n",
    "    # Check that values are normalized (0-1 range)\n",
    "    assert normalized['feature1'].max() <= 1.0\n",
    "    assert normalized['feature1'].min() >= 0.0\n",
    "    assert normalized['feature2'].max() <= 1.0\n",
    "    assert normalized['feature2'].min() >= 0.0\n",
    "    \n",
    "def test_handle_missing_values():\n",
    "    # Create test data with missing values\n",
    "    test_data = pd.DataFrame({\n",
    "        'feature1': [1, np.nan, 3, 4, 5],\n",
    "        'feature2': [10, 20, np.nan, 40, 50]\n",
    "    })\n",
    "    \n",
    "    # Apply missing value handling\n",
    "    processed = handle_missing_values(test_data)\n",
    "    \n",
    "    # Check that no NaN values remain\n",
    "    assert processed.isna().sum().sum() == 0\n",
    "```\n",
    "\n",
    "### Model Tests\n",
    "\n",
    "```python\n",
    "# test_model.py\n",
    "import pytest\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from src.model import train_model, predict\n",
    "\n",
    "@pytest.fixture\n",
    "def sample_data():\n",
    "    \"\"\"Generate sample data for testing\"\"\"\n",
    "    X, y = make_classification(n_samples=100, n_features=5, random_state=42)\n",
    "    return pd.DataFrame(X), pd.Series(y)\n",
    "\n",
    "def test_model_training(sample_data):\n",
    "    \"\"\"Test that model training runs without errors and returns a model object\"\"\"\n",
    "    X, y = sample_data\n",
    "    model = train_model(X, y)\n",
    "    assert model is not None\n",
    "    \n",
    "def test_model_prediction(sample_data):\n",
    "    \"\"\"Test that model predictions are the expected shape and type\"\"\"\n",
    "    X, y = sample_data\n",
    "    model = train_model(X, y)\n",
    "    \n",
    "    # Test on same data (not best practice but OK for unit test)\n",
    "    predictions = predict(model, X)\n",
    "    \n",
    "    # Check output format\n",
    "    assert len(predictions) == len(X)\n",
    "    assert all(isinstance(pred, (int, float, np.integer, np.float)) for pred in predictions)\n",
    "    \n",
    "def test_model_performance(sample_data):\n",
    "    \"\"\"Test that model meets minimum performance threshold\"\"\"\n",
    "    X, y = sample_data\n",
    "    model = train_model(X, y)\n",
    "    \n",
    "    # Split into train/test\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    model = train_model(X_train, y_train)\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    predictions = predict(model, X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    \n",
    "    # Ensure model performs better than a dummy baseline\n",
    "    assert accuracy > 0.6\n",
    "```\n",
    "\n",
    "## Setting up a Complete ML CI/CD Pipeline\n",
    "\n",
    "### 1. Prepare Your Repository Structure\n",
    "\n",
    "```\n",
    "ml-project/\n",
    "├── .github/\n",
    "│   └── workflows/\n",
    "│       └── ml-pipeline.yml   # GitHub Actions workflow\n",
    "├── src/\n",
    "│   ├── __init__.py\n",
    "│   ├── preprocessing.py      # Data preprocessing code\n",
    "│   ├── train.py              # Model training code\n",
    "│   ├── evaluate.py           # Model evaluation code\n",
    "│   ├── predict.py            # Prediction code\n",
    "│   └── app.py                # Flask API\n",
    "├── tests/\n",
    "│   ├── __init__.py\n",
    "│   ├── test_preprocessing.py\n",
    "│   ├── test_model.py\n",
    "│   └── test_api.py\n",
    "├── data/\n",
    "│   └── .gitignore            # Ignore large data files\n",
    "├── models/\n",
    "│   └── .gitignore            # Ignore model files\n",
    "├── Dockerfile                # Container definition\n",
    "├── docker-compose.yml        # Local development setup\n",
    "├── requirements.txt          # Python dependencies\n",
    "└── README.md                 # Project documentation\n",
    "```\n",
    "\n",
    "### 2. Create Secrets for CI/CD\n",
    "\n",
    "In GitHub repository settings, add the following secrets:\n",
    "- `AWS_ACCESS_KEY_ID`\n",
    "- `AWS_SECRET_ACCESS_KEY`\n",
    "- `ECR_REPOSITORY`\n",
    "- `ECS_CLUSTER`\n",
    "- `ECS_SERVICE`\n",
    "\n",
    "### 3. Define Deployment Infrastructure\n",
    "\n",
    "Create AWS resources using CloudFormation, Terraform, or AWS CDK (sample below uses Terraform).\n",
    "\n",
    "```hcl\n",
    "# main.tf\n",
    "provider \"aws\" {\n",
    "  region = \"us-east-1\"\n",
    "}\n",
    "\n",
    "# ECR Repository\n",
    "resource \"aws_ecr_repository\" \"ml_model_repo\" {\n",
    "  name = \"ml-model-api\"\n",
    "}\n",
    "\n",
    "# ECS Cluster\n",
    "resource \"aws_ecs_cluster\" \"ml_cluster\" {\n",
    "  name = \"ml-model-cluster\"\n",
    "}\n",
    "\n",
    "# ECS Task Definition\n",
    "resource \"aws_ecs_task_definition\" \"ml_task\" {\n",
    "  family                   = \"ml-model-task\"\n",
    "  requires_compatibilities = [\"FARGATE\"]\n",
    "  network_mode             = \"awsvpc\"\n",
    "  cpu                      = \"1024\"\n",
    "  memory                   = \"2048\"\n",
    "  execution_role_arn       = aws_iam_role.ecs_execution_role.arn\n",
    "  task_role_arn            = aws_iam_role.ecs_task_role.arn\n",
    "  \n",
    "  container_definitions = jsonencode([\n",
    "    {\n",
    "      name      = \"ml-model-container\"\n",
    "      image     = \"${aws_ecr_repository.ml_model_repo.repository_url}:latest\"\n",
    "      essential = true\n",
    "      portMappings = [\n",
    "        {\n",
    "          containerPort = 5000\n",
    "          hostPort      = 5000\n",
    "        }\n",
    "      ]\n",
    "      logConfiguration = {\n",
    "        logDriver = \"awslogs\"\n",
    "        options = {\n",
    "          \"awslogs-group\"         = \"/ecs/ml-model-task\"\n",
    "          \"awslogs-region\"        = \"us-east-1\"\n",
    "          \"awslogs-stream-prefix\" = \"ecs\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  ])\n",
    "}\n",
    "\n",
    "# More resources: IAM roles, VPC, security groups, load balancer, etc.\n",
    "```\n",
    "\n",
    "### 4. Advanced ML Pipeline Features\n",
    "\n",
    "#### Model Version Tracking\n",
    "\n",
    "```python\n",
    "# src/train.py\n",
    "import mlflow\n",
    "import os\n",
    "\n",
    "# Set tracking URI (could be MLflow server or local directory)\n",
    "mlflow.set_tracking_uri(os.environ.get(\"MLFLOW_TRACKING_URI\", \"file:./mlruns\"))\n",
    "\n",
    "# Start a run\n",
    "with mlflow.start_run(run_name=\"model_training\") as run:\n",
    "    # Your training code\n",
    "    model = train_model(X_train, y_train)\n",
    "    \n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"learning_rate\", learning_rate)\n",
    "    mlflow.log_param(\"max_depth\", max_depth)\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    mlflow.log_metric(\"precision\", precision)\n",
    "    \n",
    "    # Log model\n",
    "    mlflow.sklearn.log_model(model, \"model\")\n",
    "    \n",
    "    # Tag with Git commit\n",
    "    mlflow.set_tag(\"git_commit\", os.environ.get(\"GITHUB_SHA\"))\n",
    "    \n",
    "    # Get run ID for deployment\n",
    "    run_id = run.info.run_id\n",
    "    \n",
    "    # Save run ID to file for subsequent steps\n",
    "    with open(\"run_id.txt\", \"w\") as f:\n",
    "        f.write(run_id)\n",
    "```\n",
    "\n",
    "#### Feature Store Integration\n",
    "\n",
    "```python\n",
    "# src/preprocessing.py\n",
    "from feast import FeatureStore\n",
    "\n",
    "def get_training_features(entity_df):\n",
    "    \"\"\"Get features from the feature store for training\"\"\"\n",
    "    store = FeatureStore(repo_path=\"./feature_repo\")\n",
    "    \n",
    "    # Get historical features\n",
    "    training_df = store.get_historical_features(\n",
    "        entity_df=entity_df,\n",
    "        features=[\n",
    "            \"customer_features:age\",\n",
    "            \"customer_features:income\",\n",
    "            \"transaction_features:purchase_frequency\"\n",
    "        ]\n",
    "    ).to_df()\n",
    "    \n",
    "    return training_df\n",
    "```\n",
    "\n",
    "#### A/B Testing Deployment\n",
    "\n",
    "```yaml\n",
    "# .github/workflows/ab-testing.yml\n",
    "name: A/B Test Deployment\n",
    "\n",
    "on:\n",
    "  workflow_dispatch:\n",
    "    inputs:\n",
    "      version_a:\n",
    "        description: 'Version A model ID'\n",
    "        required: true\n",
    "      version_b:\n",
    "        description: 'Version B model ID'\n",
    "        required: true\n",
    "      traffic_split:\n",
    "        description: 'Percentage of traffic to version B (0-100)'\n",
    "        required: true\n",
    "        default: '10'\n",
    "\n",
    "jobs:\n",
    "  deploy:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v2\n",
    "      \n",
    "      # Configure AWS credentials and other setup steps...\n",
    "      \n",
    "      - name: Deploy A/B test\n",
    "        run: |\n",
    "          # Update ECS task definition with both model versions\n",
    "          MODEL_A=${{ github.event.inputs.version_a }}\n",
    "          MODEL_B=${{ github.event.inputs.version_b }}\n",
    "          TRAFFIC_SPLIT=${{ github.event.inputs.traffic_split }}\n",
    "          \n",
    "          # Create task definition with environment variables for A/B testing\n",
    "          cat > task-definition.json << EOF\n",
    "          {\n",
    "            \"containerDefinitions\": [\n",
    "              {\n",
    "                \"name\": \"ml-model-container\",\n",
    "                \"image\": \"${ECR_REGISTRY}/${ECR_REPOSITORY}:latest\",\n",
    "                \"environment\": [\n",
    "                  {\"name\": \"MODEL_A_ID\", \"value\": \"${MODEL_A}\"},\n",
    "                  {\"name\": \"MODEL_B_ID\", \"value\": \"${MODEL_B}\"},\n",
    "                  {\"name\": \"TRAFFIC_SPLIT\", \"value\": \"${TRAFFIC_SPLIT}\"}\n",
    "                ]\n",
    "              }\n",
    "            ]\n",
    "          }\n",
    "          EOF\n",
    "          \n",
    "          # Register and deploy new task definition\n",
    "          aws ecs register-task-definition --cli-input-json file://task-definition.json\n",
    "          aws ecs update-service --cluster ${ECS_CLUSTER} --service ${ECS_SERVICE} --force-new-deployment\n",
    "```\n",
    "\n",
    "## Automated Model Retraining\n",
    "\n",
    "```yaml\n",
    "# .github/workflows/model-retraining.yml\n",
    "name: Automated Model Retraining\n",
    "\n",
    "on:\n",
    "  schedule:\n",
    "    - cron: '0 0 * * 1'  # Every Monday at midnight\n",
    "  workflow_dispatch:  # Manual trigger\n",
    "\n",
    "jobs:\n",
    "  check_data_drift:\n",
    "    runs-on: ubuntu-latest\n",
    "    outputs:\n",
    "      drift_detected: ${{ steps.check_drift.outputs.drift_detected }}\n",
    "    steps:\n",
    "      - uses: actions/checkout@v2\n",
    "      \n",
    "      - name: Set up Python\n",
    "        uses: actions/setup-python@v2\n",
    "        with:\n",
    "          python-version: '3.9'\n",
    "          \n",
    "      - name: Install dependencies\n",
    "        run: |\n",
    "          python -m pip install --upgrade pip\n",
    "          pip install -r requirements.txt\n",
    "          \n",
    "      - name: Check for data drift\n",
    "        id: check_drift\n",
    "        run: |\n",
    "          python src/check_drift.py\n",
    "          echo \"::set-output name=drift_detected::$(cat drift_detected.txt)\"\n",
    "  \n",
    "  retrain_model:\n",
    "    needs: check_data_drift\n",
    "    if: needs.check_data_drift.outputs.drift_detected == 'true'\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v2\n",
    "      \n",
    "      # Set up environment, train model, evaluate, and deploy as in previous example\n",
    "```\n",
    "\n",
    "## Full End-to-End ML CI/CD Example for a Production System\n",
    "\n",
    "Here's a comprehensive example for a production-grade ML system that integrates all the best practices:\n",
    "\n",
    "```yaml\n",
    "# .github/workflows/production-ml-pipeline.yml\n",
    "name: Production ML Pipeline\n",
    "\n",
    "on:\n",
    "  push:\n",
    "    branches: [ main ]\n",
    "    paths:\n",
    "      - 'src/**'\n",
    "      - 'models/**'\n",
    "      - 'tests/**'\n",
    "      - 'Dockerfile'\n",
    "      - 'requirements.txt'\n",
    "  pull_request:\n",
    "    branches: [ main ]\n",
    "  workflow_dispatch:\n",
    "  schedule:\n",
    "    - cron: '0 0 * * 1'  # Weekly retraining\n",
    "\n",
    "jobs:\n",
    "  lint_and_test:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v2\n",
    "      \n",
    "      - name: Set up Python\n",
    "        uses: actions/setup-python@v2\n",
    "        with:\n",
    "          python-version: '3.9'\n",
    "          \n",
    "      - name: Cache pip packages\n",
    "        uses: actions/cache@v2\n",
    "        with:\n",
    "          path: ~/.cache/pip\n",
    "          key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}\n",
    "          \n",
    "      - name: Install dependencies\n",
    "        run: |\n",
    "          python -m pip install --upgrade pip\n",
    "          pip install -r requirements.txt\n",
    "          pip install pytest pytest-cov black isort mypy\n",
    "          \n",
    "      - name: Lint with black\n",
    "        run: black --check src tests\n",
    "          \n",
    "      - name: Check imports with isort\n",
    "        run: isort --check-only --profile black src tests\n",
    "          \n",
    "      - name: Type check with mypy\n",
    "        run: mypy src\n",
    "          \n",
    "      - name: Test with pytest\n",
    "        run: |\n",
    "          pytest tests/ --cov=src/ --cov-report=xml --junitxml=test-results.xml\n",
    "          \n",
    "      - name: Upload coverage report\n",
    "        uses: codecov/codecov-action@v1\n",
    "        with:\n",
    "          file: ./coverage.xml\n",
    "          \n",
    "      - name: Upload test results\n",
    "        uses: actions/upload-artifact@v2\n",
    "        with:\n",
    "          name: test-results\n",
    "          path: test-results.xml\n",
    "  \n",
    "  train_and_evaluate:\n",
    "    needs: lint_and_test\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v2\n",
    "      \n",
    "      - name: Set up Python\n",
    "        uses: actions/setup-python@v2\n",
    "        with:\n",
    "          python-version: '3.9'\n",
    "          \n",
    "      - name: Install dependencies\n",
    "        run: |\n",
    "          python -m pip install --upgrade pip\n",
    "          pip install -r requirements.txt\n",
    "          \n",
    "      - name: Download latest data\n",
    "        run: |\n",
    "          aws s3 cp s3://my-ml-data-bucket/latest/ data/ --recursive\n",
    "        env:\n",
    "          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}\n",
    "          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n",
    "          \n",
    "      - name: Train model\n",
    "        id: train\n",
    "        run: |\n",
    "          python src/train.py --config config/production.yaml\n",
    "          # Get model ID from MLflow\n",
    "          MODEL_ID=$(cat model_id.txt)\n",
    "          echo \"::set-output name=model_id::$MODEL_ID\"\n",
    "          \n",
    "      - name: Evaluate model\n",
    "        id: evaluate\n",
    "        run: |\n",
    "          python src/evaluate.py --model-id ${{ steps.train.outputs.model_id }}\n",
    "          # Check if model meets performance threshold\n",
    "          MEETS_THRESHOLD=$(cat meets_threshold.txt)\n",
    "          echo \"::set-output name=meets_threshold::$MEETS_THRESHOLD\"\n",
    "          \n",
    "      - name: Register model if performance is good\n",
    "        if: steps.evaluate.outputs.meets_threshold == 'true'\n",
    "        run: |\n",
    "          python src/register_model.py --model-id ${{ steps.train.outputs.model_id }} --stage \"Production\"\n",
    "          \n",
    "      - name: Upload model artifacts\n",
    "        uses: actions/upload-artifact@v2\n",
    "        with:\n",
    "          name: model-artifacts\n",
    "          path: |\n",
    "            models/\n",
    "            metrics/\n",
    "            \n",
    "  build_and_push:\n",
    "    needs: [train_and_evaluate]\n",
    "    if: needs.train_and_evaluate.outputs.meets_threshold == 'true' && github.event_name != 'pull_request'\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v2\n",
    "      \n",
    "      - name: Download model artifacts\n",
    "        uses: actions/download-artifact@v2\n",
    "        with:\n",
    "          name: model-artifacts\n",
    "          \n",
    "      - name: Set up Docker Buildx\n",
    "        uses: docker/setup-buildx-action@v1\n",
    "        \n",
    "      - name: Cache Docker layers\n",
    "        uses: actions/cache@v2\n",
    "        with:\n",
    "          path: /tmp/.buildx-cache\n",
    "          key: ${{ runner.os }}-buildx-${{ github.sha }}\n",
    "          restore-keys: |\n",
    "            ${{ runner.os }}-buildx-\n",
    "        \n",
    "      - name: Login to Amazon ECR\n",
    "        id: login-ecr\n",
    "        uses: aws-actions/amazon-ecr-login@v1\n",
    "        env:\n",
    "          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}\n",
    "          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n",
    "          \n",
    "      - name: Build and push\n",
    "        uses: docker/build-push-action@v2\n",
    "        with:\n",
    "          context: .\n",
    "          push: true\n",
    "          tags: |\n",
    "            ${{ steps.login-ecr.outputs.registry }}/${{ secrets.ECR_REPOSITORY }}:latest\n",
    "            ${{ steps.login-ecr.outputs.registry }}/${{ secrets.ECR_REPOSITORY }}:${{ github.sha }}\n",
    "          cache-from: type=local,src=/tmp/.buildx-cache\n",
    "          cache-to: type=local,dest=/tmp/.buildx-cache-new\n",
    "          \n",
    "      # Move cache\n",
    "      - name: Move cache\n",
    "        run: |\n",
    "          rm -rf /tmp/.buildx-cache\n",
    "          mv /tmp/.buildx-cache-new /tmp/.buildx-cache\n",
    "          \n",
    "  deploy:\n",
    "    needs: build_and_push\n",
    "    runs-on: ubuntu-latest\n",
    "    environment: production\n",
    "    steps:\n",
    "      - uses: actions/checkout@v2\n",
    "      \n",
    "      - name: Configure AWS credentials\n",
    "        uses: aws-actions/configure-aws-credentials@v1\n",
    "        with:\n",
    "          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}\n",
    "          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n",
    "          aws-region: us-east-1\n",
    "          \n",
    "      - name: Update task definition\n",
    "        id: task-def\n",
    "        uses: aws-actions/amazon-ecs-render-task-definition@v1\n",
    "        with:\n",
    "          task-definition: infrastructure/task-definition.json\n",
    "          container-name: ml-model-container\n",
    "          image: ${{ steps.login-ecr.outputs.registry }}/${{ secrets.ECR_REPOSITORY }}:${{ github.sha }}\n",
    "          \n",
    "      - name: Deploy to Amazon ECS\n",
    "        uses: aws-actions/amazon-ecs-deploy-task-definition@v1\n",
    "        with:\n",
    "          task-definition: ${{ steps.task-def.outputs.task-definition }}\n",
    "          service: ${{ secrets.ECS_SERVICE }}\n",
    "          cluster: ${{ secrets.ECS_CLUSTER }}\n",
    "          wait-for-service-stability: true\n",
    "          \n",
    "      - name: Notify deployment\n",
    "        uses: slackapi/slack-github-action@v1.18.0\n",
    "        with:\n",
    "          payload: |\n",
    "            {\n",
    "              \"text\": \"🚀 New model deployed to production!\\nCommit: ${{ github.sha }}\\nAuthor: ${{ github.actor }}\"\n",
    "            }\n",
    "        env:\n",
    "          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}\n",
    "          \n",
    "  monitoring:\n",
    "    needs: deploy\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v2\n",
    "      \n",
    "      - name: Set up monitoring for new model\n",
    "        run: |\n",
    "          python src/setup_monitoring.py --model-version ${{ github.sha }}\n",
    "        env:\n",
    "          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}\n",
    "          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n",
    "```\n",
    "\n",
    "## ⚠️ CI/CD Best Practices for ML Projects\n",
    "\n",
    "1. **Version Control Everything**:\n",
    "   - Code, configuration, data schemas, and model artifacts\n",
    "   - Use Git LFS or DVC for large files\n",
    "\n",
    "2. **Modularize Your Code**:\n",
    "   - Separate preprocessing, training, evaluation, and serving code\n",
    "   - Makes testing and maintenance easier\n",
    "\n",
    "3. **Automate Tests at Multiple Levels**:\n",
    "   - Unit tests for individual functions\n",
    "   - Integration tests for components working together\n",
    "   - System tests for end-to-end workflows\n",
    "\n",
    "4. **Use Environment Variables for Configuration**:\n",
    "   - Keep credentials out of code\n",
    "   - Configure behavior based on environment (dev/staging/prod)\n",
    "\n",
    "5. **Implement Model Validation Gates**:\n",
    "   - Only deploy models that meet performance thresholds\n",
    "   - Compare new model performance against baseline\n",
    "\n",
    "6. **Track Data and Model Lineage**:\n",
    "   - Know which data produced which model\n",
    "   - Enables reproducibility and debugging\n",
    "\n",
    "7. **Monitor Deployed Models**:\n",
    "   - Track data drift, performance degradation\n",
    "   - Set up alerts for model health\n",
    "\n",
    "8. **Use Infrastructure as Code**:\n",
    "   - Define all infrastructure in code (Terraform, CloudFormation)\n",
    "   - Makes environments reproducible and testable\n",
    "\n",
    "9. **Implement Progressive Deployment**:\n",
    "   - Canary releases\n",
    "   - A/B testing for models\n",
    "\n",
    "10. **Document Everything**:\n",
    "    - Include README files\n",
    "    - Add comments to CI/CD configuration\n",
    "    - Document model assumptions and limitations\n",
    "\n",
    "By implementing these CI/CD practices, you'll create a robust ML deployment pipeline that ensures your models are tested thoroughly, deployed safely, and monitored continuously in production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. MLFlow for Experiment Tracking in ML Projects\n",
    "\n",
    "## What is MLFlow?\n",
    "\n",
    "MLFlow is an open-source platform designed to manage the complete machine learning lifecycle. It provides tools for tracking experiments, packaging code into reproducible runs, and sharing and deploying models.\n",
    "\n",
    "## Key Components of MLFlow\n",
    "\n",
    "| Component | Purpose | Key Features |\n",
    "|-----------|---------|--------------|\n",
    "| **MLFlow Tracking** | Record and query experiments | Track parameters, metrics, artifacts, and model info |\n",
    "| **MLFlow Projects** | Package ML code for reproducibility | Define dependencies and entry points in standard format |\n",
    "| **MLFlow Models** | Package models for various deployment platforms | Standard model format and saving interfaces |\n",
    "| **MLFlow Registry** | Central model store with lifecycle management | Version control for ML models |\n",
    "| **MLFlow UI** | Visualize and compare results | Web interface for experiment management |\n",
    "\n",
    "## MLFlow Tracking Core Concepts\n",
    "\n",
    "1. **Experiment**: Container for organizing related runs (default = \"Default\")\n",
    "2. **Run**: A single execution of model code\n",
    "3. **Parameters**: Input variables (hyperparameters, dataset paths)\n",
    "4. **Metrics**: Performance measurements (accuracy, loss)\n",
    "5. **Artifacts**: Files produced during the run (models, plots, datasets)\n",
    "6. **Tags**: Additional metadata for organizing and filtering\n",
    "\n",
    "## Getting Started with MLFlow\n",
    "\n",
    "```python\n",
    "# Install MLFlow\n",
    "pip install mlflow\n",
    "\n",
    "# Basic Usage Example\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Start a run\n",
    "with mlflow.start_run(run_name=\"my_first_run\"):\n",
    "    # Set parameters\n",
    "    n_estimators = 100\n",
    "    max_depth = 5\n",
    "    \n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"n_estimators\", n_estimators)\n",
    "    mlflow.log_param(\"max_depth\", max_depth)\n",
    "    \n",
    "    # Train model\n",
    "    model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Log metrics\n",
    "    predictions = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    \n",
    "    # Log model\n",
    "    mlflow.sklearn.log_model(model, \"random_forest_model\")\n",
    "    \n",
    "    # Log artifacts\n",
    "    # Generate a feature importance plot\n",
    "    import matplotlib.pyplot as plt\n",
    "    feature_importance = model.feature_importances_\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(range(len(feature_importance)), feature_importance)\n",
    "    plt.yticks(range(len(feature_importance)), feature_names)\n",
    "    plt.title(\"Feature Importance\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"feature_importance.png\")\n",
    "    \n",
    "    # Log the plot as an artifact\n",
    "    mlflow.log_artifact(\"feature_importance.png\")\n",
    "```\n",
    "\n",
    "## Running the MLFlow UI\n",
    "\n",
    "```bash\n",
    "# Start MLFlow UI server with default SQLite backend\n",
    "mlflow ui\n",
    "\n",
    "# Start with custom tracking URI (e.g., MySQL, PostgreSQL)\n",
    "mlflow ui --backend-store-uri postgresql://username:password@localhost/mlflow_db\n",
    "```\n",
    "\n",
    "## ⚠️ Advanced MLFlow Tracking\n",
    "\n",
    "### Nested Runs for Complex Workflows\n",
    "\n",
    "```python\n",
    "# Parent run\n",
    "with mlflow.start_run(run_name=\"parent_run\") as parent_run:\n",
    "    mlflow.log_param(\"parent_param\", \"parent_value\")\n",
    "    \n",
    "    # Child run 1 - Data preprocessing\n",
    "    with mlflow.start_run(run_name=\"data_preprocessing\", nested=True) as child_run1:\n",
    "        mlflow.log_param(\"preprocessing_method\", \"standard_scaling\")\n",
    "        # Preprocessing code here\n",
    "        mlflow.log_metric(\"preprocessing_time\", 10.5)\n",
    "    \n",
    "    # Child run 2 - Model training\n",
    "    with mlflow.start_run(run_name=\"model_training\", nested=True) as child_run2:\n",
    "        mlflow.log_param(\"algorithm\", \"random_forest\")\n",
    "        # Training code here\n",
    "        mlflow.log_metric(\"training_time\", 120.3)\n",
    "        mlflow.log_metric(\"accuracy\", 0.92)\n",
    "        \n",
    "    # Child run 3 - Model evaluation\n",
    "    with mlflow.start_run(run_name=\"model_evaluation\", nested=True) as child_run3:\n",
    "        # Evaluation code here\n",
    "        mlflow.log_metric(\"auc\", 0.95)\n",
    "```\n",
    "\n",
    "### Automatic Parameter Logging\n",
    "\n",
    "```python\n",
    "# Scikit-learn model with automatic logging\n",
    "import mlflow.sklearn\n",
    "\n",
    "# Enable autologging\n",
    "mlflow.sklearn.autolog()\n",
    "\n",
    "# Now train your model - parameters, metrics, and model will be logged automatically\n",
    "model = RandomForestClassifier(n_estimators=100, max_depth=5)\n",
    "model.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "### Using Different Experiment Names\n",
    "\n",
    "```python\n",
    "# Create and set experiment\n",
    "mlflow.set_experiment(\"customer_churn_prediction\")\n",
    "\n",
    "# Start a run under this experiment\n",
    "with mlflow.start_run():\n",
    "    # Your code here\n",
    "    pass\n",
    "```\n",
    "\n",
    "## Logging Different Types of Artifacts\n",
    "\n",
    "```python\n",
    "# Log a Pandas DataFrame\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]})\n",
    "mlflow.log_table(data=df, artifact_file=\"data_sample.json\")\n",
    "\n",
    "# Log a confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.savefig(\"confusion_matrix.png\")\n",
    "mlflow.log_artifact(\"confusion_matrix.png\")\n",
    "\n",
    "# Log a text file\n",
    "with open(\"model_summary.txt\", \"w\") as f:\n",
    "    f.write(f\"Model trained on {len(X_train)} samples\\n\")\n",
    "    f.write(f\"Test accuracy: {accuracy}\\n\")\n",
    "mlflow.log_artifact(\"model_summary.txt\")\n",
    "```\n",
    "\n",
    "## MLFlow Projects for Reproducible Runs\n",
    "\n",
    "### Example MLproject File\n",
    "\n",
    "```yaml\n",
    "# MLproject\n",
    "\n",
    "name: my_ml_project\n",
    "\n",
    "conda_env: conda.yaml\n",
    "\n",
    "entry_points:\n",
    "  main:\n",
    "    parameters:\n",
    "      data_path: {type: string, default: \"data/input.csv\"}\n",
    "      n_estimators: {type: int, default: 100}\n",
    "      max_depth: {type: int, default: 5}\n",
    "      test_size: {type: float, default: 0.2}\n",
    "    command: \"python train.py --data-path {data_path} --n-estimators {n_estimators} --max-depth {max_depth} --test-size {test_size}\"\n",
    "  \n",
    "  preprocess:\n",
    "    parameters:\n",
    "      raw_data_path: {type: string}\n",
    "      output_path: {type: string}\n",
    "    command: \"python preprocess.py --raw-data-path {raw_data_path} --output-path {output_path}\"\n",
    "```\n",
    "\n",
    "### Corresponding conda.yaml File\n",
    "\n",
    "```yaml\n",
    "# conda.yaml\n",
    "\n",
    "name: ml_project_env\n",
    "channels:\n",
    "  - defaults\n",
    "  - conda-forge\n",
    "dependencies:\n",
    "  - python=3.9\n",
    "  - pip\n",
    "  - numpy=1.21.5\n",
    "  - pandas=1.3.5\n",
    "  - scikit-learn=1.0.2\n",
    "  - matplotlib=3.5.1\n",
    "  - seaborn=0.11.2\n",
    "  - pip:\n",
    "    - mlflow==1.25.1\n",
    "```\n",
    "\n",
    "### Running MLFlow Projects\n",
    "\n",
    "```bash\n",
    "# Run the project from a local directory\n",
    "mlflow run ./my_project -P n_estimators=200 -P max_depth=10\n",
    "\n",
    "# Run from a Git repository\n",
    "mlflow run https://github.com/username/ml-project.git -P n_estimators=200\n",
    "```\n",
    "\n",
    "## MLFlow Models: Packaging for Deployment\n",
    "\n",
    "```python\n",
    "# Log a scikit-learn model\n",
    "import mlflow.sklearn\n",
    "\n",
    "with mlflow.start_run():\n",
    "    model = RandomForestClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Log the model\n",
    "    mlflow.sklearn.log_model(\n",
    "        model, \n",
    "        \"random_forest_model\",\n",
    "        signature=mlflow.models.signature.infer_signature(X_train, model.predict(X_train)),\n",
    "        input_example=X_train.iloc[0:5]\n",
    "    )\n",
    "```\n",
    "\n",
    "### Serving the Model\n",
    "\n",
    "```bash\n",
    "# Serve the model as a REST API\n",
    "mlflow models serve -m runs:/<run_id>/random_forest_model --port 5000\n",
    "```\n",
    "\n",
    "### Making Predictions on the Served Model\n",
    "\n",
    "```python\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Prepare data for prediction\n",
    "data = X_test.iloc[0:5].values.tolist()\n",
    "\n",
    "# Make POST request to the server\n",
    "response = requests.post(\n",
    "    \"http://localhost:5000/invocations\",\n",
    "    data=json.dumps({\"instances\": data}),\n",
    "    headers={\"Content-Type\": \"application/json\"}\n",
    ")\n",
    "\n",
    "# Get predictions\n",
    "predictions = response.json()\n",
    "print(predictions)\n",
    "```\n",
    "\n",
    "## MLFlow Model Registry for Production Models\n",
    "\n",
    "```python\n",
    "# Register a model from a run\n",
    "run_id = \"abcdef123456\"\n",
    "model_uri = f\"runs:/{run_id}/random_forest_model\"\n",
    "registered_model = mlflow.register_model(model_uri, \"CustomerChurnPredictor\")\n",
    "\n",
    "# Transition a model to production\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "client.transition_model_version_stage(\n",
    "    name=\"CustomerChurnPredictor\",\n",
    "    version=1,\n",
    "    stage=\"Production\"\n",
    ")\n",
    "\n",
    "# Get a production model for inference\n",
    "production_model = mlflow.pyfunc.load_model(\n",
    "    model_uri=\"models:/CustomerChurnPredictor/Production\"\n",
    ")\n",
    "predictions = production_model.predict(X_test)\n",
    "```\n",
    "\n",
    "## Integrating MLFlow with Deep Learning Frameworks\n",
    "\n",
    "### PyTorch Example\n",
    "\n",
    "```python\n",
    "import mlflow.pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a simple model\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.layer2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "# Training with MLFlow tracking\n",
    "with mlflow.start_run():\n",
    "    # Log parameters\n",
    "    input_dim = X_train.shape[1]\n",
    "    hidden_dim = 64\n",
    "    output_dim = 1\n",
    "    learning_rate = 0.001\n",
    "    epochs = 100\n",
    "    \n",
    "    mlflow.log_params({\n",
    "        \"input_dim\": input_dim,\n",
    "        \"hidden_dim\": hidden_dim,\n",
    "        \"output_dim\": output_dim,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"epochs\": epochs\n",
    "    })\n",
    "    \n",
    "    # Create and train model\n",
    "    model = SimpleNN(input_dim, hidden_dim, output_dim)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    X_tensor = torch.FloatTensor(X_train.values)\n",
    "    y_tensor = torch.FloatTensor(y_train.values).reshape(-1, 1)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass\n",
    "        outputs = model(X_tensor)\n",
    "        loss = criterion(outputs, y_tensor)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Log metrics\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            mlflow.log_metric(\"loss\", loss.item(), step=epoch)\n",
    "    \n",
    "    # Evaluate model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_test_tensor = torch.FloatTensor(X_test.values)\n",
    "        y_pred = model(X_test_tensor).numpy()\n",
    "        mse = ((y_pred - y_test.values.reshape(-1, 1))**2).mean()\n",
    "        mlflow.log_metric(\"mse\", mse)\n",
    "    \n",
    "    # Log model\n",
    "    mlflow.pytorch.log_model(model, \"pytorch_model\")\n",
    "```\n",
    "\n",
    "### TensorFlow/Keras Example\n",
    "\n",
    "```python\n",
    "import mlflow.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Enable autologging\n",
    "mlflow.keras.autolog()\n",
    "\n",
    "with mlflow.start_run():\n",
    "    # Build model\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Custom metrics not included in autologging\n",
    "    test_loss, test_mae = model.evaluate(X_test, y_test, verbose=0)\n",
    "    mlflow.log_metric(\"test_mae\", test_mae)\n",
    "```\n",
    "\n",
    "## Setting Up Remote Tracking Server\n",
    "\n",
    "### Simple Setup\n",
    "\n",
    "```bash\n",
    "# Start a tracking server with SQLite backend\n",
    "mlflow server --backend-store-uri sqlite:///mlflow.db --default-artifact-root ./mlruns\n",
    "\n",
    "# Start with PostgreSQL and S3\n",
    "mlflow server \\\n",
    "    --backend-store-uri postgresql://username:password@localhost/mlflow \\\n",
    "    --default-artifact-root s3://my-bucket/mlflow-artifacts \\\n",
    "    --host 0.0.0.0 \\\n",
    "    --port 5000\n",
    "```\n",
    "\n",
    "### Using Remote Server in Your Code\n",
    "\n",
    "```python\n",
    "# Set the tracking URI\n",
    "mlflow.set_tracking_uri(\"http://tracking-server:5000\")\n",
    "\n",
    "# Now all MLflow operations will use this remote server\n",
    "with mlflow.start_run():\n",
    "    # Your experiment code here\n",
    "    pass\n",
    "```\n",
    "\n",
    "## MLFlow Best Practices\n",
    "\n",
    "1. **Create Meaningful Experiment Names**:\n",
    "   - Organize runs logically by project, algorithm, or dataset\n",
    "   - Example: \"customer_churn_random_forest\", \"customer_churn_xgboost\"\n",
    "\n",
    "2. **Log Everything Relevant**:\n",
    "   - Hyperparameters: All model configuration\n",
    "   - Metrics: Training and validation metrics\n",
    "   - Artifacts: Models, visualizations, sample predictions\n",
    "   - Tags: Dataset versions, Git commits, environment info\n",
    "\n",
    "3. **Structure Your Runs**:\n",
    "   - Use nested runs for multi-step workflows\n",
    "   - Log data preprocessing steps separately\n",
    "\n",
    "4. **Standardize Metric Names**:\n",
    "   - Use consistent naming conventions for metrics across experiments\n",
    "   - Example: \"train_accuracy\", \"val_accuracy\", \"test_accuracy\"\n",
    "\n",
    "5. **Version Your Data**:\n",
    "   - Log dataset hash or version\n",
    "   - Track data preprocessing parameters\n",
    "   - Store sample data as artifacts\n",
    "\n",
    "6. **Use Tags for Filtering**:\n",
    "   - Add tags for model type, dataset version, author\n",
    "   - Makes filtering in the UI easier\n",
    "\n",
    "7. **Set Up Cross-Project Standards**:\n",
    "   - Define team standards for parameter names\n",
    "   - Create templates for common experiment types\n",
    "\n",
    "8. **Integrate with CI/CD**:\n",
    "   - Run MLflow experiments within CI pipelines\n",
    "   - Register models automatically if they meet performance thresholds\n",
    "\n",
    "## Complete MLFlow Project Example\n",
    "\n",
    "### Directory Structure\n",
    "\n",
    "```\n",
    "ml-project/\n",
    "├── data/\n",
    "│   ├── raw/\n",
    "│   └── processed/\n",
    "├── models/\n",
    "├── notebooks/\n",
    "├── src/\n",
    "│   ├── __init__.py\n",
    "│   ├── data/\n",
    "│   │   ├── __init__.py\n",
    "│   │   └── preprocessing.py\n",
    "│   ├── features/\n",
    "│   │   ├── __init__.py\n",
    "│   │   └── feature_engineering.py\n",
    "│   ├── models/\n",
    "│   │   ├── __init__.py\n",
    "│   │   └── train.py\n",
    "│   └── visualization/\n",
    "│       ├── __init__.py\n",
    "│       └── visualize.py\n",
    "├── .gitignore\n",
    "├── MLproject\n",
    "├── README.md\n",
    "├── conda.yaml\n",
    "└── main.py\n",
    "```\n",
    "\n",
    "### main.py\n",
    "\n",
    "```python\n",
    "import os\n",
    "import click\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "from src.data.preprocessing import preprocess_data\n",
    "from src.features.feature_engineering import create_features\n",
    "from src.models.train import train_model\n",
    "from src.visualization.visualize import create_evaluation_plots\n",
    "\n",
    "@click.command()\n",
    "@click.option(\"--data-path\", type=str, default=\"data/raw/dataset.csv\", help=\"Path to the input dataset\")\n",
    "@click.option(\"--model-type\", type=str, default=\"random_forest\", help=\"Type of model to train\")\n",
    "@click.option(\"--n-estimators\", type=int, default=100, help=\"Number of trees in the forest\")\n",
    "@click.option(\"--max-depth\", type=int, default=None, help=\"Maximum depth of the trees\")\n",
    "@click.option(\"--test-size\", type=float, default=0.2, help=\"Proportion of the dataset to include in the test split\")\n",
    "def main(data_path, model_type, n_estimators, max_depth, test_size):\n",
    "    \"\"\"Main pipeline for training and evaluating a model.\"\"\"\n",
    "    \n",
    "    # Set experiment name based on model type\n",
    "    mlflow.set_experiment(f\"{model_type}_experiment\")\n",
    "    \n",
    "    # Start MLflow run\n",
    "    with mlflow.start_run() as run:\n",
    "        # Log git commit hash if available\n",
    "        try:\n",
    "            import git\n",
    "            repo = git.Repo(search_parent_directories=True)\n",
    "            sha = repo.head.object.hexsha\n",
    "            mlflow.set_tag(\"git_commit\", sha)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Log parameters\n",
    "        mlflow.log_params({\n",
    "            \"data_path\": data_path,\n",
    "            \"model_type\": model_type,\n",
    "            \"n_estimators\": n_estimators,\n",
    "            \"max_depth\": max_depth,\n",
    "            \"test_size\": test_size\n",
    "        })\n",
    "        \n",
    "        # Load and preprocess data\n",
    "        with mlflow.start_run(run_name=\"preprocessing\", nested=True):\n",
    "            df = pd.read_csv(data_path)\n",
    "            mlflow.log_metric(\"raw_data_rows\", len(df))\n",
    "            \n",
    "            X_train, X_test, y_train, y_test = preprocess_data(\n",
    "                df, test_size=test_size\n",
    "            )\n",
    "            mlflow.log_metric(\"train_size\", len(X_train))\n",
    "            mlflow.log_metric(\"test_size\", len(X_test))\n",
    "        \n",
    "        # Feature engineering\n",
    "        with mlflow.start_run(run_name=\"feature_engineering\", nested=True):\n",
    "            X_train, X_test = create_features(X_train, X_test)\n",
    "            mlflow.log_metric(\"feature_count\", X_train.shape[1])\n",
    "            \n",
    "            # Log feature names\n",
    "            mlflow.log_param(\"features\", list(X_train.columns))\n",
    "            \n",
    "            # Save sample data\n",
    "            X_train.head(10).to_csv(\"sample_train_data.csv\", index=False)\n",
    "            mlflow.log_artifact(\"sample_train_data.csv\")\n",
    "        \n",
    "        # Train model\n",
    "        with mlflow.start_run(run_name=\"model_training\", nested=True):\n",
    "            model, metrics = train_model(\n",
    "                X_train, y_train, X_test, y_test,\n",
    "                model_type=model_type,\n",
    "                n_estimators=n_estimators,\n",
    "                max_depth=max_depth\n",
    "            )\n",
    "            \n",
    "            # Log metrics\n",
    "            for name, value in metrics.items():\n",
    "                mlflow.log_metric(name, value)\n",
    "        \n",
    "        # Create and log visualizations\n",
    "        with mlflow.start_run(run_name=\"visualization\", nested=True):\n",
    "            plot_paths = create_evaluation_plots(model, X_test, y_test)\n",
    "            \n",
    "            # Log all plots\n",
    "            for plot_path in plot_paths:\n",
    "                mlflow.log_artifact(plot_path)\n",
    "        \n",
    "        # Log trained model\n",
    "        if model_type == \"random_forest\":\n",
    "            mlflow.sklearn.log_model(model, \"model\")\n",
    "        elif model_type == \"xgboost\":\n",
    "            mlflow.xgboost.log_model(model, \"model\")\n",
    "        \n",
    "        # Print run info\n",
    "        print(f\"Run ID: {run.info.run_id}\")\n",
    "        print(f\"Model saved in run {run.info.run_id}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```\n",
    "\n",
    "## Comparing Experiment Runs\n",
    "\n",
    "The MLFlow UI makes it easy to compare different runs to identify the best model configuration:\n",
    "\n",
    "1. Select multiple runs by checking the boxes next to them\n",
    "2. Click \"Compare\" to see metrics and parameters side by side\n",
    "3. Sort by any metric to find the best performing model\n",
    "4. View parameter differences to understand performance variations\n",
    "5. Compare artifacts like feature importance plots\n",
    "\n",
    "## MLFlow vs Other Experiment Tracking Tools\n",
    "\n",
    "| Tool | Strengths | Limitations |\n",
    "|------|-----------|-------------|\n",
    "| **MLFlow** | Open-source, language-agnostic, comprehensive | Requires more setup for production |\n",
    "| **TensorBoard** | Great for deep learning visualization | Primarily for TensorFlow, limited scope |\n",
    "| **Weights & Biases** | Great UI, team collaboration | Commercial product, not fully open-source |\n",
    "| **Neptune.ai** | Rich UI, metadata tracking | Commercial product |\n",
    "| **DVC** | Strong data versioning | Primarily for data, not full experiment tracking |\n",
    "\n",
    "## ⚠️ Important Considerations for MLFlow in Production\n",
    "\n",
    "1. **Security**:\n",
    "   - Use authentication with a remote tracking server\n",
    "   - Control access to model registry\n",
    "\n",
    "2. **Scalability**:\n",
    "   - Use a database backend (PostgreSQL, MySQL) for many experiments\n",
    "   - Use cloud storage (S3, Azure Blob) for artifacts\n",
    "   - Consider MLFlow with Databricks for managed service\n",
    "\n",
    "3. **Integration with Model Serving**:\n",
    "   - Connect MLFlow Registry to deployment pipelines\n",
    "   - Automate A/B testing of new models\n",
    "\n",
    "4. **Compliance and Governance**:\n",
    "   - Track model lineage for audit purposes\n",
    "   - Document model decisions and approvals\n",
    "\n",
    "5. **Performance Monitoring**:\n",
    "   - Set up alerts for model drift\n",
    "   - Log production inference metrics back to MLFlow\n",
    "\n",
    "MLFlow provides a comprehensive framework for experiment tracking that scales from individual data scientists to large teams working on complex ML systems. By incorporating MLFlow into your workflow, you can ensure reproducibility, collaboration, and efficient model development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. ML System Design for Production\n",
    "\n",
    "## Key Principles of ML System Design\n",
    "\n",
    "| Principle | Description | Impact |\n",
    "|-----------|-------------|--------|\n",
    "| **Scalability** | System's ability to handle growing data and users | Determines if model can serve increasing traffic |\n",
    "| **Reliability** | System's ability to function correctly consistently | Affects user trust and business operations |\n",
    "| **Maintainability** | Ease of updating and debugging the system | Influences long-term operational costs |\n",
    "| **Adaptability** | System's ability to evolve with changing requirements | Determines system's longevity |\n",
    "| **Monitorability** | Ability to observe system behavior and performance | Critical for detecting issues early |\n",
    "\n",
    "## ML System Architecture Patterns\n",
    "\n",
    "### Layered Architecture\n",
    "\n",
    "```\n",
    "┌─────────────────────────┐\n",
    "│     User Interface      │\n",
    "├─────────────────────────┤\n",
    "│    Application Layer    │\n",
    "├─────────────────────────┤\n",
    "│     Feature Layer       │\n",
    "├─────────────────────────┤\n",
    "│      Model Layer        │\n",
    "├─────────────────────────┤\n",
    "│      Data Layer         │\n",
    "└─────────────────────────┘\n",
    "```\n",
    "\n",
    "### Microservice Architecture for ML\n",
    "\n",
    "```\n",
    "┌────────────┐   ┌────────────┐   ┌────────────┐\n",
    "│ Data       │   │ Feature    │   │ Model      │\n",
    "│ Collection │   │ Engineering│   │ Training   │\n",
    "└────────────┘   └────────────┘   └────────────┘\n",
    "       │               │                │\n",
    "       └───────────────┼────────────────┘\n",
    "                       ▼\n",
    "               ┌────────────┐\n",
    "               │ Prediction │\n",
    "               │ Service    │\n",
    "               └────────────┘\n",
    "                       │\n",
    "                       ▼\n",
    "               ┌────────────┐\n",
    "               │ Monitoring │\n",
    "               │ Service    │\n",
    "               └────────────┘\n",
    "```\n",
    "\n",
    "## ⚠️ Requirements Analysis\n",
    "\n",
    "### Functional Requirements (What the system should do)\n",
    "\n",
    "1. **Data Collection & Processing**\n",
    "   - Collect data from various sources\n",
    "   - Clean and preprocess data\n",
    "   - Store data efficiently\n",
    "\n",
    "2. **Model Training & Evaluation**\n",
    "   - Train models with various algorithms\n",
    "   - Evaluate model performance\n",
    "   - Compare models against baselines\n",
    "\n",
    "3. **Model Serving**\n",
    "   - Serve predictions with low latency\n",
    "   - Handle batch and real-time predictions\n",
    "   - Version control for models\n",
    "\n",
    "4. **Monitoring & Feedback**\n",
    "   - Monitor model performance\n",
    "   - Detect drift and anomalies\n",
    "   - Collect user feedback\n",
    "\n",
    "### Non-Functional Requirements (Quality attributes)\n",
    "\n",
    "1. **Performance**\n",
    "   - Latency: How fast should predictions be?\n",
    "   - Throughput: How many predictions per second?\n",
    "   - Resource utilization: CPU, memory, GPU constraints\n",
    "\n",
    "2. **Scalability**\n",
    "   - Data volume growth\n",
    "   - User request growth\n",
    "   - Feature dimension growth\n",
    "\n",
    "3. **Reliability**\n",
    "   - Availability (e.g., 99.9%)\n",
    "   - Error rates\n",
    "   - Recovery capabilities\n",
    "\n",
    "4. **Security**\n",
    "   - Data privacy\n",
    "   - Authentication/authorization\n",
    "   - Protection against adversarial attacks\n",
    "\n",
    "## Data Engineering Components\n",
    "\n",
    "### Data Collection\n",
    "\n",
    "```python\n",
    "# Example data collection pipeline with Airflow\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def collect_from_database():\n",
    "    # Code to extract data from database\n",
    "    pass\n",
    "\n",
    "def collect_from_api():\n",
    "    # Code to collect data from API\n",
    "    pass\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'data_engineer',\n",
    "    'depends_on_past': False,\n",
    "    'email_on_failure': True,\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "dag = DAG(\n",
    "    'data_collection_pipeline',\n",
    "    default_args=default_args,\n",
    "    description='Collect data from various sources',\n",
    "    schedule_interval=timedelta(days=1),\n",
    "    start_date=datetime(2023, 1, 1),\n",
    ")\n",
    "\n",
    "db_task = PythonOperator(\n",
    "    task_id='collect_from_database',\n",
    "    python_callable=collect_from_database,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "api_task = PythonOperator(\n",
    "    task_id='collect_from_api',\n",
    "    python_callable=collect_from_api,\n",
    "    dag=dag,\n",
    ")\n",
    "```\n",
    "\n",
    "### Data Storage Options\n",
    "\n",
    "| Storage Type | Use Case | Examples | Considerations |\n",
    "|--------------|----------|----------|----------------|\n",
    "| **Relational Database** | Structured data, transactional data | PostgreSQL, MySQL | Schema rigidity, ACID compliance |\n",
    "| **NoSQL Database** | Semi-structured data, flexible schema | MongoDB, Cassandra | Horizontal scaling, eventual consistency |\n",
    "| **Data Lake** | Raw unprocessed data | AWS S3, Azure Data Lake | Cost-effective, schema-on-read |\n",
    "| **Data Warehouse** | Processed analytical data | Snowflake, BigQuery | Query performance, columnar storage |\n",
    "| **Time Series Database** | Temporal data, metrics | InfluxDB, Timescale | Time-based queries, data retention |\n",
    "\n",
    "### Feature Engineering & Storage\n",
    "\n",
    "```python\n",
    "# Example feature store with Feast\n",
    "from feast import Entity, Feature, FeatureView, FileSource, ValueType\n",
    "from datetime import timedelta\n",
    "\n",
    "# Define an entity for users\n",
    "user = Entity(\n",
    "    name=\"user_id\",\n",
    "    value_type=ValueType.INT64,\n",
    "    description=\"user identifier\",\n",
    ")\n",
    "\n",
    "# Define a data source for user features\n",
    "user_source = FileSource(\n",
    "    path=\"s3://bucket/user_features.parquet\",\n",
    "    event_timestamp_column=\"event_timestamp\",\n",
    ")\n",
    "\n",
    "# Define a feature view\n",
    "user_features_view = FeatureView(\n",
    "    name=\"user_features\",\n",
    "    entities=[\"user_id\"],\n",
    "    ttl=timedelta(days=1),\n",
    "    features=[\n",
    "        Feature(name=\"age\", dtype=ValueType.INT64),\n",
    "        Feature(name=\"gender\", dtype=ValueType.STRING),\n",
    "        Feature(name=\"subscription_type\", dtype=ValueType.STRING),\n",
    "        Feature(name=\"activity_level\", dtype=ValueType.FLOAT),\n",
    "    ],\n",
    "    batch_source=user_source,\n",
    ")\n",
    "```\n",
    "\n",
    "## Model Development & Training\n",
    "\n",
    "### Training Pipeline Architecture\n",
    "\n",
    "```\n",
    "┌─────────────────┐     ┌─────────────────┐     ┌─────────────────┐\n",
    "│  Data Ingestion │  →  │ Preprocessing   │  →  │ Feature         │\n",
    "│  & Validation   │     │ & Cleaning      │     │ Engineering     │\n",
    "└─────────────────┘     └─────────────────┘     └─────────────────┘\n",
    "         ↓                                               ↓\n",
    "┌─────────────────┐     ┌─────────────────┐     ┌─────────────────┐\n",
    "│  Model Registry │  ←  │ Model           │  ←  │ Training &      │\n",
    "│  & Versioning   │     │ Evaluation      │     │ Hyperparameter  │\n",
    "└─────────────────┘     └─────────────────┘     │ Optimization    │\n",
    "                                                └─────────────────┘\n",
    "```\n",
    "\n",
    "### Training Infrastructure Options\n",
    "\n",
    "| Option | Advantages | Considerations |\n",
    "|--------|------------|----------------|\n",
    "| **On-Premises GPU Servers** | Full control, no data transfer | High upfront cost, maintenance overhead |\n",
    "| **Cloud VM with GPUs** (e.g., EC2) | Flexible, scalable | Cost can be high for long-running jobs |\n",
    "| **Managed ML Services** (e.g., SageMaker) | Simplified deployment, managed infrastructure | Vendor lock-in, less flexibility |\n",
    "| **Container Orchestration** (e.g., Kubernetes) | Scale based on demand, resource efficient | Complex setup and management |\n",
    "| **Serverless** (e.g., AWS Lambda with EFS) | Pay-per-use, no infrastructure management | Limited for complex models, time constraints |\n",
    "\n",
    "### Distributed Training\n",
    "\n",
    "```python\n",
    "# Example distributed training with TensorFlow\n",
    "import tensorflow as tf\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print(f\"Number of devices: {strategy.num_replicas_in_sync}\")\n",
    "\n",
    "# Model definition within strategy scope\n",
    "with strategy.scope():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(256, activation='relu', input_shape=(features,)),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    epochs=100,\n",
    "    validation_data=val_dataset\n",
    ")\n",
    "```\n",
    "\n",
    "## Model Serving Architecture\n",
    "\n",
    "### Synchronous (Real-time) Serving\n",
    "\n",
    "```\n",
    "Client → API Gateway → Load Balancer → Prediction Service → Model Server\n",
    "```\n",
    "\n",
    "```python\n",
    "# Example FastAPI service for model serving\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Load model (in production, use a better loading strategy)\n",
    "with open(\"model.pkl\", \"rb\") as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "class PredictionRequest(BaseModel):\n",
    "    features: list[float]\n",
    "\n",
    "class PredictionResponse(BaseModel):\n",
    "    prediction: float\n",
    "    confidence: float\n",
    "    latency_ms: float\n",
    "\n",
    "@app.post(\"/predict\", response_model=PredictionResponse)\n",
    "async def predict(request: PredictionRequest):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Convert features to numpy array\n",
    "        features = np.array(request.features).reshape(1, -1)\n",
    "        \n",
    "        # Make prediction\n",
    "        prediction = model.predict(features)[0]\n",
    "        \n",
    "        # Get confidence (for classification models)\n",
    "        confidence = 0.95  # Simplified - get actual confidence based on model type\n",
    "        \n",
    "        # Calculate latency\n",
    "        latency_ms = (time.time() - start_time) * 1000\n",
    "        \n",
    "        return PredictionResponse(\n",
    "            prediction=float(prediction),\n",
    "            confidence=confidence,\n",
    "            latency_ms=latency_ms\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "```\n",
    "\n",
    "### Asynchronous (Batch) Serving\n",
    "\n",
    "```\n",
    "┌─────────────┐     ┌─────────────┐     ┌─────────────┐\n",
    "│ Client      │  →  │ Message     │  →  │ Batch       │\n",
    "│ Application │     │ Queue       │     │ Processing  │\n",
    "└─────────────┘     └─────────────┘     └─────────────┘\n",
    "                                               ↓\n",
    "                                        ┌─────────────┐\n",
    "                                        │ Results     │\n",
    "                                        │ Storage     │\n",
    "                                        └─────────────┘\n",
    "```\n",
    "\n",
    "```python\n",
    "# Example batch processing with Celery\n",
    "from celery import Celery\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "app = Celery('batch_predictions', broker='redis://localhost:6379/0')\n",
    "\n",
    "@app.task\n",
    "def predict_batch(batch_id, features_file_path):\n",
    "    # Load the model\n",
    "    with open(\"model.pkl\", \"rb\") as f:\n",
    "        model = pickle.load(f)\n",
    "    \n",
    "    # Load the batch data\n",
    "    features_df = pd.read_csv(features_file_path)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = model.predict(features_df)\n",
    "    \n",
    "    # Save predictions\n",
    "    results_df = pd.DataFrame({\n",
    "        'id': features_df.index,\n",
    "        'prediction': predictions\n",
    "    })\n",
    "    \n",
    "    # Store results\n",
    "    results_path = f\"results/batch_{batch_id}_results.csv\"\n",
    "    results_df.to_csv(results_path, index=False)\n",
    "    \n",
    "    return {\n",
    "        'batch_id': batch_id,\n",
    "        'results_path': results_path,\n",
    "        'num_predictions': len(predictions)\n",
    "    }\n",
    "```\n",
    "\n",
    "### Model Serving Options Comparison\n",
    "\n",
    "| Approach | Advantages | Disadvantages | Best For |\n",
    "|----------|------------|--------------|----------|\n",
    "| **REST API** | Simple, standard interface | Overhead for each request | General-purpose serving |\n",
    "| **gRPC** | High performance, efficient | Less universal than REST | High-throughput systems |\n",
    "| **TensorFlow Serving** | Optimized for TF models | TensorFlow-specific | TensorFlow models |\n",
    "| **TorchServe** | Optimized for PyTorch | PyTorch-specific | PyTorch models |\n",
    "| **ONNX Runtime** | Framework-agnostic | Conversion complexity | Cross-framework deployment |\n",
    "| **KFServing/Seldon** | Kubernetes-native, autoscaling | Complex setup | Cloud-native environments |\n",
    "| **SageMaker Endpoints** | Managed, scalable | AWS-specific, cost | AWS-based ML systems |\n",
    "\n",
    "## Monitoring & Feedback System\n",
    "\n",
    "### Key Metrics to Monitor\n",
    "\n",
    "1. **Model Performance Metrics**\n",
    "   - Accuracy, precision, recall, F1 score\n",
    "   - Business-specific KPIs\n",
    "\n",
    "2. **Operational Metrics**\n",
    "   - Latency (p50, p95, p99)\n",
    "   - Throughput\n",
    "   - Error rates\n",
    "   - Resource utilization\n",
    "\n",
    "3. **Data Quality Metrics**\n",
    "   - Missing values\n",
    "   - Outliers\n",
    "   - Feature distribution changes\n",
    "\n",
    "4. **Drift Metrics**\n",
    "   - Feature drift\n",
    "   - Concept drift\n",
    "   - Label drift\n",
    "\n",
    "### Monitoring Architecture\n",
    "\n",
    "```\n",
    "┌─────────────┐     ┌─────────────┐     ┌─────────────┐\n",
    "│ Prediction  │  →  │ Monitoring  │  →  │ Metrics     │\n",
    "│ Service     │     │ Service     │     │ Database    │\n",
    "└─────────────┘     └─────────────┘     └─────────────┘\n",
    "                          ↓                    ↓\n",
    "                    ┌─────────────┐     ┌─────────────┐\n",
    "                    │ Alerting    │  ←  │ Dashboard   │\n",
    "                    │ System      │     │ & Reporting │\n",
    "                    └─────────────┘     └─────────────┘\n",
    "```\n",
    "\n",
    "### Drift Detection Implementation\n",
    "\n",
    "```python\n",
    "# Example drift detection with scikit-multiflow\n",
    "from skmultiflow.drift_detection import ADWIN\n",
    "import numpy as np\n",
    "\n",
    "class DriftDetector:\n",
    "    def __init__(self):\n",
    "        self.drift_detector = ADWIN()\n",
    "        self.drift_count = 0\n",
    "        self.samples_count = 0\n",
    "        self.last_drift_at = 0\n",
    "    \n",
    "    def update(self, prediction, actual):\n",
    "        # Calculate error (0 if correct, 1 if wrong)\n",
    "        error = 0 if prediction == actual else 1\n",
    "        \n",
    "        # Update drift detector\n",
    "        self.drift_detector.add_element(error)\n",
    "        self.samples_count += 1\n",
    "        \n",
    "        # Check if drift is detected\n",
    "        if self.drift_detector.detected_change():\n",
    "            self.drift_count += 1\n",
    "            self.last_drift_at = self.samples_count\n",
    "            \n",
    "            return {\n",
    "                'drift_detected': True,\n",
    "                'total_drift_count': self.drift_count,\n",
    "                'samples_since_last_drift': 0\n",
    "            }\n",
    "        \n",
    "        return {\n",
    "            'drift_detected': False,\n",
    "            'total_drift_count': self.drift_count,\n",
    "            'samples_since_last_drift': self.samples_count - self.last_drift_at\n",
    "        }\n",
    "```\n",
    "\n",
    "## Scaling ML Systems\n",
    "\n",
    "### Horizontal vs Vertical Scaling for ML\n",
    "\n",
    "| Aspect | Horizontal Scaling | Vertical Scaling |\n",
    "|--------|-------------------|------------------|\n",
    "| **Definition** | Adding more machines | Adding resources to existing machines |\n",
    "| **Data Processing** | Distribute data across nodes | Process larger chunks on single machine |\n",
    "| **Training** | Distributed training frameworks | More powerful GPUs/CPUs |\n",
    "| **Serving** | Load balancing across servers | More powerful server instances |\n",
    "| **Cost Efficiency** | Better for linear scaling | Better for model complexity |\n",
    "| **Implementation** | More complex, needs orchestration | Simpler but limited by hardware |\n",
    "\n",
    "### Database Scaling Considerations\n",
    "\n",
    "1. **Read Scalability**\n",
    "   - Replica databases for read operations\n",
    "   - Caching frequently accessed data\n",
    "\n",
    "2. **Write Scalability**\n",
    "   - Sharding/partitioning data\n",
    "   - Time-based partitioning for time-series data\n",
    "\n",
    "3. **Feature Store Scaling**\n",
    "   - Online/offline store separation\n",
    "   - Columnar storage for analytical queries\n",
    "\n",
    "### Traffic Handling\n",
    "\n",
    "```\n",
    "┌──────────┐     ┌──────────┐\n",
    "│ Client   │  →  │ API      │\n",
    "└──────────┘     │ Gateway  │\n",
    "                 └──────────┘\n",
    "                      ↓\n",
    "     ┌─────────────────────────────┐\n",
    "     │        Load Balancer        │\n",
    "     └─────────────────────────────┘\n",
    "        ↓           ↓           ↓\n",
    "┌──────────┐  ┌──────────┐  ┌──────────┐\n",
    "│ Model    │  │ Model    │  │ Model    │\n",
    "│ Server 1 │  │ Server 2 │  │ Server 3 │\n",
    "└──────────┘  └──────────┘  └──────────┘\n",
    "```\n",
    "\n",
    "## Comprehensive ML System Design Example: Recommendation System\n",
    "\n",
    "### System Architecture\n",
    "\n",
    "```\n",
    "                                 ┌────────────────┐\n",
    "                                 │   API Gateway  │\n",
    "                                 └────────────────┘\n",
    "                                        ↓\n",
    "┌───────────────┐            ┌────────────────┐            ┌───────────────┐\n",
    "│ Batch         │            │ Recommendation │            │ User Activity │\n",
    "│ Processing    │ ←────────→ │ Service        │ ←────────→ │ Tracking      │\n",
    "└───────────────┘            └────────────────┘            └───────────────┘\n",
    "       ↓                              ↓                             ↓\n",
    "┌───────────────┐            ┌────────────────┐            ┌───────────────┐\n",
    "│ Feature Store │            │ Model Registry │            │ Event Stream  │\n",
    "└───────────────┘            └────────────────┘            └───────────────┘\n",
    "       ↓                              ↓                             ↓\n",
    "┌───────────────┐            ┌────────────────┐            ┌───────────────┐\n",
    "│ Data Lake     │            │ Model          │            │ Real-time     │\n",
    "│               │            │ Monitoring     │            │ Processing    │\n",
    "└───────────────┘            └────────────────┘            └───────────────┘\n",
    "```\n",
    "\n",
    "### Data Flow\n",
    "\n",
    "1. **User Interaction Data Collection**\n",
    "   - Capture clicks, views, likes, purchases\n",
    "   - Store events in real-time event stream (Kafka)\n",
    "   - Process events both in real-time and batch\n",
    "\n",
    "2. **Feature Engineering**\n",
    "   - Extract user features (demographics, historical behavior)\n",
    "   - Extract item features (category, attributes, popularity)\n",
    "   - Engineer interaction features (user-item affinity)\n",
    "   - Store in feature store with time-based versioning\n",
    "\n",
    "3. **Model Training**\n",
    "   - Train collaborative filtering model\n",
    "   - Train content-based models\n",
    "   - Train hybrid models combining multiple signals\n",
    "   - Evaluate and register best models in registry\n",
    "\n",
    "4. **Recommendation Serving**\n",
    "   - Pre-compute recommendations for popular items/users\n",
    "   - Serve real-time recommendations for active users\n",
    "   - Combine model predictions with business rules\n",
    "   - A/B test different recommendation strategies\n",
    "\n",
    "5. **Feedback Loop**\n",
    "   - Track recommendation success (clicks, conversions)\n",
    "   - Update models based on new interaction data\n",
    "   - Detect and handle cold-start problems\n",
    "\n",
    "### Scaling Considerations\n",
    "\n",
    "1. **Data Volume Scaling**\n",
    "   - Partition event data by time and user segments\n",
    "   - Use distributed processing for batch computation\n",
    "   - Implement TTL policies for historical data\n",
    "\n",
    "2. **Model Complexity Scaling**\n",
    "   - Use embedding techniques to reduce dimensionality\n",
    "   - Implement approximate nearest neighbor search\n",
    "   - Consider model distillation for inference optimization\n",
    "\n",
    "3. **Request Volume Scaling**\n",
    "   - Cache recommendations for frequent users\n",
    "   - Implement tiered serving (real-time for high-value users, batch for others)\n",
    "   - Geographic distribution for global user base\n",
    "\n",
    "## ⚠️ Best Practices for ML System Design\n",
    "\n",
    "1. **Start Simple, Iterate Fast**\n",
    "   - Begin with simple models and baseline approaches\n",
    "   - Implement MVP quickly and gather feedback\n",
    "   - Add complexity only when justified by metrics\n",
    "\n",
    "2. **Design for Failure**\n",
    "   - Implement graceful degradation\n",
    "   - Have fallback strategies when models or services fail\n",
    "   - Implement circuit breakers and timeouts\n",
    "\n",
    "3. **Separate Concerns**\n",
    "   - Decouple data processing from model training\n",
    "   - Separate model serving from application logic\n",
    "   - Create clear interfaces between components\n",
    "\n",
    "4. **Make Reproducibility a Priority**\n",
    "   - Version data, code, and models\n",
    "   - Document experiments and decisions\n",
    "   - Use containerization for consistent environments\n",
    "\n",
    "5. **Plan for Monitoring from Day One**\n",
    "   - Define key metrics before deployment\n",
    "   - Implement logging for debugging and auditing\n",
    "   - Set up alerts for critical failures\n",
    "\n",
    "6. **Consider Security and Privacy**\n",
    "   - Apply principles of least privilege\n",
    "   - Encrypt sensitive data\n",
    "   - Implement access controls and audit logs\n",
    "\n",
    "7. **Design for Evolution**\n",
    "   - Expect models to change over time\n",
    "   - Build for easy model updates and versioning\n",
    "   - Plan for concept drift and data distribution changes\n",
    "\n",
    "8. **Optimize for the Right Constraints**\n",
    "   - Balance accuracy vs. latency requirements\n",
    "   - Consider resource costs in design decisions\n",
    "   - Right-size infrastructure for actual needs\n",
    "\n",
    "By following these principles and understanding the components of ML systems, you can design robust, scalable, and maintainable ML applications that deliver business value while minimizing operational overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Building ML Pipelines Using Amazon SageMaker\n",
    "\n",
    "## What is Amazon SageMaker?\n",
    "\n",
    "Amazon SageMaker is a fully managed service that covers the entire machine learning workflow, allowing data scientists and developers to build, train, and deploy ML models quickly in a cloud environment.\n",
    "\n",
    "## SageMaker Components for ML Pipelines\n",
    "\n",
    "| Component | Purpose | Benefits |\n",
    "|-----------|---------|----------|\n",
    "| **SageMaker Studio** | Integrated ML development environment | Single interface for all ML tasks |\n",
    "| **SageMaker Notebooks** | Interactive development environment | Pre-configured with ML libraries |\n",
    "| **SageMaker Processing** | Data processing and feature engineering | Managed infrastructure for ETL jobs |\n",
    "| **SageMaker Training** | Model training at scale | Distributed training capabilities |\n",
    "| **SageMaker Pipelines** | End-to-end ML workflow orchestration | Automated and reproducible workflows |\n",
    "| **SageMaker Model Registry** | Model versioning and lineage tracking | Governance and compliance |\n",
    "| **SageMaker Endpoints** | Model deployment and serving | Scalable real-time inference |\n",
    "| **SageMaker Batch Transform** | Batch predictions | Efficient processing of large datasets |\n",
    "\n",
    "## SageMaker ML Pipeline Architecture\n",
    "\n",
    "```\n",
    "┌───────────────┐    ┌───────────────┐    ┌───────────────┐    ┌───────────────┐\n",
    "│ Data          │ → │ Feature       │ → │ Model         │ → │ Model         │\n",
    "│ Preprocessing │    │ Engineering   │    │ Training      │    │ Evaluation    │\n",
    "└───────────────┘    └───────────────┘    └───────────────┘    └───────────────┘\n",
    "         ↓                                                             ↓\n",
    "┌───────────────┐                                           ┌───────────────┐\n",
    "│ Data          │                                           │ Model         │\n",
    "│ Validation    │                                           │ Registration  │\n",
    "└───────────────┘                                           └───────────────┘\n",
    "                                                                    ↓\n",
    "                                                           ┌───────────────┐\n",
    "                                                           │ Model         │\n",
    "                                                           │ Deployment    │\n",
    "                                                           └───────────────┘\n",
    "```\n",
    "\n",
    "## Setting Up SageMaker Environment\n",
    "\n",
    "### Creating a SageMaker Notebook Instance\n",
    "\n",
    "```python\n",
    "import boto3\n",
    "\n",
    "sagemaker_client = boto3.client('sagemaker')\n",
    "\n",
    "notebook_instance_name = 'ml-pipeline-notebook'\n",
    "\n",
    "response = sagemaker_client.create_notebook_instance(\n",
    "    NotebookInstanceName=notebook_instance_name,\n",
    "    InstanceType='ml.t3.medium',\n",
    "    RoleArn='arn:aws:iam::123456789012:role/SageMakerRole',\n",
    "    VolumeSizeInGB=50,\n",
    "    Tags=[\n",
    "        {\n",
    "            'Key': 'Project',\n",
    "            'Value': 'MLOps-Pipeline'\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"Notebook instance {notebook_instance_name} created.\")\n",
    "```\n",
    "\n",
    "### Setting Up SageMaker Studio\n",
    "\n",
    "```python\n",
    "import boto3\n",
    "\n",
    "sm_client = boto3.client('sagemaker')\n",
    "\n",
    "# Create a SageMaker domain\n",
    "response = sm_client.create_domain(\n",
    "    DomainName='ml-pipeline-domain',\n",
    "    AuthMode='IAM',\n",
    "    DefaultUserSettings={\n",
    "        'ExecutionRole': 'arn:aws:iam::123456789012:role/SageMakerExecutionRole'\n",
    "    },\n",
    "    SubnetIds=['subnet-0123456789abcdef0', 'subnet-0123456789abcdef1'],\n",
    "    VpcId='vpc-0123456789abcdef0'\n",
    ")\n",
    "\n",
    "domain_id = response['DomainId']\n",
    "print(f\"SageMaker domain created with ID: {domain_id}\")\n",
    "\n",
    "# Create a user profile\n",
    "response = sm_client.create_user_profile(\n",
    "    DomainId=domain_id,\n",
    "    UserProfileName='ml-pipeline-user',\n",
    "    UserSettings={\n",
    "        'ExecutionRole': 'arn:aws:iam::123456789012:role/SageMakerExecutionRole'\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"User profile 'ml-pipeline-user' created in domain {domain_id}\")\n",
    "```\n",
    "\n",
    "## Building an End-to-End ML Pipeline\n",
    "\n",
    "### 1. Data Preparation and Processing\n",
    "\n",
    "```python\n",
    "import sagemaker\n",
    "from sagemaker.processing import ScriptProcessor\n",
    "\n",
    "# Initialize the SageMaker session\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'sagemaker/ml-pipeline'\n",
    "\n",
    "# Create a processor for data preprocessing\n",
    "processor = ScriptProcessor(\n",
    "    command=['python3'],\n",
    "    image_uri='123456789012.dkr.ecr.us-east-1.amazonaws.com/my-preprocessing-image:latest',\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "# Run the processing job\n",
    "processor.run(\n",
    "    code='preprocessing.py',\n",
    "    inputs=[\n",
    "        sagemaker.processing.ProcessingInput(\n",
    "            source=f's3://{bucket}/{prefix}/raw-data/',\n",
    "            destination='/opt/ml/processing/input',\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        sagemaker.processing.ProcessingOutput(\n",
    "            output_name='train',\n",
    "            source='/opt/ml/processing/output/train',\n",
    "            destination=f's3://{bucket}/{prefix}/processed/train'\n",
    "        ),\n",
    "        sagemaker.processing.ProcessingOutput(\n",
    "            output_name='validation',\n",
    "            source='/opt/ml/processing/output/validation',\n",
    "            destination=f's3://{bucket}/{prefix}/processed/validation'\n",
    "        ),\n",
    "        sagemaker.processing.ProcessingOutput(\n",
    "            output_name='test',\n",
    "            source='/opt/ml/processing/output/test',\n",
    "            destination=f's3://{bucket}/{prefix}/processed/test'\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "```\n",
    "\n",
    "The `preprocessing.py` script might look like:\n",
    "\n",
    "```python\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def preprocess_data(input_path, output_path):\n",
    "    # Read data\n",
    "    data = pd.read_csv(os.path.join(input_path, 'input.csv'))\n",
    "    \n",
    "    # Perform data cleaning\n",
    "    data = data.dropna()\n",
    "    \n",
    "    # Feature engineering\n",
    "    data['feature_x'] = data['a'] / data['b']\n",
    "    \n",
    "    # Split data\n",
    "    train, temp = train_test_split(data, test_size=0.3, random_state=42)\n",
    "    validation, test = train_test_split(temp, test_size=0.5, random_state=42)\n",
    "    \n",
    "    # Scale features\n",
    "    features = [col for col in data.columns if col != 'target']\n",
    "    scaler = StandardScaler()\n",
    "    train[features] = scaler.fit_transform(train[features])\n",
    "    validation[features] = scaler.transform(validation[features])\n",
    "    test[features] = scaler.transform(test[features])\n",
    "    \n",
    "    # Save datasets\n",
    "    os.makedirs(os.path.join(output_path, 'train'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_path, 'validation'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_path, 'test'), exist_ok=True)\n",
    "    \n",
    "    train.to_csv(os.path.join(output_path, 'train/train.csv'), index=False)\n",
    "    validation.to_csv(os.path.join(output_path, 'validation/validation.csv'), index=False)\n",
    "    test.to_csv(os.path.join(output_path, 'test/test.csv'), index=False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--input-path', type=str, default='/opt/ml/processing/input')\n",
    "    parser.add_argument('--output-path', type=str, default='/opt/ml/processing/output')\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    preprocess_data(args.input_path, args.output_path)\n",
    "```\n",
    "\n",
    "### 2. Model Training\n",
    "\n",
    "```python\n",
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "# Define the training job\n",
    "estimator = Estimator(\n",
    "    image_uri='123456789012.dkr.ecr.us-east-1.amazonaws.com/my-training-image:latest',\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    volume_size=30,\n",
    "    max_run=3600,\n",
    "    input_mode='File',\n",
    "    output_path=f's3://{bucket}/{prefix}/model/',\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    hyperparameters={\n",
    "        'epochs': 100,\n",
    "        'batch_size': 64,\n",
    "        'learning_rate': 0.001,\n",
    "        'hidden_units': 128\n",
    "    }\n",
    ")\n",
    "\n",
    "# Define data channels\n",
    "train_data = f's3://{bucket}/{prefix}/processed/train'\n",
    "validation_data = f's3://{bucket}/{prefix}/processed/validation'\n",
    "\n",
    "estimator.fit(\n",
    "    inputs={\n",
    "        'train': train_data,\n",
    "        'validation': validation_data\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "### 3. Model Evaluation\n",
    "\n",
    "```python\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "# Use the same processor for evaluation\n",
    "evaluation_processor = ScriptProcessor(\n",
    "    command=['python3'],\n",
    "    image_uri='123456789012.dkr.ecr.us-east-1.amazonaws.com/my-evaluation-image:latest',\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "# Run the evaluation job\n",
    "evaluation_processor.run(\n",
    "    code='evaluate.py',\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=estimator.model_data,\n",
    "            destination='/opt/ml/processing/model'\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=f's3://{bucket}/{prefix}/processed/test',\n",
    "            destination='/opt/ml/processing/test'\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name='evaluation',\n",
    "            source='/opt/ml/processing/evaluation',\n",
    "            destination=f's3://{bucket}/{prefix}/evaluation'\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "```\n",
    "\n",
    "### 4. Model Registration and Deployment\n",
    "\n",
    "```python\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "# Check if the model meets quality threshold\n",
    "s3_client = boto3.client('s3')\n",
    "evaluation_file = s3_client.get_object(\n",
    "    Bucket=bucket,\n",
    "    Key=f'{prefix}/evaluation/evaluation.json'\n",
    ")\n",
    "evaluation_data = json.loads(evaluation_file['Body'].read().decode('utf-8'))\n",
    "\n",
    "# Register the model if it meets quality criteria\n",
    "if evaluation_data['accuracy'] >= 0.85:\n",
    "    from sagemaker.model import Model\n",
    "    \n",
    "    # Create a model\n",
    "    model = Model(\n",
    "        image_uri='123456789012.dkr.ecr.us-east-1.amazonaws.com/my-inference-image:latest',\n",
    "        model_data=estimator.model_data,\n",
    "        role=role,\n",
    "        sagemaker_session=sagemaker_session\n",
    "    )\n",
    "    \n",
    "    # Deploy the model to an endpoint\n",
    "    predictor = model.deploy(\n",
    "        initial_instance_count=1,\n",
    "        instance_type='ml.m5.large',\n",
    "        endpoint_name='ml-pipeline-endpoint'\n",
    "    )\n",
    "    \n",
    "    print(f\"Model deployed to endpoint: {predictor.endpoint_name}\")\n",
    "else:\n",
    "    print(f\"Model did not meet quality threshold. Accuracy: {evaluation_data['accuracy']}\")\n",
    "```\n",
    "\n",
    "## Using SageMaker Pipelines for Orchestration\n",
    "\n",
    "SageMaker Pipelines provides a more structured way to define and manage ML workflows:\n",
    "\n",
    "```python\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.steps import ProcessingStep, TrainingStep\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.functions import JsonGet\n",
    "\n",
    "# Define the processing step\n",
    "processing_step = ProcessingStep(\n",
    "    name=\"DataPreprocessing\",\n",
    "    processor=processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=f's3://{bucket}/{prefix}/raw-data/',\n",
    "            destination='/opt/ml/processing/input',\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name='train',\n",
    "            source='/opt/ml/processing/output/train',\n",
    "            destination=f's3://{bucket}/{prefix}/processed/train'\n",
    "        ),\n",
    "        ProcessingOutput(\n",
    "            output_name='validation',\n",
    "            source='/opt/ml/processing/output/validation',\n",
    "            destination=f's3://{bucket}/{prefix}/processed/validation'\n",
    "        ),\n",
    "        ProcessingOutput(\n",
    "            output_name='test',\n",
    "            source='/opt/ml/processing/output/test',\n",
    "            destination=f's3://{bucket}/{prefix}/processed/test'\n",
    "        )\n",
    "    ],\n",
    "    code='preprocessing.py'\n",
    ")\n",
    "\n",
    "# Define the training step\n",
    "training_step = TrainingStep(\n",
    "    name=\"ModelTraining\",\n",
    "    estimator=estimator,\n",
    "    inputs={\n",
    "        'train': sagemaker.inputs.TrainingInput(\n",
    "            s3_data=processing_step.properties.ProcessingOutputConfig.Outputs['train'].S3Output.S3Uri,\n",
    "            content_type='text/csv'\n",
    "        ),\n",
    "        'validation': sagemaker.inputs.TrainingInput(\n",
    "            s3_data=processing_step.properties.ProcessingOutputConfig.Outputs['validation'].S3Output.S3Uri,\n",
    "            content_type='text/csv'\n",
    "        )\n",
    "    }\n",
    ")\n",
    "\n",
    "# Define the evaluation step\n",
    "evaluation_step = ProcessingStep(\n",
    "    name=\"ModelEvaluation\",\n",
    "    processor=evaluation_processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=training_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "            destination='/opt/ml/processing/model'\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=processing_step.properties.ProcessingOutputConfig.Outputs['test'].S3Output.S3Uri,\n",
    "            destination='/opt/ml/processing/test'\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name='evaluation',\n",
    "            source='/opt/ml/processing/evaluation',\n",
    "            destination=f's3://{bucket}/{prefix}/evaluation'\n",
    "        )\n",
    "    ],\n",
    "    code='evaluate.py'\n",
    ")\n",
    "\n",
    "# Define the evaluation report\n",
    "evaluation_report = PropertyFile(\n",
    "    name=\"EvaluationReport\",\n",
    "    output_name='evaluation',\n",
    "    path='evaluation.json'\n",
    ")\n",
    "evaluation_step.add_property_file(evaluation_report)\n",
    "\n",
    "# Define a condition to check model quality\n",
    "accuracy_condition = ConditionGreaterThanOrEqualTo(\n",
    "    left=JsonGet(\n",
    "        step_name=evaluation_step.name,\n",
    "        property_file=evaluation_report,\n",
    "        json_path='accuracy'\n",
    "    ),\n",
    "    right=0.85\n",
    ")\n",
    "\n",
    "# Define the model registration step\n",
    "from sagemaker.workflow.model_step import ModelStep\n",
    "\n",
    "model = Model(\n",
    "    image_uri='123456789012.dkr.ecr.us-east-1.amazonaws.com/my-inference-image:latest',\n",
    "    model_data=training_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    role=role,\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "model_step = ModelStep(\n",
    "    name=\"RegisterModel\",\n",
    "    step_args=model.create(\n",
    "        content_types=[\"text/csv\"],\n",
    "        response_types=[\"text/csv\"],\n",
    "        inference_instances=[\"ml.m5.large\"],\n",
    "        transform_instances=[\"ml.m5.large\"],\n",
    "        model_package_group_name=\"MyModelPackageGroup\",\n",
    "        approval_status=\"Approved\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Define the conditional step for model registration\n",
    "condition_step = ConditionStep(\n",
    "    name=\"CheckAccuracy\",\n",
    "    conditions=[accuracy_condition],\n",
    "    if_steps=[model_step],\n",
    "    else_steps=[]\n",
    ")\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(\n",
    "    name=\"MyMLPipeline\",\n",
    "    parameters=[],\n",
    "    steps=[processing_step, training_step, evaluation_step, condition_step],\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "# Submit the pipeline definition\n",
    "pipeline.upsert(role_arn=role)\n",
    "execution = pipeline.start()\n",
    "```\n",
    "\n",
    "## Using Built-in SageMaker Algorithms\n",
    "\n",
    "SageMaker provides optimized implementations of popular algorithms that can be used in your pipelines:\n",
    "\n",
    "```python\n",
    "from sagemaker.xgboost.estimator import XGBoost\n",
    "\n",
    "# Create an XGBoost estimator\n",
    "xgb_estimator = XGBoost(\n",
    "    entry_point='train.py',\n",
    "    framework_version='1.2-1',\n",
    "    hyperparameters={\n",
    "        'max_depth': 5,\n",
    "        'eta': 0.2,\n",
    "        'gamma': 4,\n",
    "        'min_child_weight': 6,\n",
    "        'subsample': 0.8,\n",
    "        'objective': 'binary:logistic',\n",
    "        'num_round': 100\n",
    "    },\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    output_path=f's3://{bucket}/{prefix}/xgboost-model/'\n",
    ")\n",
    "\n",
    "# Define training step with XGBoost\n",
    "xgb_train_step = TrainingStep(\n",
    "    name=\"XGBoostTraining\",\n",
    "    estimator=xgb_estimator,\n",
    "    inputs={\n",
    "        'train': sagemaker.inputs.TrainingInput(\n",
    "            s3_data=processing_step.properties.ProcessingOutputConfig.Outputs['train'].S3Output.S3Uri,\n",
    "            content_type='text/csv'\n",
    "        ),\n",
    "        'validation': sagemaker.inputs.TrainingInput(\n",
    "            s3_data=processing_step.properties.ProcessingOutputConfig.Outputs['validation'].S3Output.S3Uri,\n",
    "            content_type='text/csv'\n",
    "        )\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "## Hyperparameter Tuning with SageMaker\n",
    "\n",
    "```python\n",
    "from sagemaker.tuner import HyperparameterTuner, ContinuousParameter, IntegerParameter, CategoricalParameter\n",
    "\n",
    "# Define the hyperparameter ranges\n",
    "hyperparameter_ranges = {\n",
    "    'max_depth': IntegerParameter(3, 10),\n",
    "    'eta': ContinuousParameter(0.1, 0.5),\n",
    "    'min_child_weight': IntegerParameter(2, 10),\n",
    "    'subsample': ContinuousParameter(0.5, 1.0),\n",
    "    'gamma': ContinuousParameter(0, 5)\n",
    "}\n",
    "\n",
    "# Create a tuner\n",
    "tuner = HyperparameterTuner(\n",
    "    estimator=xgb_estimator,\n",
    "    objective_metric_name='validation:auc',\n",
    "    hyperparameter_ranges=hyperparameter_ranges,\n",
    "    metric_definitions=[\n",
    "        {'Name': 'validation:auc', 'Regex': 'validation-auc: ([0-9\\\\.]+)'}\n",
    "    ],\n",
    "    max_jobs=20,\n",
    "    max_parallel_jobs=5,\n",
    "    strategy='Bayesian',\n",
    "    objective_type='Maximize'\n",
    ")\n",
    "\n",
    "# Create a tuning step\n",
    "from sagemaker.workflow.steps import TuningStep\n",
    "\n",
    "tuning_step = TuningStep(\n",
    "    name=\"HPTuning\",\n",
    "    tuner=tuner,\n",
    "    inputs={\n",
    "        'train': sagemaker.inputs.TrainingInput(\n",
    "            s3_data=processing_step.properties.ProcessingOutputConfig.Outputs['train'].S3Output.S3Uri,\n",
    "            content_type='text/csv'\n",
    "        ),\n",
    "        'validation': sagemaker.inputs.TrainingInput(\n",
    "            s3_data=processing_step.properties.ProcessingOutputConfig.Outputs['validation'].S3Output.S3Uri,\n",
    "            content_type='text/csv'\n",
    "        )\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "## Feature Store Integration\n",
    "\n",
    "```python\n",
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "import time\n",
    "\n",
    "# Create a feature group\n",
    "feature_group_name = f'customer-features-{int(time.time())}'\n",
    "feature_group = FeatureGroup(\n",
    "    name=feature_group_name,\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "# Set up the feature definitions\n",
    "feature_group.load_feature_definitions(data_frame=processed_df)\n",
    "\n",
    "# Create the feature group\n",
    "feature_group.create(\n",
    "    s3_uri=f's3://{bucket}/{prefix}/feature-store',\n",
    "    record_identifier_name='customer_id',\n",
    "    event_time_feature_name='timestamp',\n",
    "    role_arn=role,\n",
    "    enable_online_store=True\n",
    ")\n",
    "\n",
    "# Wait for feature group creation\n",
    "feature_group.wait()\n",
    "\n",
    "# Ingest data into the feature group\n",
    "feature_group.ingest(data_frame=processed_df, max_workers=3, wait=True)\n",
    "```\n",
    "\n",
    "## ⚠️ Best Practices for SageMaker ML Pipelines\n",
    "\n",
    "1. **Infrastructure as Code**\n",
    "   - Use AWS CloudFormation or Terraform to define and manage SageMaker resources\n",
    "   - Version control your pipeline definitions and scripts\n",
    "\n",
    "2. **Modular Pipeline Components**\n",
    "   - Create reusable processing, training, and evaluation scripts\n",
    "   - Parameterize your pipeline steps for flexibility\n",
    "\n",
    "3. **Data Management**\n",
    "   - Use SageMaker Feature Store for feature sharing across teams\n",
    "   - Implement data validation steps before model training\n",
    "   - Version your datasets in S3 with clear naming conventions\n",
    "\n",
    "4. **Security**\n",
    "   - Use IAM roles with least privilege\n",
    "   - Encrypt data at rest and in transit\n",
    "   - Implement network isolation for sensitive workloads\n",
    "\n",
    "5. **Cost Optimization**\n",
    "   - Use Spot Instances for training when possible\n",
    "   - Implement auto-shutdown for notebook instances\n",
    "   - Choose appropriate instance types for each step\n",
    "\n",
    "6. **Monitoring and Observability**\n",
    "   - Enable CloudWatch logging for all pipeline components\n",
    "   - Set up model monitoring for deployed endpoints\n",
    "   - Use SageMaker Experiments to track metrics across pipeline runs\n",
    "\n",
    "7. **CI/CD Integration**\n",
    "   - Trigger pipeline executions from CI/CD systems\n",
    "   - Implement automated testing for pipeline components\n",
    "   - Use the SageMaker Python SDK in CI/CD workflows\n",
    "\n",
    "## Real-world SageMaker Pipeline Example\n",
    "\n",
    "Here's a complete example of a real-world pipeline for a customer churn prediction model:\n",
    "\n",
    "```python\n",
    "import sagemaker\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.steps import ProcessingStep, TrainingStep, TuningStep, ModelStep\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.functions import JsonGet\n",
    "from sagemaker.processing import ScriptProcessor, ProcessingInput, ProcessingOutput\n",
    "from sagemaker.xgboost.estimator import XGBoost\n",
    "from sagemaker.tuner import HyperparameterTuner, IntegerParameter, ContinuousParameter\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.inputs import TrainingInput\n",
    "import boto3\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Initialize SageMaker session\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'customer-churn-pipeline'\n",
    "\n",
    "# Create a unique pipeline name\n",
    "pipeline_name = f\"CustomerChurnPipeline-{int(time.time())}\"\n",
    "\n",
    "# 1. Data Preprocessing Step\n",
    "preprocessing_processor = ScriptProcessor(\n",
    "    image_uri='123456789012.dkr.ecr.us-east-1.amazonaws.com/preprocessing:latest',\n",
    "    command=['python3'],\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "preprocessing_step = ProcessingStep(\n",
    "    name=\"PreprocessCustomerData\",\n",
    "    processor=preprocessing_processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=f's3://{bucket}/raw/customer-data.csv',\n",
    "            destination='/opt/ml/processing/input'\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name='training',\n",
    "            source='/opt/ml/processing/output/train',\n",
    "            destination=f's3://{bucket}/{prefix}/processed/train'\n",
    "        ),\n",
    "        ProcessingOutput(\n",
    "            output_name='validation',\n",
    "            source='/opt/ml/processing/output/validation',\n",
    "            destination=f's3://{bucket}/{prefix}/processed/validation'\n",
    "        ),\n",
    "        ProcessingOutput(\n",
    "            output_name='test',\n",
    "            source='/opt/ml/processing/output/test',\n",
    "            destination=f's3://{bucket}/{prefix}/processed/test'\n",
    "        )\n",
    "    ],\n",
    "    code=f's3://{bucket}/code/preprocessing.py'\n",
    ")\n",
    "\n",
    "# 2. Hyperparameter Tuning Step\n",
    "xgb_estimator = XGBoost(\n",
    "    entry_point='train.py',\n",
    "    framework_version='1.3-1',\n",
    "    hyperparameters={\n",
    "        'objective': 'binary:logistic',\n",
    "        'num_round': 100,\n",
    "        'eval_metric': 'auc'\n",
    "    },\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    output_path=f's3://{bucket}/{prefix}/model/'\n",
    ")\n",
    "\n",
    "hyperparameter_ranges = {\n",
    "    'max_depth': IntegerParameter(3, 10),\n",
    "    'eta': ContinuousParameter(0.1, 0.5),\n",
    "    'min_child_weight': IntegerParameter(2, 10),\n",
    "    'subsample': ContinuousParameter(0.5, 1.0),\n",
    "    'gamma': ContinuousParameter(0, 5)\n",
    "}\n",
    "\n",
    "tuner = HyperparameterTuner(\n",
    "    estimator=xgb_estimator,\n",
    "    objective_metric_name='validation-auc',\n",
    "    hyperparameter_ranges=hyperparameter_ranges,\n",
    "    max_jobs=10,\n",
    "    max_parallel_jobs=2,\n",
    "    strategy='Bayesian',\n",
    "    objective_type='Maximize'\n",
    ")\n",
    "\n",
    "tuning_step = TuningStep(\n",
    "    name=\"TuneChurnModel\",\n",
    "    tuner=tuner,\n",
    "    inputs={\n",
    "        'train': TrainingInput(\n",
    "            s3_data=preprocessing_step.properties.ProcessingOutputConfig.Outputs['training'].S3Output.S3Uri,\n",
    "            content_type='text/csv'\n",
    "        ),\n",
    "        'validation': TrainingInput(\n",
    "            s3_data=preprocessing_step.properties.ProcessingOutputConfig.Outputs['validation'].S3Output.S3Uri,\n",
    "            content_type='text/csv'\n",
    "        )\n",
    "    }\n",
    ")\n",
    "\n",
    "# 3. Model Evaluation Step\n",
    "evaluation_processor = ScriptProcessor(\n",
    "    image_uri='123456789012.dkr.ecr.us-east-1.amazonaws.com/evaluation:latest',\n",
    "    command=['python3'],\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "evaluation_step = ProcessingStep(\n",
    "    name=\"EvaluateChurnModel\",\n",
    "    processor=evaluation_processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=tuning_step.get_top_model_s3_uri(top_k=0, s3_bucket=bucket, prefix=f'{prefix}/tuner'),\n",
    "            destination='/opt/ml/processing/model'\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=preprocessing_step.properties.ProcessingOutputConfig.Outputs['test'].S3Output.S3Uri,\n",
    "            destination='/opt/ml/processing/test'\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name='evaluation',\n",
    "            source='/opt/ml/processing/evaluation',\n",
    "            destination=f's3://{bucket}/{prefix}/evaluation'\n",
    "        )\n",
    "    ],\n",
    "    code=f's3://{bucket}/code/evaluate.py'\n",
    ")\n",
    "\n",
    "# Define property file for evaluation metrics\n",
    "evaluation_report = PropertyFile(\n",
    "    name=\"EvaluationReport\",\n",
    "    output_name='evaluation',\n",
    "    path='evaluation.json'\n",
    ")\n",
    "evaluation_step.add_property_file(evaluation_report)\n",
    "\n",
    "# 4. Model Registration Condition\n",
    "accuracy_condition = ConditionGreaterThanOrEqualTo(\n",
    "    left=JsonGet(\n",
    "        step_name=evaluation_step.name,\n",
    "        property_file=evaluation_report,\n",
    "        json_path='auc'\n",
    "    ),\n",
    "    right=0.80\n",
    ")\n",
    "\n",
    "# 5. Model Registration Step\n",
    "model_metrics = {\n",
    "    'auc': {\n",
    "        'value': JsonGet(\n",
    "            step_name=evaluation_step.name,\n",
    "            property_file=evaluation_report,\n",
    "            json_path='auc'\n",
    "        ),\n",
    "        'standard_deviation': 'NaN'\n",
    "    },\n",
    "    'accuracy': {\n",
    "        'value': JsonGet(\n",
    "            step_name=evaluation_step.name,\n",
    "            property_file=evaluation_report,\n",
    "            json_path='accuracy'\n",
    "        ),\n",
    "        'standard_deviation': 'NaN'\n",
    "    }\n",
    "}\n",
    "\n",
    "model = Model(\n",
    "    image_uri='123456789012.dkr.ecr.us-east-1.amazonaws.com/inference:latest',\n",
    "    model_data=tuning_step.get_top_model_s3_uri(top_k=0, s3_bucket=bucket, prefix=f'{prefix}/tuner'),\n",
    "    role=role,\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "register_step = ModelStep(\n",
    "    name=\"RegisterChurnModel\",\n",
    "    step_args=model.register(\n",
    "        content_types=[\"text/csv\"],\n",
    "        response_types=[\"text/csv\"],\n",
    "        inference_instances=[\"ml.m5.large\"],\n",
    "        transform_instances=[\"ml.m5.large\"],\n",
    "        model_package_group_name=\"CustomerChurnModels\",\n",
    "        model_metrics=model_metrics,\n",
    "        approval_status=\"PendingManualApproval\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# 6. Conditional Step for Model Registration\n",
    "condition_step = ConditionStep(\n",
    "    name=\"CheckModelAccuracy\",\n",
    "    conditions=[accuracy_condition],\n",
    "    if_steps=[register_step],\n",
    "    else_steps=[]\n",
    ")\n",
    "\n",
    "# 7. Create and Submit Pipeline\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[],\n",
    "    steps=[preprocessing_step, tuning_step, evaluation_step, condition_step],\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "pipeline.upsert(role_arn=role)\n",
    "execution = pipeline.start()\n",
    "```\n",
    "\n",
    "## Accessing SageMaker Endpoints Outside of SageMaker\n",
    "\n",
    "Once your model is deployed through your pipeline, you can access it from any application:\n",
    "\n",
    "```python\n",
    "import boto3\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Create a SageMaker runtime client\n",
    "runtime = boto3.client(\n",
    "    'sagemaker-runtime',\n",
    "    region_name='us-east-1',\n",
    "    aws_access_key_id='YOUR_ACCESS_KEY',\n",
    "    aws_secret_access_key='YOUR_SECRET_KEY'\n",
    ")\n",
    "\n",
    "# Prepare test data\n",
    "test_data = \"42,1,125000,8,0,3,1,0\"\n",
    "\n",
    "# Invoke the endpoint\n",
    "response = runtime.invoke_endpoint(\n",
    "    EndpointName='customer-churn-endpoint',\n",
    "    ContentType='text/csv',\n",
    "    Body=test_data\n",
    ")\n",
    "\n",
    "# Parse the response\n",
    "result = response['Body'].read().decode('utf-8')\n",
    "print(f\"Churn Probability: {result}\")\n",
    "```\n",
    "\n",
    "## Automating Pipeline Execution with EventBridge\n",
    "\n",
    "You can set up automatic pipeline triggers based on events:\n",
    "\n",
    "```python\n",
    "import boto3\n",
    "\n",
    "# Create EventBridge client\n",
    "events_client = boto3.client('events')\n",
    "\n",
    "# Create a rule to run the pipeline daily\n",
    "response = events_client.put_rule(\n",
    "    Name='DailyChurnPipelineExecution',\n",
    "    ScheduleExpression='cron(0 0 * * ? *)',  # Run at midnight every day\n",
    "    State='ENABLED'\n",
    ")\n",
    "\n",
    "# Create a target for the rule\n",
    "response = events_client.put_targets(\n",
    "    Rule='DailyChurnPipelineExecution',\n",
    "    Targets=[\n",
    "        {\n",
    "            'Id': 'ChurnPipelineTarget',\n",
    "            'Arn': f'arn:aws:sagemaker:{region}:{account_id}:pipeline/{pipeline_name}',\n",
    "            'RoleArn': 'arn:aws:iam::{account_id}:role/EventBridgeSageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Processing Large-Scale Data Using Apache Spark\n",
    "\n",
    "## Comparing Pandas vs PySpark\n",
    "\n",
    "| Feature | Pandas | PySpark |\n",
    "|---------|--------|---------|\n",
    "| **Scale** | Single machine, memory-limited | Distributed across clusters |\n",
    "| **Data Size** | GB range | TB/PB range |\n",
    "| **Processing Model** | In-memory, single-thread (mostly) | Distributed, parallel processing |\n",
    "| **Memory Management** | Loads entire dataset into memory | Lazy evaluation, distributed storage |\n",
    "| **Learning Curve** | Easier to learn and use | Steeper learning curve |\n",
    "| **Use Case** | Data analysis, small-to-medium datasets | Big data processing, ETL, ML at scale |\n",
    "| **Performance for Large Data** | Slow, may crash | Fast, scalable |\n",
    "| **API Richness** | Very rich, mature ecosystem | Growing, covers most common operations |\n",
    "\n",
    "## Apache Spark Core Concepts\n",
    "\n",
    "### Spark Architecture\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────┐\n",
    "│                Driver Program                │\n",
    "│                                             │\n",
    "│  ┌─────────────┐        ┌──────────────┐    │\n",
    "│  │SparkContext │───────▶│ Cluster      │    │\n",
    "│  └─────────────┘        │ Manager      │    │\n",
    "└─────────────────────────┘──────────┬───┘────┘\n",
    "                                     │\n",
    "     ┌───────────────────────────────┼───────────────────────────┐\n",
    "     │                               │                           │\n",
    "     ▼                               ▼                           ▼\n",
    "┌──────────────┐             ┌──────────────┐             ┌──────────────┐\n",
    "│  Worker Node │             │  Worker Node │             │  Worker Node │\n",
    "│              │             │              │             │              │\n",
    "│  ┌────────┐  │             │  ┌────────┐  │             │  ┌────────┐  │\n",
    "│  │Executor│  │             │  │Executor│  │             │  │Executor│  │\n",
    "│  └────────┘  │             │  └────────┘  │             │  └────────┘  │\n",
    "└──────────────┘             └──────────────┘             └──────────────┘\n",
    "```\n",
    "\n",
    "### Key Components\n",
    "\n",
    "1. **Driver Program**: Contains the main application and creates SparkContext\n",
    "2. **Cluster Manager**: Allocates resources across the cluster (YARN, Mesos, Kubernetes, or Spark's standalone manager)\n",
    "3. **Workers**: Compute nodes that run tasks\n",
    "4. **Executors**: Processes that run on worker nodes and execute tasks\n",
    "5. **Tasks**: Individual units of work sent to executors\n",
    "\n",
    "## Setting Up PySpark Environment\n",
    "\n",
    "### Local Setup\n",
    "\n",
    "```bash\n",
    "# Install PySpark with pip\n",
    "pip install pyspark\n",
    "\n",
    "# For working with pandas APIs\n",
    "pip install pyspark[pandas]\n",
    "\n",
    "# For ML functionality\n",
    "pip install pyspark[ml]\n",
    "```\n",
    "\n",
    "### Initializing Spark Session\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"LargeScaleDataProcessing\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Check Spark version\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "\n",
    "# Set log level to reduce verbosity\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "```\n",
    "\n",
    "## Working with Spark DataFrames\n",
    "\n",
    "### Creating DataFrames\n",
    "\n",
    "```python\n",
    "# From a list\n",
    "data = [(\"John\", 28), (\"Anna\", 24), (\"Mike\", 32)]\n",
    "columns = [\"Name\", \"Age\"]\n",
    "df1 = spark.createDataFrame(data, columns)\n",
    "\n",
    "# From a Pandas DataFrame\n",
    "import pandas as pd\n",
    "pandas_df = pd.DataFrame({\n",
    "    \"Name\": [\"John\", \"Anna\", \"Mike\"],\n",
    "    \"Age\": [28, 24, 32]\n",
    "})\n",
    "df2 = spark.createDataFrame(pandas_df)\n",
    "\n",
    "# From external data sources\n",
    "df3 = spark.read.csv(\"s3://bucket/large_dataset.csv\", header=True, inferSchema=True)\n",
    "df4 = spark.read.parquet(\"hdfs://cluster/data.parquet\")\n",
    "df5 = spark.read.json(\"gs://bucket/data.json\")\n",
    "df6 = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost:5432/mydb\") \\\n",
    "    .option(\"dbtable\", \"users\") \\\n",
    "    .option(\"user\", \"username\") \\\n",
    "    .option(\"password\", \"password\") \\\n",
    "    .load()\n",
    "```\n",
    "\n",
    "### Basic DataFrame Operations\n",
    "\n",
    "```python\n",
    "# Display DataFrame schema\n",
    "df3.printSchema()\n",
    "\n",
    "# Show first few rows\n",
    "df3.show(5)\n",
    "\n",
    "# Basic statistics\n",
    "df3.describe().show()\n",
    "\n",
    "# Select specific columns\n",
    "df3.select(\"name\", \"age\", \"salary\").show(5)\n",
    "\n",
    "# Filter rows\n",
    "df3.filter(df3.age > 30).show()\n",
    "# OR using SQL-like syntax\n",
    "df3.filter(\"age > 30\").show()\n",
    "\n",
    "# Add new columns\n",
    "from pyspark.sql.functions import col\n",
    "df3 = df3.withColumn(\"salary_thousands\", col(\"salary\") / 1000)\n",
    "\n",
    "# Group and aggregate\n",
    "from pyspark.sql.functions import avg, sum, count\n",
    "df3.groupBy(\"department\").agg(\n",
    "    count(\"id\").alias(\"employee_count\"),\n",
    "    avg(\"salary\").alias(\"avg_salary\"),\n",
    "    sum(\"salary\").alias(\"total_salary\")\n",
    ").show()\n",
    "\n",
    "# Sort data\n",
    "df3.orderBy(\"age\").show()\n",
    "df3.orderBy(col(\"age\").desc()).show()\n",
    "```\n",
    "\n",
    "### Working with SQL in Spark\n",
    "\n",
    "```python\n",
    "# Register DataFrame as a temp view\n",
    "df3.createOrReplaceTempView(\"employees\")\n",
    "\n",
    "# Run SQL queries\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT department, \n",
    "           COUNT(*) as employee_count, \n",
    "           AVG(salary) as avg_salary,\n",
    "           SUM(salary) as total_salary\n",
    "    FROM employees\n",
    "    WHERE age > 25\n",
    "    GROUP BY department\n",
    "    ORDER BY avg_salary DESC\n",
    "\"\"\")\n",
    "\n",
    "result.show()\n",
    "```\n",
    "\n",
    "## RDDs (Resilient Distributed Datasets)\n",
    "\n",
    "### Creating RDDs\n",
    "\n",
    "```python\n",
    "# From a collection\n",
    "data = [1, 2, 3, 4, 5]\n",
    "rdd1 = spark.sparkContext.parallelize(data)\n",
    "\n",
    "# From external data\n",
    "rdd2 = spark.sparkContext.textFile(\"hdfs://path/to/textfile.txt\")\n",
    "\n",
    "# From DataFrame\n",
    "rdd3 = df3.rdd\n",
    "```\n",
    "\n",
    "### RDD Operations\n",
    "\n",
    "```python\n",
    "# Transformations (lazy operations)\n",
    "mapped_rdd = rdd1.map(lambda x: x * 2)\n",
    "filtered_rdd = rdd1.filter(lambda x: x % 2 == 0)\n",
    "flat_mapped_rdd = rdd1.flatMap(lambda x: [x, x*2])\n",
    "\n",
    "# Actions (execute and return results)\n",
    "print(mapped_rdd.collect())  # Returns all elements\n",
    "print(mapped_rdd.count())    # Number of elements\n",
    "print(mapped_rdd.first())    # First element\n",
    "print(mapped_rdd.take(3))    # First 3 elements\n",
    "\n",
    "# Key-value operations\n",
    "kvp_rdd = spark.sparkContext.parallelize([(\"a\", 1), (\"b\", 2), (\"a\", 3)])\n",
    "by_key = kvp_rdd.reduceByKey(lambda a, b: a + b)\n",
    "print(by_key.collect())  # [(\"a\", 4), (\"b\", 2)]\n",
    "```\n",
    "\n",
    "## ⚠️ Advanced Data Processing Techniques\n",
    "\n",
    "### Partitioning for Performance\n",
    "\n",
    "```python\n",
    "# Check number of partitions\n",
    "print(f\"Number of partitions: {df3.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Repartition (full shuffle)\n",
    "df_repartitioned = df3.repartition(10)\n",
    "\n",
    "# Coalesce (avoids full shuffle when reducing partitions)\n",
    "df_coalesced = df3.coalesce(5)\n",
    "\n",
    "# Partition by specific column (good for join performance)\n",
    "df_partitioned = df3.repartitionByRange(10, \"department\")\n",
    "```\n",
    "\n",
    "### Caching and Persistence\n",
    "\n",
    "```python\n",
    "# Cache DataFrame in memory\n",
    "df3.cache()\n",
    "\n",
    "# Alternative with more control\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "df3.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "# Check if cached\n",
    "print(f\"Is cached: {df3.is_cached}\")\n",
    "\n",
    "# Remove from cache when done\n",
    "df3.unpersist()\n",
    "```\n",
    "\n",
    "### Optimizing Joins\n",
    "\n",
    "```python\n",
    "# Join two large DataFrames\n",
    "employees = spark.read.parquet(\"employees.parquet\")\n",
    "departments = spark.read.parquet(\"departments.parquet\")\n",
    "\n",
    "# Broadcast small DataFrame for performance\n",
    "from pyspark.sql.functions import broadcast\n",
    "result = employees.join(broadcast(departments), \n",
    "                      employees.dept_id == departments.id)\n",
    "\n",
    "# Explore different join types\n",
    "inner_join = employees.join(departments, \"dept_id\")\n",
    "left_join = employees.join(departments, \"dept_id\", \"left\")\n",
    "right_join = employees.join(departments, \"dept_id\", \"right\")\n",
    "full_join = employees.join(departments, \"dept_id\", \"full\")\n",
    "```\n",
    "\n",
    "## Processing Streaming Data with Spark\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import window, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"user_id\", StringType()),\n",
    "    StructField(\"event_time\", TimestampType()),\n",
    "    StructField(\"action\", StringType()),\n",
    "    StructField(\"item_id\", IntegerType())\n",
    "])\n",
    "\n",
    "# Create streaming DataFrame\n",
    "stream_df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"broker1:9092,broker2:9092\") \\\n",
    "    .option(\"subscribe\", \"user_events\") \\\n",
    "    .load() \\\n",
    "    .selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .select(from_json(col(\"value\"), schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n",
    "\n",
    "# Process streaming data with windowing\n",
    "result = stream_df \\\n",
    "    .withWatermark(\"event_time\", \"10 minutes\") \\\n",
    "    .groupBy(\n",
    "        window(col(\"event_time\"), \"1 hour\"),\n",
    "        col(\"action\")\n",
    "    ) \\\n",
    "    .count()\n",
    "\n",
    "# Write results to console (for testing)\n",
    "query = result.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "# Write to a sink\n",
    "query = result.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\", \"hdfs://path/to/output\") \\\n",
    "    .option(\"checkpointLocation\", \"hdfs://path/to/checkpoint\") \\\n",
    "    .start()\n",
    "\n",
    "# Wait for termination\n",
    "query.awaitTermination()\n",
    "```\n",
    "\n",
    "## Machine Learning with Spark MLlib\n",
    "\n",
    "### Data Preparation\n",
    "\n",
    "```python\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder, StandardScaler\n",
    "\n",
    "# Load data\n",
    "data = spark.read.parquet(\"customer_data.parquet\")\n",
    "\n",
    "# Handle categorical features\n",
    "indexer = StringIndexer(\n",
    "    inputCols=[\"gender\", \"status\"], \n",
    "    outputCols=[\"gender_idx\", \"status_idx\"]\n",
    ")\n",
    "encoder = OneHotEncoder(\n",
    "    inputCols=[\"gender_idx\", \"status_idx\"],\n",
    "    outputCols=[\"gender_vec\", \"status_vec\"]\n",
    ")\n",
    "\n",
    "# Create feature vector\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"age\", \"income\", \"gender_vec\", \"status_vec\"], \n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features\", \n",
    "    outputCol=\"scaled_features\",\n",
    "    withStd=True, \n",
    "    withMean=True\n",
    ")\n",
    "\n",
    "# Split data\n",
    "train, test = data.randomSplit([0.8, 0.2], seed=42)\n",
    "```\n",
    "\n",
    "### Training and Evaluating ML Models\n",
    "\n",
    "```python\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Build pipeline\n",
    "pipeline = Pipeline(stages=[\n",
    "    indexer,\n",
    "    encoder,\n",
    "    assembler,\n",
    "    scaler,\n",
    "    RandomForestClassifier(\n",
    "        labelCol=\"churn\",\n",
    "        featuresCol=\"scaled_features\",\n",
    "        numTrees=100\n",
    "    )\n",
    "])\n",
    "\n",
    "# Train model\n",
    "model = pipeline.fit(train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(test)\n",
    "\n",
    "# Evaluate model\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"churn\", \n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "roc_auc = evaluator.evaluate(predictions)\n",
    "print(f\"ROC-AUC: {roc_auc}\")\n",
    "\n",
    "# Save model\n",
    "model.write().overwrite().save(\"hdfs://path/to/model\")\n",
    "```\n",
    "\n",
    "### Hyperparameter Tuning\n",
    "\n",
    "```python\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Define parameter grid\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(indexer.handleInvalid, [\"keep\", \"skip\"]) \\\n",
    "    .addGrid(model.stages[-1].numTrees, [50, 100, 200]) \\\n",
    "    .addGrid(model.stages[-1].maxDepth, [5, 10, 15]) \\\n",
    "    .build()\n",
    "\n",
    "# Create cross-validator\n",
    "cv = CrossValidator(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3\n",
    ")\n",
    "\n",
    "# Run cross-validation\n",
    "cvModel = cv.fit(train)\n",
    "\n",
    "# Get best model\n",
    "bestModel = cvModel.bestModel\n",
    "```\n",
    "\n",
    "## Real-World Use Cases with PySpark\n",
    "\n",
    "### 1. ETL Pipeline for Data Warehouse\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import col, to_date, year, month, dayofmonth\n",
    "\n",
    "# Load raw data\n",
    "raw_data = spark.read.json(\"s3://bucket/raw/*.json\")\n",
    "\n",
    "# Transformation\n",
    "transformed = raw_data \\\n",
    "    .withColumn(\"date\", to_date(col(\"timestamp\"))) \\\n",
    "    .withColumn(\"year\", year(col(\"date\"))) \\\n",
    "    .withColumn(\"month\", month(col(\"date\"))) \\\n",
    "    .withColumn(\"day\", dayofmonth(col(\"date\"))) \\\n",
    "    .withColumn(\"revenue\", col(\"price\") * col(\"quantity\")) \\\n",
    "    .drop(\"timestamp\")\n",
    "\n",
    "# Data quality checks\n",
    "data_quality = transformed \\\n",
    "    .filter(col(\"price\").isNull() | col(\"quantity\").isNull()) \\\n",
    "    .count()\n",
    "\n",
    "if data_quality > 0:\n",
    "    print(f\"Warning: {data_quality} rows with missing values\")\n",
    "\n",
    "# Write to data warehouse (partitioned)\n",
    "transformed.write \\\n",
    "    .partitionBy(\"year\", \"month\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(\"s3://bucket/warehouse/sales\")\n",
    "```\n",
    "\n",
    "### 2. Customer Segmentation\n",
    "\n",
    "```python\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "\n",
    "# Load customer data\n",
    "customers = spark.read.parquet(\"s3://bucket/customers.parquet\")\n",
    "\n",
    "# Feature engineering\n",
    "featuresDF = customers.select(\n",
    "    \"customer_id\",\n",
    "    col(\"total_spent\").alias(\"monetary\"),\n",
    "    col(\"days_since_last_order\").alias(\"recency\"),\n",
    "    col(\"order_count\").alias(\"frequency\")\n",
    ")\n",
    "\n",
    "# Create feature vector\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"monetary\", \"recency\", \"frequency\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "assembled = assembler.transform(featuresDF)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features\",\n",
    "    outputCol=\"scaled_features\",\n",
    "    withStd=True,\n",
    "    withMean=True\n",
    ")\n",
    "scalerModel = scaler.fit(assembled)\n",
    "scaledData = scalerModel.transform(assembled)\n",
    "\n",
    "# Find optimal K using Elbow method\n",
    "wcss = []\n",
    "for k in range(2, 11):\n",
    "    kmeans = KMeans(k=k, seed=42, featuresCol=\"scaled_features\")\n",
    "    model = kmeans.fit(scaledData)\n",
    "    wcss.append(model.summary.trainingCost)\n",
    "\n",
    "# Plot WCSS (would need to convert to pandas first)\n",
    "# [Code to determine optimal k from wcss]\n",
    "\n",
    "# Train final model with optimal K\n",
    "kmeans = KMeans(k=4, seed=42, featuresCol=\"scaled_features\")\n",
    "model = kmeans.fit(scaledData)\n",
    "\n",
    "# Get cluster assignments\n",
    "clusters = model.transform(scaledData)\n",
    "cluster_counts = clusters.groupBy(\"prediction\").count().orderBy(\"prediction\")\n",
    "cluster_counts.show()\n",
    "\n",
    "# Analyze clusters\n",
    "cluster_analysis = clusters.groupBy(\"prediction\").agg(\n",
    "    avg(\"monetary\").alias(\"avg_monetary\"),\n",
    "    avg(\"recency\").alias(\"avg_recency\"),\n",
    "    avg(\"frequency\").alias(\"avg_frequency\"),\n",
    "    count(\"customer_id\").alias(\"customer_count\")\n",
    ").orderBy(\"prediction\")\n",
    "\n",
    "cluster_analysis.show()\n",
    "```\n",
    "\n",
    "### 3. Log Analysis\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import col, regexp_extract, window, count, sum\n",
    "\n",
    "# Read log data\n",
    "logs = spark.read.text(\"s3://bucket/logs/*.log\")\n",
    "\n",
    "# Parse logs\n",
    "pattern = r'^(\\S+) (\\S+) (\\S+) \\[([\\w:/]+\\s[+\\-]\\d{4})\\] \"(\\S+) (\\S+) (\\S+)\" (\\d{3}) (\\d+)'\n",
    "parsed_logs = logs.select(\n",
    "    regexp_extract(col(\"value\"), pattern, 1).alias(\"ip\"),\n",
    "    regexp_extract(col(\"value\"), pattern, 4).alias(\"timestamp\"),\n",
    "    regexp_extract(col(\"value\"), pattern, 5).alias(\"method\"),\n",
    "    regexp_extract(col(\"value\"), pattern, 6).alias(\"endpoint\"),\n",
    "    regexp_extract(col(\"value\"), pattern, 7).alias(\"protocol\"),\n",
    "    regexp_extract(col(\"value\"), pattern, 8).cast(\"integer\").alias(\"status\"),\n",
    "    regexp_extract(col(\"value\"), pattern, 9).cast(\"integer\").alias(\"bytes\")\n",
    ")\n",
    "\n",
    "# Clean and type timestamp\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "parsed_logs = parsed_logs.withColumn(\n",
    "    \"timestamp\", \n",
    "    to_timestamp(col(\"timestamp\"), \"dd/MMM/yyyy:HH:mm:ss Z\")\n",
    ")\n",
    "\n",
    "# Filter valid records\n",
    "valid_logs = parsed_logs.filter(col(\"status\").isNotNull())\n",
    "\n",
    "# Analyze HTTP status codes\n",
    "status_counts = valid_logs.groupBy(\"status\").count().orderBy(col(\"count\").desc())\n",
    "status_counts.show()\n",
    "\n",
    "# Find top endpoints by request count\n",
    "top_endpoints = valid_logs.groupBy(\"endpoint\").count().orderBy(col(\"count\").desc()).limit(10)\n",
    "top_endpoints.show(truncate=False)\n",
    "\n",
    "# Detect potential attacks - find IPs with high 404 rates\n",
    "potential_attacks = valid_logs \\\n",
    "    .filter(col(\"status\") == 404) \\\n",
    "    .groupBy(\"ip\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .limit(10)\n",
    "\n",
    "potential_attacks.show()\n",
    "\n",
    "# Time-based analysis\n",
    "hourly_traffic = valid_logs \\\n",
    "    .groupBy(window(col(\"timestamp\"), \"1 hour\")) \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"request_count\"),\n",
    "        sum(col(\"bytes\")).alias(\"total_bytes\")\n",
    "    ) \\\n",
    "    .orderBy(\"window\")\n",
    "\n",
    "hourly_traffic.show(24)\n",
    "```\n",
    "\n",
    "## ⚠️ Performance Optimization Tips\n",
    "\n",
    "1. **Use Appropriate File Format**\n",
    "   - Parquet and ORC are columnar formats that provide better performance than CSV or JSON\n",
    "   - Example: `df.write.parquet(\"data.parquet\")` instead of `df.write.csv(\"data.csv\")`\n",
    "\n",
    "2. **Partition Data Wisely**\n",
    "   - Choose high-cardinality columns that are frequently used in filters\n",
    "   - Avoid over-partitioning (too many small files)\n",
    "   - Example: `df.write.partitionBy(\"year\", \"month\").parquet(\"data.parquet\")`\n",
    "\n",
    "3. **Cache Strategically**\n",
    "   - Cache DataFrames that are reused multiple times\n",
    "   - Choose appropriate storage level based on memory constraints\n",
    "   - Example: `df.persist(StorageLevel.MEMORY_AND_DISK)`\n",
    "\n",
    "4. **Broadcast Small DataFrames in Joins**\n",
    "   - Reduces data shuffling across the network\n",
    "   - Example: `df1.join(broadcast(df2), \"key\")`\n",
    "\n",
    "5. **Use DataFrame APIs Instead of RDDs**\n",
    "   - DataFrame operations are optimized by Catalyst optimizer\n",
    "   - Example: Use `df.filter(col(\"age\") > 30)` instead of `df.rdd.filter(lambda x: x.age > 30)`\n",
    "\n",
    "6. **Monitor and Adjust Partitions**\n",
    "   - Too few partitions limit parallelism\n",
    "   - Too many partitions increase overhead\n",
    "   - Example: `df.repartition(numPartitions)` or `df.coalesce(numPartitions)`\n",
    "\n",
    "7. **Use Efficient UDFs**\n",
    "   - Pandas UDFs (vectorized UDFs) are much faster than regular Python UDFs\n",
    "   - Example: Use `@pandas_udf` instead of `udf`\n",
    "\n",
    "8. **Avoid `collect()` on Large DataFrames**\n",
    "   - Brings all data to the driver, which may cause OOM errors\n",
    "   - Use `take()`, `limit()`, or `sample()` instead\n",
    "\n",
    "9. **Optimize Serialization**\n",
    "   - Set `spark.serializer` to `org.apache.spark.serializer.KryoSerializer`\n",
    "   - Example: `.config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")`\n",
    "\n",
    "10. **Configure Memory Properly**\n",
    "    - Set driver and executor memory according to data size\n",
    "    - Example: `.config(\"spark.executor.memory\", \"10g\").config(\"spark.driver.memory\", \"4g\")`\n",
    "\n",
    "## Deploying Spark Applications\n",
    "\n",
    "### Using spark-submit\n",
    "\n",
    "```bash\n",
    "# Submit a PySpark job\n",
    "spark-submit \\\n",
    "  --master yarn \\\n",
    "  --deploy-mode cluster \\\n",
    "  --num-executors 10 \\\n",
    "  --executor-memory 4G \\\n",
    "  --executor-cores 2 \\\n",
    "  --driver-memory 2G \\\n",
    "  --conf spark.dynamicAllocation.enabled=true \\\n",
    "  --conf spark.shuffle.service.enabled=true \\\n",
    "  --py-files dependencies.zip \\\n",
    "  --files config.json \\\n",
    "  my_spark_app.py arg1 arg2\n",
    "```\n",
    "\n",
    "### Running on Kubernetes\n",
    "\n",
    "```yaml\n",
    "# example-spark-job.yaml\n",
    "apiVersion: \"sparkoperator.k8s.io/v1beta2\"\n",
    "kind: SparkApplication\n",
    "metadata:\n",
    "  name: pyspark-etl-job\n",
    "  namespace: spark-jobs\n",
    "spec:\n",
    "  type: Python\n",
    "  pythonVersion: \"3\"\n",
    "  mode: cluster\n",
    "  image: \"my-spark-image:latest\"\n",
    "  mainApplicationFile: \"local:///opt/spark/app/etl_job.py\"\n",
    "  sparkVersion: \"3.1.1\"\n",
    "  restartPolicy:\n",
    "    type: Never\n",
    "  driver:\n",
    "    cores: 1\n",
    "    memory: \"2G\"\n",
    "    labels:\n",
    "      version: 3.1.1\n",
    "  executor:\n",
    "    cores: 2\n",
    "    instances: 5\n",
    "    memory: \"4G\"\n",
    "    labels:\n",
    "      version: 3.1.1\n",
    "```\n",
    "\n",
    "### Managing Dependencies\n",
    "\n",
    "```bash\n",
    "# Create a zip file with dependencies\n",
    "pip install -t ./packages pandas numpy scikit-learn\n",
    "cd packages\n",
    "zip -r ../dependencies.zip .\n",
    "cd ..\n",
    "\n",
    "# Submit job with dependencies\n",
    "spark-submit \\\n",
    "  --py-files dependencies.zip \\\n",
    "  my_spark_app.py\n",
    "```\n",
    "\n",
    "## PySpark vs. Other Big Data Technologies\n",
    "\n",
    "| Technology | Pros | Cons | Best For |\n",
    "|------------|------|------|----------|\n",
    "| **PySpark** | Fast, in-memory processing, unified API | Complex setup, resource intensive | General-purpose big data processing |\n",
    "| **Hadoop MapReduce** | Mature, reliable | Slower than Spark, complex programming model | Batch processing on very large datasets |\n",
    "| **Dask** | Lightweight, Pandas-like API | Less mature, smaller ecosystem | Python users with medium-sized data |\n",
    "| **Ray** | Fast parallel execution, ML focus | Newer, smaller ecosystem | Distributed ML workloads |\n",
    "| **Flink** | Excellent streaming, low latency | Steeper learning curve | Real-time stream processing |\n",
    "\n",
    "## Conclusion: When to Use PySpark\n",
    "\n",
    "**Use PySpark when:**\n",
    "1. Working with datasets that exceed single-machine memory\n",
    "2. Need distributed processing across a cluster\n",
    "3. Performing complex ETL operations on large datasets\n",
    "4. Training ML models on massive datasets\n",
    "5. Processing streaming data at scale\n",
    "6. Unified batch and streaming processing is required\n",
    "\n",
    "**Consider alternatives when:**\n",
    "1. Working with smaller datasets that fit in memory (Pandas)\n",
    "2. Need simpler setup for medium-sized data (Dask)\n",
    "3. Focused primarily on streaming (Flink)\n",
    "4. Specific use cases with dedicated tools (BigQuery, Snowflake for data warehousing)\n",
    "\n",
    "PySpark excels at providing a unified platform for processing large-scale structured and unstructured data, with rich APIs for ETL, SQL, machine learning, and streaming applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
